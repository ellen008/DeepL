{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential_Privacy_Project_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "--L39NRSBSaO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellen008/DeepL/blob/master/Differential_Privacy_Project_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgnjb0HRJ69b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, SubsetRandomSampler\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQrGH00KJ69m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download the data\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download = True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "#mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5icqaUjJ69v",
        "colab_type": "text"
      },
      "source": [
        "# Datasets (data + labels):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNC-rhLyJ69y",
        "colab_type": "code",
        "outputId": "d80404d1-c0bc-4d80-c314-c095acf10572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7YIf6sTJ697",
        "colab_type": "code",
        "outputId": "cf9b6706-6bb5-4d2e-ccce-de3582524842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXCUUEMJ6-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  #++++++++++++\n",
        "  #+++++++++split MNIST train dataset on train.train/train.valid\n",
        "\n",
        "  #+++++++++create 10 teachers' datasets from train.train //train_ds//\n",
        "  #+++++++++split 10 teachers' train.train dataset in train/valid\n",
        "  #output:\n",
        "  #teacher_train_loader = [1 ... 10]\n",
        "  #teacher_valid_loader = [1 ... 10]\n",
        "    \n",
        "  #++++++++++++++++++++++++++\n",
        "  #+++++++++train_with_teacher() 10 models on 10 splitted trainsets\n",
        "   \n",
        "#TODO   \n",
        "  #model.forward(train.valid_dataset) //valid_ds//\n",
        "  #return 10 sets of true_labels = []  \n",
        "    \n",
        "#TODO\n",
        "  #dp_labels = true_labels + laplacianM\n",
        "\n",
        "#TODO\n",
        "  #split train.valid_dataset and true_labels on train/valid\n",
        "  #train.valid.train_loader\n",
        "  #train.valid.valid_loader\n",
        "\n",
        "  #train_model(train.valid_dataset + dp_labels)\n",
        "\n",
        "#TODO\n",
        "  #predict(MNIST test_dataset)\n",
        "  #compare predictions accuracy for true labels and dp_labels\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzRfxpVGXHGV",
        "colab_type": "code",
        "outputId": "5eafd15c-e61c-4b82-d9b3-c3f09db5dbe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#split train dataset into train.train = train_ds/ train.valid = valid_ds\n",
        "\n",
        "train_size = int(5 * len(train_dataset)/6)\n",
        "valid_size = len(train_dataset) - train_size\n",
        "\n",
        "train_ds, valid_ds = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
        "\n",
        "print(f\"Train.train dataset = {len(train_ds)}\",\n",
        "      f\"\\nTrain.valid dataset= {len(valid_ds)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train.train dataset = 50000 \n",
            "Train.valid dataset= 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_QnLjVZ4x6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch_size = 64\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=64,shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=64,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOVp2h8Yr0Ws",
        "colab_type": "text"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZaxYDGnl7Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,kernel_size = 5)\n",
        "        self.conv2 = nn.Conv2d(10,20, kernel_size = 5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \n",
        "        x = F.max_pool2d(self.conv1(x),2)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(self.conv2_drop(self.conv2(x)),2)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1,320)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training = self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x,dim = 1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkfaP_C0J6-1",
        "colab_type": "code",
        "outputId": "77a429e3-47b2-454d-ca81-bb3fa1b0118d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#model = Classifier_1()\n",
        "#model = Classifier_2()\n",
        "model = Conv()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vpljVdZbxri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYxMJPU8bxdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJg6k6He6hHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#reset the model\n",
        "def reset_model_conv():\n",
        "    model.conv1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.conv1.bias.data.fill_(0)\n",
        "    model.conv2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.conv2.bias.data.fill_(0)\n",
        "    model.fc1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc1.bias.data.fill_(0)\n",
        "    model.fc2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc2.bias.data.fill_(0)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnXn4-vlhxCr",
        "colab_type": "text"
      },
      "source": [
        "# train model_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOlpKPcwrlN",
        "colab_type": "code",
        "outputId": "db06583c-e6c0-4cb9-b583-2af811fe8a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#reset conv network:\n",
        "reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2qaE-NJoY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_1(device,criterion,optimizer):\n",
        "    \n",
        "    #device = device\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels in testloader:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "\n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Sx4KD-Cgwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_1():\n",
        "    \n",
        "    device = 'cuda'\n",
        "    epochs = 10\n",
        "    count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
        "    optimizer = optim.SGD(model.parameters(),lr= 0.01, momentum = 0.5)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            # Flatten MNIST images into a 784 long vector\n",
        "            #images = images.view(images.shape[0], -1)\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Training pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #output = model(images)\n",
        "            output = model.forward(images)\n",
        "\n",
        "            #print(output.shape, labels.shape)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #print(train_count)\n",
        "            train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_1(device,criterion,optimizer)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader))\n",
        "            test_losses.append(test_loss/len(testloader))\n",
        "            accuracy.append(acc/len(testloader))\n",
        "        \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "                  \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                  )\n",
        "            \n",
        "        \n",
        "    return -1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur2iZ7UyI9_L",
        "colab_type": "code",
        "outputId": "ad1a2ce6-df63-41ce-cd40-83c36e14a222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        " train_model_1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/15..  Training Loss: 1.481831..  Test Loss: 0.420661..  Test Accuracy: 0.876990\n",
            "Epoch: 2/15..  Training Loss: 0.580260..  Test Loss: 0.250757..  Test Accuracy: 0.924363\n",
            "Epoch: 3/15..  Training Loss: 0.446941..  Test Loss: 0.186798..  Test Accuracy: 0.944367\n",
            "Epoch: 4/15..  Training Loss: 0.378882..  Test Loss: 0.156817..  Test Accuracy: 0.950935\n",
            "Epoch: 5/15..  Training Loss: 0.343157..  Test Loss: 0.144729..  Test Accuracy: 0.955115\n",
            "Epoch: 6/15..  Training Loss: 0.315194..  Test Loss: 0.127703..  Test Accuracy: 0.960191\n",
            "Epoch: 7/15..  Training Loss: 0.292774..  Test Loss: 0.119321..  Test Accuracy: 0.963973\n",
            "Epoch: 8/15..  Training Loss: 0.278124..  Test Loss: 0.107984..  Test Accuracy: 0.968451\n",
            "Epoch: 9/15..  Training Loss: 0.261998..  Test Loss: 0.098294..  Test Accuracy: 0.969447\n",
            "Epoch: 10/15..  Training Loss: 0.253352..  Test Loss: 0.093691..  Test Accuracy: 0.971835\n",
            "Epoch: 11/15..  Training Loss: 0.242978..  Test Loss: 0.092988..  Test Accuracy: 0.973328\n",
            "Epoch: 12/15..  Training Loss: 0.237427..  Test Loss: 0.089859..  Test Accuracy: 0.973726\n",
            "Epoch: 13/15..  Training Loss: 0.231217..  Test Loss: 0.084466..  Test Accuracy: 0.976115\n",
            "Epoch: 14/15..  Training Loss: 0.225284..  Test Loss: 0.081426..  Test Accuracy: 0.975318\n",
            "Epoch: 15/15..  Training Loss: 0.217904..  Test Loss: 0.083075..  Test Accuracy: 0.975119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVmxJlvi7dVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7o96o7TJ6-h",
        "colab_type": "text"
      },
      "source": [
        "# Let's divide train data into 10 private datasets :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOwtJGNiJ6-i",
        "colab_type": "code",
        "outputId": "00964c48-f026-4d03-babf-a12b5078a517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#num_teachers = int(len(train_dataset)/len(test_dataset)) # we're working with  private datasets\n",
        "num_examples = len(train_ds) # the size of OUR dataset\n",
        "num_labels = 10 # number of lablels for our classifier\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "max_ind= num_examples   #50000\n",
        "num_teachers = 10\n",
        "teacher_subset = int(max_ind/num_teachers)\n",
        "\n",
        "print(f\"max_ind = {max_ind}\",f\"teacher_subset = {teacher_subset}\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_ind = 50000 teacher_subset = 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zO5RsOdIiCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(len(test_dataset))\n",
        "#test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size=1)\n",
        "#test_loader\n",
        "\n",
        "#train_loader10 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=teacher_subset, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5J0G1w6J6-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_ds():\n",
        "\n",
        "    max_ind= num_examples  #50000\n",
        "    teacher_len = int(max_ind/num_teachers)\n",
        "\n",
        "    \n",
        "\n",
        "    teacher_train_loader = []\n",
        "    teacher_valid_loader = []\n",
        "\n",
        "    start = 0\n",
        "    stop = teacher_len                          #5000\n",
        "    train_size = int(0.8* teacher_len)          #4000\n",
        "    test_size = teacher_len - train_size        #1000\n",
        "    \n",
        "    print(f\"start = {start}\",f\"train_size = {train_size}\",f\"test_size = {test_size}\")\n",
        "\n",
        "    indicies = torch.randperm(max_ind)\n",
        "\n",
        "    for i in range(num_teachers):\n",
        "        idx =[j for j in range(start,stop)]\n",
        "        idx = indicies[start:stop]\n",
        "        \n",
        "        #split teacher_j dataset into train/validation\n",
        "\n",
        "        \n",
        "        #train_teach_ds, test__teach_ds = torch.utils.data.random_split(train_dataset, [train_size, test_size])\n",
        "        \n",
        "        #split every teacher subset into train/valid\n",
        "        train_t_idx=indicies[start:(start+train_size)]\n",
        "        valid_t_idx = indicies[(start+train_size): stop]\n",
        "\n",
        "        #print(f\"Teacher Train dataset = {start+train_size}\",\n",
        "              #f\"\\nTeacher Valid dataset= {stop}\")\n",
        "    \n",
        "        teacher_train_loader.append(torch.utils.data.DataLoader( train_ds, batch_size=batch_size, sampler = SubsetRandomSampler(train_t_idx)))\n",
        "        teacher_valid_loader.append(torch.utils.data.DataLoader( train_ds, batch_size=batch_size, sampler = SubsetRandomSampler(valid_t_idx)))\n",
        "        \n",
        "        #print(f\"teacher_train_loader = {len(teacher_train_loader[i])}\",f\"teacher_valid_loader = {len(teacher_valid_loader[i])}\")\n",
        "    \n",
        "        start = stop\n",
        "        stop = stop+teacher_len\n",
        "        \n",
        "    return teacher_train_loader, teacher_valid_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8lri8iWJ6-p",
        "colab_type": "code",
        "outputId": "ceeae047-9bdc-470c-b9b2-740f07cbded9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "teacher_train_loader,teacher_valid_loader = split_train_ds()\n",
        "print(len(teacher_train_loader), len(teacher_valid_loader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start = 0 train_size = 4000 test_size = 1000\n",
            "10 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cek7VYhT_4ep",
        "colab_type": "text"
      },
      "source": [
        "# Train model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsjvc2-VC628",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kun97w7Wv6xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_t(device,testloader_t,criterion):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels in testloader_t:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "\n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels)\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc       \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwiligkBwDYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_t(trainloader_t,testloader_t):\n",
        "    \n",
        "    device = 'cuda'\n",
        "    epochs = 12\n",
        "    count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    \n",
        "    #testloader = testloader\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader_t:\n",
        "            # Flatten MNIST images into a 784 long vector\n",
        "            #images = images.view(images.shape[0], -1)\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Training pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #output = model(images)\n",
        "            output = model.forward(images)\n",
        "\n",
        "            #print(output.shape, labels.shape)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #print(train_count)\n",
        "            train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_t(device,testloader_t,criterion)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader_t))\n",
        "            test_losses.append(test_loss/len(testloader_t))\n",
        "            accuracy.append(acc/len(testloader_t))\n",
        "        \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "                  \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                  )\n",
        "            \n",
        "        \n",
        "    return -1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYw_HLlvwsEX",
        "colab_type": "code",
        "outputId": "6b43b3c0-14f4-4a53-be46-e60de2be8b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for t in range(num_teachers):\n",
        "    \n",
        "    print(f\"Teacher {t+1}\")\n",
        "    #instansiate next teacher model\n",
        "    model = Conv()\n",
        "    \n",
        "    #train next teacher model\n",
        "    train_model_t(teacher_train_loader[t],teacher_valid_loader[t])\n",
        "    \n",
        "    #re-initialize model parameters\n",
        "    reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Teacher 1\n",
            "Epoch: 1/12..  Training Loss: 2.009555..  Test Loss: 1.130273..  Test Accuracy: 0.731836\n",
            "Epoch: 2/12..  Training Loss: 1.034178..  Test Loss: 0.535665..  Test Accuracy: 0.855859\n",
            "Epoch: 3/12..  Training Loss: 0.722373..  Test Loss: 0.396389..  Test Accuracy: 0.883398\n",
            "Epoch: 4/12..  Training Loss: 0.612483..  Test Loss: 0.350082..  Test Accuracy: 0.900195\n",
            "Epoch: 5/12..  Training Loss: 0.522144..  Test Loss: 0.313830..  Test Accuracy: 0.907031\n",
            "Epoch: 6/12..  Training Loss: 0.487215..  Test Loss: 0.285592..  Test Accuracy: 0.920117\n",
            "Epoch: 7/12..  Training Loss: 0.448696..  Test Loss: 0.263580..  Test Accuracy: 0.926562\n",
            "Epoch: 8/12..  Training Loss: 0.410505..  Test Loss: 0.262283..  Test Accuracy: 0.922656\n",
            "Epoch: 9/12..  Training Loss: 0.411605..  Test Loss: 0.251336..  Test Accuracy: 0.926953\n",
            "Epoch: 10/12..  Training Loss: 0.357945..  Test Loss: 0.243183..  Test Accuracy: 0.923828\n",
            "Epoch: 11/12..  Training Loss: 0.357115..  Test Loss: 0.232707..  Test Accuracy: 0.929883\n",
            "Epoch: 12/12..  Training Loss: 0.323556..  Test Loss: 0.225863..  Test Accuracy: 0.931836\n",
            "Teacher 2\n",
            "Epoch: 1/12..  Training Loss: 2.005743..  Test Loss: 1.096665..  Test Accuracy: 0.734375\n",
            "Epoch: 2/12..  Training Loss: 1.021844..  Test Loss: 0.480655..  Test Accuracy: 0.870117\n",
            "Epoch: 3/12..  Training Loss: 0.715473..  Test Loss: 0.357610..  Test Accuracy: 0.898242\n",
            "Epoch: 4/12..  Training Loss: 0.587223..  Test Loss: 0.268719..  Test Accuracy: 0.922852\n",
            "Epoch: 5/12..  Training Loss: 0.535589..  Test Loss: 0.234911..  Test Accuracy: 0.935742\n",
            "Epoch: 6/12..  Training Loss: 0.470327..  Test Loss: 0.210046..  Test Accuracy: 0.936914\n",
            "Epoch: 7/12..  Training Loss: 0.447540..  Test Loss: 0.194602..  Test Accuracy: 0.945508\n",
            "Epoch: 8/12..  Training Loss: 0.423203..  Test Loss: 0.175215..  Test Accuracy: 0.951367\n",
            "Epoch: 9/12..  Training Loss: 0.392644..  Test Loss: 0.168319..  Test Accuracy: 0.951563\n",
            "Epoch: 10/12..  Training Loss: 0.366906..  Test Loss: 0.159154..  Test Accuracy: 0.962695\n",
            "Epoch: 11/12..  Training Loss: 0.366268..  Test Loss: 0.143338..  Test Accuracy: 0.960938\n",
            "Epoch: 12/12..  Training Loss: 0.337802..  Test Loss: 0.130942..  Test Accuracy: 0.965430\n",
            "Teacher 3\n",
            "Epoch: 1/12..  Training Loss: 1.954983..  Test Loss: 1.101106..  Test Accuracy: 0.752344\n",
            "Epoch: 2/12..  Training Loss: 1.019922..  Test Loss: 0.523582..  Test Accuracy: 0.856055\n",
            "Epoch: 3/12..  Training Loss: 0.697259..  Test Loss: 0.396642..  Test Accuracy: 0.891016\n",
            "Epoch: 4/12..  Training Loss: 0.624540..  Test Loss: 0.335182..  Test Accuracy: 0.902734\n",
            "Epoch: 5/12..  Training Loss: 0.530721..  Test Loss: 0.293872..  Test Accuracy: 0.917578\n",
            "Epoch: 6/12..  Training Loss: 0.475546..  Test Loss: 0.256668..  Test Accuracy: 0.924219\n",
            "Epoch: 7/12..  Training Loss: 0.456607..  Test Loss: 0.243904..  Test Accuracy: 0.927344\n",
            "Epoch: 8/12..  Training Loss: 0.430102..  Test Loss: 0.222809..  Test Accuracy: 0.935352\n",
            "Epoch: 9/12..  Training Loss: 0.411179..  Test Loss: 0.209538..  Test Accuracy: 0.936328\n",
            "Epoch: 10/12..  Training Loss: 0.375799..  Test Loss: 0.199590..  Test Accuracy: 0.939453\n",
            "Epoch: 11/12..  Training Loss: 0.365302..  Test Loss: 0.197646..  Test Accuracy: 0.936719\n",
            "Epoch: 12/12..  Training Loss: 0.356339..  Test Loss: 0.182016..  Test Accuracy: 0.947070\n",
            "Teacher 4\n",
            "Epoch: 1/12..  Training Loss: 1.983424..  Test Loss: 0.995677..  Test Accuracy: 0.746289\n",
            "Epoch: 2/12..  Training Loss: 0.991065..  Test Loss: 0.478582..  Test Accuracy: 0.890234\n",
            "Epoch: 3/12..  Training Loss: 0.723687..  Test Loss: 0.364047..  Test Accuracy: 0.905273\n",
            "Epoch: 4/12..  Training Loss: 0.601228..  Test Loss: 0.301551..  Test Accuracy: 0.926172\n",
            "Epoch: 5/12..  Training Loss: 0.530076..  Test Loss: 0.287757..  Test Accuracy: 0.929102\n",
            "Epoch: 6/12..  Training Loss: 0.482917..  Test Loss: 0.261125..  Test Accuracy: 0.929883\n",
            "Epoch: 7/12..  Training Loss: 0.466830..  Test Loss: 0.233834..  Test Accuracy: 0.939844\n",
            "Epoch: 8/12..  Training Loss: 0.413023..  Test Loss: 0.223909..  Test Accuracy: 0.937109\n",
            "Epoch: 9/12..  Training Loss: 0.392863..  Test Loss: 0.211153..  Test Accuracy: 0.940820\n",
            "Epoch: 10/12..  Training Loss: 0.383817..  Test Loss: 0.190298..  Test Accuracy: 0.946094\n",
            "Epoch: 11/12..  Training Loss: 0.348994..  Test Loss: 0.199001..  Test Accuracy: 0.942187\n",
            "Epoch: 12/12..  Training Loss: 0.346470..  Test Loss: 0.183129..  Test Accuracy: 0.946484\n",
            "Teacher 5\n",
            "Epoch: 1/12..  Training Loss: 1.965974..  Test Loss: 0.973301..  Test Accuracy: 0.756445\n",
            "Epoch: 2/12..  Training Loss: 1.027243..  Test Loss: 0.518938..  Test Accuracy: 0.871875\n",
            "Epoch: 3/12..  Training Loss: 0.796530..  Test Loss: 0.380495..  Test Accuracy: 0.889844\n",
            "Epoch: 4/12..  Training Loss: 0.664508..  Test Loss: 0.316989..  Test Accuracy: 0.915430\n",
            "Epoch: 5/12..  Training Loss: 0.605358..  Test Loss: 0.283095..  Test Accuracy: 0.927930\n",
            "Epoch: 6/12..  Training Loss: 0.538526..  Test Loss: 0.260332..  Test Accuracy: 0.922656\n",
            "Epoch: 7/12..  Training Loss: 0.506818..  Test Loss: 0.222002..  Test Accuracy: 0.936914\n",
            "Epoch: 8/12..  Training Loss: 0.476395..  Test Loss: 0.220129..  Test Accuracy: 0.937891\n",
            "Epoch: 9/12..  Training Loss: 0.454405..  Test Loss: 0.215355..  Test Accuracy: 0.940625\n",
            "Epoch: 10/12..  Training Loss: 0.426866..  Test Loss: 0.202501..  Test Accuracy: 0.941211\n",
            "Epoch: 11/12..  Training Loss: 0.418477..  Test Loss: 0.199746..  Test Accuracy: 0.942383\n",
            "Epoch: 12/12..  Training Loss: 0.382434..  Test Loss: 0.179880..  Test Accuracy: 0.944531\n",
            "Teacher 6\n",
            "Epoch: 1/12..  Training Loss: 1.965934..  Test Loss: 0.970992..  Test Accuracy: 0.790430\n",
            "Epoch: 2/12..  Training Loss: 1.000624..  Test Loss: 0.474734..  Test Accuracy: 0.875586\n",
            "Epoch: 3/12..  Training Loss: 0.684383..  Test Loss: 0.320791..  Test Accuracy: 0.912305\n",
            "Epoch: 4/12..  Training Loss: 0.564449..  Test Loss: 0.268084..  Test Accuracy: 0.921094\n",
            "Epoch: 5/12..  Training Loss: 0.487875..  Test Loss: 0.236246..  Test Accuracy: 0.927930\n",
            "Epoch: 6/12..  Training Loss: 0.433376..  Test Loss: 0.211007..  Test Accuracy: 0.937305\n",
            "Epoch: 7/12..  Training Loss: 0.407513..  Test Loss: 0.195626..  Test Accuracy: 0.939844\n",
            "Epoch: 8/12..  Training Loss: 0.371037..  Test Loss: 0.173814..  Test Accuracy: 0.940625\n",
            "Epoch: 9/12..  Training Loss: 0.368031..  Test Loss: 0.165638..  Test Accuracy: 0.946094\n",
            "Epoch: 10/12..  Training Loss: 0.342770..  Test Loss: 0.154849..  Test Accuracy: 0.955469\n",
            "Epoch: 11/12..  Training Loss: 0.337600..  Test Loss: 0.157301..  Test Accuracy: 0.950195\n",
            "Epoch: 12/12..  Training Loss: 0.294796..  Test Loss: 0.152315..  Test Accuracy: 0.950781\n",
            "Teacher 7\n",
            "Epoch: 1/12..  Training Loss: 2.094435..  Test Loss: 1.401835..  Test Accuracy: 0.674219\n",
            "Epoch: 2/12..  Training Loss: 1.153211..  Test Loss: 0.583951..  Test Accuracy: 0.833789\n",
            "Epoch: 3/12..  Training Loss: 0.771032..  Test Loss: 0.404443..  Test Accuracy: 0.889063\n",
            "Epoch: 4/12..  Training Loss: 0.617568..  Test Loss: 0.333187..  Test Accuracy: 0.903711\n",
            "Epoch: 5/12..  Training Loss: 0.540293..  Test Loss: 0.293785..  Test Accuracy: 0.913477\n",
            "Epoch: 6/12..  Training Loss: 0.481990..  Test Loss: 0.255005..  Test Accuracy: 0.925977\n",
            "Epoch: 7/12..  Training Loss: 0.435410..  Test Loss: 0.231559..  Test Accuracy: 0.931445\n",
            "Epoch: 8/12..  Training Loss: 0.412696..  Test Loss: 0.213986..  Test Accuracy: 0.933594\n",
            "Epoch: 9/12..  Training Loss: 0.391153..  Test Loss: 0.202184..  Test Accuracy: 0.943555\n",
            "Epoch: 10/12..  Training Loss: 0.372025..  Test Loss: 0.190045..  Test Accuracy: 0.945508\n",
            "Epoch: 11/12..  Training Loss: 0.333575..  Test Loss: 0.189709..  Test Accuracy: 0.942969\n",
            "Epoch: 12/12..  Training Loss: 0.331611..  Test Loss: 0.188223..  Test Accuracy: 0.941602\n",
            "Teacher 8\n",
            "Epoch: 1/12..  Training Loss: 1.982107..  Test Loss: 1.082318..  Test Accuracy: 0.729492\n",
            "Epoch: 2/12..  Training Loss: 1.031788..  Test Loss: 0.489701..  Test Accuracy: 0.866211\n",
            "Epoch: 3/12..  Training Loss: 0.754394..  Test Loss: 0.360781..  Test Accuracy: 0.898242\n",
            "Epoch: 4/12..  Training Loss: 0.589678..  Test Loss: 0.292770..  Test Accuracy: 0.924609\n",
            "Epoch: 5/12..  Training Loss: 0.539615..  Test Loss: 0.255864..  Test Accuracy: 0.930469\n",
            "Epoch: 6/12..  Training Loss: 0.480514..  Test Loss: 0.225178..  Test Accuracy: 0.927930\n",
            "Epoch: 7/12..  Training Loss: 0.425224..  Test Loss: 0.215281..  Test Accuracy: 0.935156\n",
            "Epoch: 8/12..  Training Loss: 0.421451..  Test Loss: 0.202888..  Test Accuracy: 0.936523\n",
            "Epoch: 9/12..  Training Loss: 0.378729..  Test Loss: 0.182476..  Test Accuracy: 0.940039\n",
            "Epoch: 10/12..  Training Loss: 0.360564..  Test Loss: 0.178485..  Test Accuracy: 0.939453\n",
            "Epoch: 11/12..  Training Loss: 0.336957..  Test Loss: 0.152905..  Test Accuracy: 0.954492\n",
            "Epoch: 12/12..  Training Loss: 0.335167..  Test Loss: 0.152910..  Test Accuracy: 0.949609\n",
            "Teacher 9\n",
            "Epoch: 1/12..  Training Loss: 1.988812..  Test Loss: 1.097440..  Test Accuracy: 0.712891\n",
            "Epoch: 2/12..  Training Loss: 1.075506..  Test Loss: 0.524461..  Test Accuracy: 0.850195\n",
            "Epoch: 3/12..  Training Loss: 0.759741..  Test Loss: 0.385123..  Test Accuracy: 0.887695\n",
            "Epoch: 4/12..  Training Loss: 0.631291..  Test Loss: 0.336310..  Test Accuracy: 0.900586\n",
            "Epoch: 5/12..  Training Loss: 0.541278..  Test Loss: 0.286777..  Test Accuracy: 0.907227\n",
            "Epoch: 6/12..  Training Loss: 0.472866..  Test Loss: 0.251949..  Test Accuracy: 0.925000\n",
            "Epoch: 7/12..  Training Loss: 0.444297..  Test Loss: 0.221878..  Test Accuracy: 0.934766\n",
            "Epoch: 8/12..  Training Loss: 0.388776..  Test Loss: 0.198781..  Test Accuracy: 0.932813\n",
            "Epoch: 9/12..  Training Loss: 0.367189..  Test Loss: 0.187484..  Test Accuracy: 0.937891\n",
            "Epoch: 10/12..  Training Loss: 0.361297..  Test Loss: 0.182020..  Test Accuracy: 0.937305\n",
            "Epoch: 11/12..  Training Loss: 0.349152..  Test Loss: 0.164002..  Test Accuracy: 0.945117\n",
            "Epoch: 12/12..  Training Loss: 0.315773..  Test Loss: 0.157490..  Test Accuracy: 0.951563\n",
            "Teacher 10\n",
            "Epoch: 1/12..  Training Loss: 2.008691..  Test Loss: 1.114853..  Test Accuracy: 0.754883\n",
            "Epoch: 2/12..  Training Loss: 1.057100..  Test Loss: 0.564213..  Test Accuracy: 0.836523\n",
            "Epoch: 3/12..  Training Loss: 0.759388..  Test Loss: 0.406937..  Test Accuracy: 0.878320\n",
            "Epoch: 4/12..  Training Loss: 0.623293..  Test Loss: 0.335912..  Test Accuracy: 0.898633\n",
            "Epoch: 5/12..  Training Loss: 0.543769..  Test Loss: 0.290240..  Test Accuracy: 0.916211\n",
            "Epoch: 6/12..  Training Loss: 0.507859..  Test Loss: 0.259657..  Test Accuracy: 0.923633\n",
            "Epoch: 7/12..  Training Loss: 0.443967..  Test Loss: 0.256585..  Test Accuracy: 0.920703\n",
            "Epoch: 8/12..  Training Loss: 0.410927..  Test Loss: 0.217266..  Test Accuracy: 0.933008\n",
            "Epoch: 9/12..  Training Loss: 0.377819..  Test Loss: 0.204383..  Test Accuracy: 0.936719\n",
            "Epoch: 10/12..  Training Loss: 0.328948..  Test Loss: 0.197825..  Test Accuracy: 0.936523\n",
            "Epoch: 11/12..  Training Loss: 0.340229..  Test Loss: 0.196873..  Test Accuracy: 0.940625\n",
            "Epoch: 12/12..  Training Loss: 0.316826..  Test Loss: 0.177604..  Test Accuracy: 0.943750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XhwueKowr06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--L39NRSBSaO",
        "colab_type": "text"
      },
      "source": [
        "# Snippets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQyH0fZsJ6-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = []\n",
        "labels = []\n",
        "j=0\n",
        "\n",
        "for i in range(len(train_targets)):\n",
        "    \n",
        "    images.append(train_data[i])\n",
        "    labels.append(train_targets[i])\n",
        "    \n",
        "    \n",
        "    \n",
        "    if (i+1)%6000==0:\n",
        "        print (\"Teacher \", j)\n",
        "        print(len(images),len(labels))\n",
        "        images.clear()\n",
        "        labels.clear() \n",
        "        j+=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzV-8-raJ6-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = []\n",
        "labels = []\n",
        "image_folder=[]\n",
        "label_folder=[]\n",
        "j=0\n",
        "\n",
        "for i in range(len(train_targets)):\n",
        "    \n",
        "    images.append(train_data[i])\n",
        "    labels.append(train_targets[i])\n",
        "    \n",
        "    if (i+1)%6000==0:\n",
        "        print (j)\n",
        "        image_folder.append(images) \n",
        "        #label_folder[j] = labels\n",
        "        \n",
        "        #images = []\n",
        "        #labels = []\n",
        "        j+=1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ1gS9yoJ6-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier_1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Defining the layers, 128, 64, 10 units each\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        return x\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_6JFiZghDG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset the model\n",
        "def reset_model_2():\n",
        "    model.fc1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc1.bias.data.fill_(0)\n",
        "    model.fc2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc2.bias.data.fill_(0)\n",
        "    model.fc3.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc3.bias.data.fill_(0)\n",
        "    model.fc4.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc4.bias.data.fill_(0)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIdRHXrEgR2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Defining the layers, 128, 64, 10 units each\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        return x\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW4lluvDQ-7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Teacher():\n",
        "  \n",
        "  def __init__(self, loader, dest)\n",
        "  \n",
        "    self.image_loader = loader\n",
        "    self.destination = dest\n",
        "    \n",
        "    batch_size = 60\n",
        "    #n_iters = 1000\n",
        "    #epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "    \n",
        "    self.criterion = nn.NLLLoss()\n",
        "    self.optimaizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "    \n",
        "    \n",
        "  def train(self):\n",
        "    \n",
        "    model.to('cuda')\n",
        "\n",
        "\n",
        "    device = 'cuda'\n",
        "    epochs = 5\n",
        "    steps = 0\n",
        "\n",
        "    train_losses, test_losses = [], []\n",
        "    \n",
        "    \n",
        "    for e in range(epochs):\n",
        "      \n",
        "        running_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        for inputs, labels in trainloader:\n",
        "        \n",
        "            #model.train()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "        \n",
        "            preds = model.forward(inputs)\n",
        "            loss = criterion(preds, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        else:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "        \n",
        "            model.eval()\n",
        "        \n",
        "            # Turn off gradients for validation, saves memory and computations\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in testloader:\n",
        "              \n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                \n",
        "                    log_ps = model.forward(inputs)\n",
        "                    test_loss += criterion(log_ps, labels)\n",
        "                \n",
        "                    ps = torch.exp(log_ps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                \n",
        "            train_losses.append(running_loss/len(trainloader))\n",
        "            test_losses.append(test_loss/len(testloader))\n",
        "        \n",
        "        running_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
        "      \n",
        "      \n",
        "    return accuracy\n",
        "      \n",
        "      \n",
        "  def get_labels():\n",
        "    \n",
        "    return true_labels\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdX0YXcpoOzp",
        "colab_type": "code",
        "outputId": "51d9a16a-0451-4a00-f10b-53bd78a4714c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "images, labels = next(iter(testloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "# Clear the gradients, do this because gradients are accumulated\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Forward pass, then backward pass, then update weights\n",
        "output = model(images)\n",
        "print(output.shape,labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10]) torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtL9FNNkl0bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Defining the layers, 784, 256, 10 units each\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        # Output layer, 10 units - one for each digit\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = self.fc3(x)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x = F.log_softmax(x, dim=1)            #softmax\n",
        "        #print(x.shape)\n",
        "        \n",
        "        return x\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPk_g7N8jvxr",
        "colab_type": "code",
        "outputId": "271971ef-df4f-4106-f53d-5afb88198b32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model = Classifier()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cd4Ue9dSN1e",
        "colab_type": "code",
        "outputId": "acd851e2-aa9b-4f78-fcba-0ba5f3ef2d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    \n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "fc1.weight \t torch.Size([256, 784])\n",
            "fc1.bias \t torch.Size([256])\n",
            "fc2.weight \t torch.Size([64, 256])\n",
            "fc2.bias \t torch.Size([64])\n",
            "fc3.weight \t torch.Size([10, 64])\n",
            "fc3.bias \t torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5-NCPLsK7Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset the model\n",
        "def reset_model_1():\n",
        "    model.fc1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc1.bias.data.fill_(0)\n",
        "    model.fc2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc2.bias.data.fill_(0)\n",
        "    model.fc3.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc3.bias.data.fill_(0)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn24Oj2NMHGX",
        "colab_type": "code",
        "outputId": "8501e94c-4312-4e48-87a1-31a873346a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "params = list(model.parameters())\n",
        "print(len(params))\n",
        "#params"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAelf4GTMqv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch_size = 64\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=64,shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=64,shuffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uofbq0j1KM_S",
        "colab_type": "code",
        "outputId": "e6ea0f85-aaf4-4c70-a324-eafcd891dfd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#train the model on the whole 'MNIST' to tune hyperparameter\n",
        "\n",
        "device = 'cuda'\n",
        "epochs = 7\n",
        "count = 0\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "  \n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    train_count = 0\n",
        "    \n",
        "    for images, labels in trainloader:\n",
        "        # Flatten MNIST images into a 784 long vector\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "    \n",
        "        # TODO: Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #output = model(images)\n",
        "        output = model.forward(images)\n",
        "        \n",
        "        #print(output.shape, labels.shape)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        #print(train_count)\n",
        "        train_count+=1\n",
        "  \n",
        "    else:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        model.eval()\n",
        "        \n",
        "            \n",
        "        with torch.no_grad():\n",
        "            #test_count =0\n",
        "            for inputs, labels in testloader:\n",
        "                \n",
        "                inputs = inputs.view(inputs.shape[0], -1)\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                \n",
        "                log_ps = model.forward(inputs)\n",
        "                \n",
        "                #print(labels.shape,log_ps.shape)\n",
        "                test_loss += criterion(log_ps, labels)\n",
        "                \n",
        "                ps = torch.exp(log_ps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                \n",
        "                #test_count +=1\n",
        "                \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.6f}.. \".format(running_loss/len(trainloader)),\n",
        "              \"Test Loss: {:.6f}.. \".format(test_loss/len(testloader)),\n",
        "              \"Test Accuracy: {:.6f}\".format(accuracy/len(testloader))\n",
        "              )\n",
        "    \n",
        "    #collect train_losses and test_losses for each epoch\n",
        "    #train_losses.append(running_loss/len(trainloader))\n",
        "    #test_losses.append(test_loss/len(testloader))\n",
        "        \n",
        "     \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/7..  Training Loss: 363.900989..  Test Loss: 6.337336..  Test Accuracy: 0.871218\n",
            "Epoch: 2/7..  Training Loss: 3.837848..  Test Loss: 2.177621..  Test Accuracy: 0.903364\n",
            "Epoch: 3/7..  Training Loss: 1.061672..  Test Loss: 0.714465..  Test Accuracy: 0.934415\n",
            "Epoch: 4/7..  Training Loss: 0.374929..  Test Loss: 0.459030..  Test Accuracy: 0.943770\n",
            "Epoch: 5/7..  Training Loss: 0.200605..  Test Loss: 0.259740..  Test Accuracy: 0.955215\n",
            "Epoch: 6/7..  Training Loss: 0.121275..  Test Loss: 0.255535..  Test Accuracy: 0.958002\n",
            "Epoch: 7/7..  Training Loss: 0.113166..  Test Loss: 0.241711..  Test Accuracy: 0.954021\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}