{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Differential_Privacy_Project_v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "EOVp2h8Yr0Ws",
        "UnXn4-vlhxCr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellen008/DeepL/blob/master/Copy_of_Differential_Privacy_Project_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXCUUEMJ6-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "      #++++++++++++\n",
        "      #+++++++++split MNIST train dataset on train.train/train.valid\n",
        "\n",
        "      #+++++++++create 10 teachers' datasets from train.train //train_ds//\n",
        "      #+++++++++split 10 teachers' train.train dataset in train/valid\n",
        "      #output:\n",
        "      #teacher_train_loader = [1 ... 10]\n",
        "      #teacher_valid_loader = [1 ... 10]\n",
        "\n",
        "      #++++++++++++++++++++++++++\n",
        "      #+++++++++train_with_teacher() 10 models on 10 splitted trainsets\n",
        "\n",
        "      #++++++++++++++++++++++++++++   \n",
        "      #10 teachers model.forward(train.valid_dataset) //valid_ds//\n",
        "      #return 10 sets of true_labels = []  and mnist_labels = []\n",
        "\n",
        "      #+++++++++++++++++\n",
        "      #PATE analysis to figure out epsilon\n",
        "      #dp_labels = true_labels + laplacianM(epsilon, beta)\n",
        "\n",
        "      #++++++++++++++++++++++++++++++++\n",
        "      #split train.valid_dataset and true_labels on train/valid\n",
        "      #train.valid.train_loader\n",
        "      #train.valid.valid_loader\n",
        "\n",
        "      #+++++++++++++++++\n",
        "      #train_local_model(train.valid_dataset + dp_labels)\n",
        "\n",
        "      #++++++++++++++++++\n",
        "      #predict(MNIST test_dataset)\n",
        "      #compare predictions accuracy for true labels and dp_labels\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4-CKjx_4bix",
        "colab_type": "text"
      },
      "source": [
        "#Download and transform data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgnjb0HRJ69b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, SubsetRandomSampler, SequentialSampler\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQrGH00KJ69m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download the data\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download = True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "#mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5icqaUjJ69v",
        "colab_type": "text"
      },
      "source": [
        "# Datasets (data + labels):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNC-rhLyJ69y",
        "colab_type": "code",
        "outputId": "79bf03d2-76c7-472f-f014-103edfd11763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7YIf6sTJ697",
        "colab_type": "code",
        "outputId": "80a36117-60d4-43ab-efa8-85cf17da11ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8ZvacWKmHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(dataset, ratio):\n",
        "  \n",
        "    train_size = int(ratio * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "\n",
        "    train_ds, valid_ds = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "  \n",
        "    return train_ds,valid_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adxQOswdXHog",
        "colab_type": "code",
        "outputId": "465a0f16-2513-4873-c748-5bb04b626136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#split train dataset into train.train = train_ds/ train.valid = valid_ds\n",
        "\n",
        "test_train_ds, test_valid_ds = split_dataset(test_dataset,0.95)\n",
        "\n",
        "print(f\"test_train dataset = {len(test_train_ds)}\",\n",
        "      f\"\\ntest_valid dataset= {len(test_valid_ds)}\")\n",
        "print()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_train dataset = 9500 \n",
            "test_valid dataset= 500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOVp2h8Yr0Ws",
        "colab_type": "text"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZaxYDGnl7Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,kernel_size = 5)\n",
        "        self.conv2 = nn.Conv2d(10,20, kernel_size = 5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \n",
        "        x = F.max_pool2d(self.conv1(x),2)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(self.conv2_drop(self.conv2(x)),2)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1,320)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training = self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x,dim = 1)\n",
        "        \n",
        "        return x\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkfaP_C0J6-1",
        "colab_type": "code",
        "outputId": "290eb408-039e-49c5-a444-1cca0ba7e9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model = Conv()\n",
        "#local_model = Conv()\n",
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJg6k6He6hHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset the model\n",
        "def reset_model_conv(model):\n",
        "    model.conv1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.conv1.bias.data.fill_(0)\n",
        "    model.conv2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.conv2.bias.data.fill_(0)\n",
        "    model.fc1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc1.bias.data.fill_(0)\n",
        "    model.fc2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc2.bias.data.fill_(0)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vpljVdZbxri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYxMJPU8bxdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnXn4-vlhxCr",
        "colab_type": "text"
      },
      "source": [
        "# train model_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_QnLjVZ4x6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loaders for training a single model on\n",
        "#train.valid = valid_ds\n",
        "\n",
        "\n",
        "#trainloader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=64,shuffle = True)\n",
        "#testloader = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=64,shuffle = True)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset = p_train_ds,  batch_size=64,shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset = p_valid_ds, batch_size=64,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOlpKPcwrlN",
        "colab_type": "code",
        "outputId": "bc5118d7-ce9f-4dfc-c7ec-ef5c5754c79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#reset conv network:\n",
        "reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2qaE-NJoY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_1(device,criterion,optimizer):\n",
        "    \n",
        "    #device = device\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels in testloader:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "            \n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            \n",
        "            #print(f\"top_p: {top_p}\")\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            #print(f\"equals: {equals}\")\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "            #print(f\"acc: {acc}\")\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Sx4KD-Cgwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_1():\n",
        "    \n",
        "    device = 'cuda'\n",
        "    epochs = 10\n",
        "    count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
        "    optimizer = optim.SGD(model.parameters(),lr= 0.01, momentum = 0.5)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            # Flatten MNIST images into a 784 long vector\n",
        "            #images = images.view(images.shape[0], -1)\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Training pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #output = model(images)\n",
        "            output = model.forward(images)\n",
        "\n",
        "            #print(output.shape, labels.shape)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #print(train_count)\n",
        "            train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_1(device,criterion,optimizer)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader))\n",
        "            test_losses.append(test_loss/len(testloader))\n",
        "            accuracy.append(acc/len(testloader))\n",
        "        \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "                  \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                  )\n",
        "            \n",
        "        \n",
        "    return -1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur2iZ7UyI9_L",
        "colab_type": "code",
        "outputId": "8e67a081-dd2e-4458-d41d-831dfade7dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        " train_model_1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 2.295185..  Test Loss: 2.275266..  Test Accuracy: 0.251919\n",
            "Epoch: 2/10..  Training Loss: 2.201518..  Test Loss: 1.952294..  Test Accuracy: 0.651590\n",
            "Epoch: 3/10..  Training Loss: 1.529691..  Test Loss: 0.764752..  Test Accuracy: 0.827303\n",
            "Epoch: 4/10..  Training Loss: 0.940943..  Test Loss: 0.493012..  Test Accuracy: 0.860746\n",
            "Epoch: 5/10..  Training Loss: 0.744956..  Test Loss: 0.395128..  Test Accuracy: 0.885691\n",
            "Epoch: 6/10..  Training Loss: 0.650084..  Test Loss: 0.332801..  Test Accuracy: 0.894189\n",
            "Epoch: 7/10..  Training Loss: 0.584911..  Test Loss: 0.295193..  Test Accuracy: 0.912007\n",
            "Epoch: 8/10..  Training Loss: 0.529476..  Test Loss: 0.262136..  Test Accuracy: 0.907621\n",
            "Epoch: 9/10..  Training Loss: 0.505346..  Test Loss: 0.244295..  Test Accuracy: 0.924890\n",
            "Epoch: 10/10..  Training Loss: 0.471746..  Test Loss: 0.228448..  Test Accuracy: 0.923246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJKFMP__ta_r",
        "colab_type": "code",
        "outputId": "70d1cb1f-f9e6-433a-ed35-eaed33a8e7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#check predicitions:\n",
        "\n",
        "model.to('cpu')\n",
        "\n",
        "images, labels = next(iter(testloader))\n",
        "# Get the class probabilities\n",
        "ps = torch.exp(model(images))\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)\n",
        "\n",
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "# Look at the most likely classes for the first 10 examples\n",
        "print(top_class[10:15,:])\n",
        "print(top_class.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "tensor([[3],\n",
            "        [4],\n",
            "        [8],\n",
            "        [5],\n",
            "        [5]])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7o96o7TJ6-h",
        "colab_type": "text"
      },
      "source": [
        "# Let's divide train data(60000) into \"*number _of_teachers*\" private datasets :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3uppZ1bbNUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_teacher(ds):\n",
        "\n",
        "    num_teachers = 80\n",
        "    batch_size = 64\n",
        "    \n",
        "    max_ind = len (ds)\n",
        "    print(len(ds))\n",
        "    \n",
        "    teacher_len = int(max_ind/num_teachers)\n",
        "\n",
        "    teacher_loader = []\n",
        "    \n",
        "    start = 0\n",
        "    stop = teacher_len           \n",
        "    \n",
        "       \n",
        "    #print(f\"start = {start}\",f\"train_size = {train_size}\",f\"test_size = {test_size}\")\n",
        "\n",
        "    indicies = torch.randperm(max_ind)\n",
        "    \n",
        "\n",
        "    for i in range(num_teachers):\n",
        "      \n",
        "        idx =[j for j in range(start,stop)]\n",
        "        idx = indicies[start:stop]\n",
        "        \n",
        "        #print(\"Teacher indicies = \",start,\" | \", stop, \" | \",{stop-start})\n",
        "    \n",
        "        teacher_loader.append(torch.utils.data.DataLoader( ds, batch_size=batch_size, sampler = SubsetRandomSampler(idx)))\n",
        "\n",
        "        #print(f\"teacher_loader = {len(teacher_loader[i])}\")\n",
        "    \n",
        "        start = stop\n",
        "        stop = stop+teacher_len\n",
        "        \n",
        "    return  teacher_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1370Ma4ZR8P",
        "colab_type": "code",
        "outputId": "223e4363-21de-4ba6-9856-edcf934b7bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#loaders for teacher training process\n",
        "batch_size = 64\n",
        "\n",
        "teacher_train_loader = split_train_teacher(train_dataset)    #60000/80 = 750 samples per teacher\n",
        "\n",
        "#500 samples  reused for every teacher validation\n",
        "teacher_valid_loader = torch.utils.data.DataLoader( test_valid_ds, batch_size=batch_size, shuffle = True)   \n",
        "\n",
        "print(f\"teacher_train_loader = {len(teacher_train_loader)}\",f\"teacher_valid_loader = {len(teacher_valid_loader)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "teacher_train_loader = 80 teacher_valid_loader = 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf91DG9xGcc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loader for fed_labels production (9500) from test_dataset (10000)\n",
        "\n",
        "batch_size = 64\n",
        "split = 9500\n",
        "idx =[j for j in range(len(test_dataset))]\n",
        "train_id = [j for j in range(split)]\n",
        "test_id = [j for j in range(split,len(test_dataset))]\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, sampler = torch.utils.data.SequentialSampler(train_id))\n",
        "test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, sampler = torch.utils.data.SequentialSampler(test_id))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v3Roi8o-POv",
        "colab_type": "code",
        "outputId": "0d2b8375-0633-4b3c-8f31-b88214477d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_id)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONhjX9YV89EQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46b03e8c-6f81-4819-ff8b-698ebe8b63fb"
      },
      "source": [
        "len(test_id)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZGRTK2qr7QC",
        "colab_type": "code",
        "outputId": "abac9bbe-41f7-4dd4-9a2c-58935e78efb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(valid_loader),len(test_loader))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cek7VYhT_4ep",
        "colab_type": "text"
      },
      "source": [
        "# Train model on 10 teacher datadets \n",
        "#Predict 10 sets of true_labels[ 10 ] on valid dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsjvc2-VC628",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kun97w7Wv6xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_t(device,testloader_t,criterion):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels in testloader_t:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "\n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels)\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            \n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc \n",
        "  \n",
        "\n",
        "def print_test_accuracy(e,epochs,accuracy):\n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                 )\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwiligkBwDYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_t(trainloader_t,testloader_t):\n",
        "    \n",
        "    #device = 'cuda'\n",
        "    epochs = 12\n",
        "    #count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    \n",
        "    #testloader = testloader\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    true_labels = []\n",
        "    dc_valid = []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader_t:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model.forward(images)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_t(device,testloader_t,criterion)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader_t))\n",
        "            test_losses.append(test_loss/len(testloader_t))\n",
        "            accuracy.append(acc/len(testloader_t))\n",
        "        \n",
        "            #print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "             #     \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "             #    \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "             #    \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "             #    )\n",
        "            print_test_accuracy(e,epochs,accuracy) \n",
        "            \n",
        "    return -1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYw_HLlvwsEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train teacher_model and make predictions\n",
        "\n",
        "device = 'cuda'\n",
        "preds = []\n",
        "true_labels = []\n",
        "dc_valid = torch.Tensor(0).cpu()\n",
        "\n",
        "for t in range(num_teachers):\n",
        "    reset_model_conv(model)\n",
        "    print(f\"Teacher {t+1}\")\n",
        "    #instansiate next teacher model\n",
        "    model = Conv()\n",
        "    \n",
        "    \n",
        "    #train next teacher model\n",
        "    train_model_t(teacher_train_loader[t],teacher_valid_loader[t])\n",
        "    \n",
        "    batch_preds = []\n",
        "    batch_count = 0\n",
        "    dc_t = torch.Tensor(0).cpu()\n",
        "    \n",
        "    for images, label in valid_loader:\n",
        "        \n",
        "        #images, labels = images.to(device), labels.to(device)\n",
        "        images = images.to(device)\n",
        "        true_labels.append(label)\n",
        "        \n",
        "        ps = torch.exp(model.forward(images))\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        \n",
        "        #batch_preds.append(top_class.flatten())\n",
        "        dc_batch = top_class.flatten().float().cpu()        \n",
        "        dc_t = torch.cat((dc_t,dc_batch),0).view(-1)\n",
        "        \n",
        "\n",
        "    #preds.append(np.array(batch_preds))\n",
        "    dc_valid = torch.cat((dc_valid,dc_t),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4lsb8CnkXd",
        "colab_type": "text"
      },
      "source": [
        "#train teachers and produce federated labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY0q9QfqcVND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_t_labels(device,testloader_t,criterion):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    #v_count = 0\n",
        "    \n",
        "    #dc_t = torch.Tensor(0).cpu()\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "        for inputs, labels in testloader_t:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "\n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels)\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "          \n",
        "    return test_loss, acc\n",
        "  \n",
        "\n",
        "def print_test_accuracy(e,epochs,accuracy):\n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                 )\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuOXL8OTmVft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_t_labels(trainloader_t,testloader_t,valid_loader):\n",
        "    \n",
        "    #device = 'cuda'\n",
        "    epochs = 12\n",
        "    #count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader_t:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model.forward(images)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_t_labels(device,testloader_t,criterion)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader_t))\n",
        "            test_losses.append(test_loss/len(testloader_t))\n",
        "            accuracy.append(acc/len(testloader_t))\n",
        "            \n",
        "            \n",
        "        \n",
        "            #print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "             #     \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "             #    \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "             #    \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "             #    )\n",
        "            print_test_accuracy(e,epochs,accuracy)\n",
        "            \n",
        "    #produce true_labels\n",
        "    \n",
        "    acc_teacher = 0\n",
        "    v_count = 0\n",
        "    labels_teacher = torch.Tensor(0).cpu()\n",
        "    ds_labels = torch.FloatTensor(0).cpu()\n",
        "    \n",
        "    #preserve sequence of images,labels in valid_loader\n",
        "    imgs = []\n",
        "    labs = []\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "        for inputs, labels in valid_loader:\n",
        "            #print(\"batch # \",v_count)\n",
        "            \n",
        "            #preserve the sequence of images and labels\n",
        "            imgs.extend(inputs)\n",
        "            labs.extend(labels.flatten().numpy())\n",
        "            \n",
        "            \n",
        "            \n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "                                                 #print(labels.shape,log_ps.shape)\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc_teacher += torch.mean(equals.type(torch.FloatTensor))\n",
        "            \n",
        "            #collect original MNIST labels\n",
        "            ds_labels = torch.cat((ds_labels,labels.float().cpu()),0).view(-1)\n",
        "            #print(\"dataset_labels \",len(ds_labels))\n",
        "            \n",
        "            \n",
        "            #Federated learning labels batch tensor\n",
        "            labels_batch = top_class.flatten().float().cpu()\n",
        "            #print(\"dc_batch.size \", dc_batch.size())\n",
        "            labels_teacher = torch.cat((labels_teacher,labels_batch),0).view(-1)\n",
        "            #print(\"labels_teacher.size \",labels_teacher.size())\n",
        "          \n",
        "            \n",
        "            v_count +=1\n",
        "          \n",
        "    #return labels_teacher, acc_teacher, mnist_labels\n",
        "    \n",
        "    \n",
        "            \n",
        "    return labels_teacher, ds_labels, acc_teacher, imgs, labs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2FqlvjWu0jF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afc72085-1770-4eaa-e98d-14cf94e09061"
      },
      "source": [
        "num_teachers = 80\n",
        "device = 'cuda'\n",
        "#true_labels = torch.Tensor(0).cpu()\n",
        "true_labels = []\n",
        "mnist_labels = []\n",
        "accuracy_teacher = []\n",
        "\n",
        "for t in range(num_teachers):         \n",
        "    reset_model_conv(model)\n",
        "    \n",
        "    print(f\"Teacher {t}\")\n",
        "    #instansiate next teacher model\n",
        "    model = Conv()\n",
        "    \n",
        "    #train next teacher model\n",
        "    #labels_teacher, ds_labels, acc_teacher = train_model_t_labels(teacher_train_loader[t],teacher_valid_loader[t])\n",
        "    labels_teacher, ds_labels, acc_teacher, imgs, labs = train_model_t_labels(teacher_train_loader[t], teacher_valid_loader,valid_loader)\n",
        "    \n",
        "    #collect true_labels, mnist_labels for the teacher\n",
        "    true_labels.append(labels_teacher.cpu())\n",
        "    mnist_labels.append(ds_labels.cpu())\n",
        "    accuracy_teacher.append(acc_teacher.cpu())\n",
        "    \n",
        "#save sequence of images for local_model training\n",
        "\n",
        "#imgs_iter = itertools.chain(imgs)\n",
        "#labs_iter = itertools.chain(labs)\n",
        "    #print(f\"Teacher {t} has been trained\\n\\n\")\n",
        "    \n",
        "print(f\"All {t+1} Teachers  has been trained\\n\\n\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Teacher 0\n",
            "Epoch: 1/12..  Test Accuracy: 0.093600\n",
            "Epoch: 2/12..  Test Accuracy: 0.300481\n",
            "Epoch: 3/12..  Test Accuracy: 0.534105\n",
            "Epoch: 4/12..  Test Accuracy: 0.678936\n",
            "Epoch: 5/12..  Test Accuracy: 0.733924\n",
            "Epoch: 6/12..  Test Accuracy: 0.796875\n",
            "Epoch: 7/12..  Test Accuracy: 0.860577\n",
            "Epoch: 8/12..  Test Accuracy: 0.865535\n",
            "Epoch: 9/12..  Test Accuracy: 0.901593\n",
            "Epoch: 10/12..  Test Accuracy: 0.899639\n",
            "Epoch: 11/12..  Test Accuracy: 0.897686\n",
            "Epoch: 12/12..  Test Accuracy: 0.911809\n",
            "Teacher 1\n",
            "Epoch: 1/12..  Test Accuracy: 0.118990\n",
            "Epoch: 2/12..  Test Accuracy: 0.282752\n",
            "Epoch: 3/12..  Test Accuracy: 0.437200\n",
            "Epoch: 4/12..  Test Accuracy: 0.644982\n",
            "Epoch: 5/12..  Test Accuracy: 0.715895\n",
            "Epoch: 6/12..  Test Accuracy: 0.797776\n",
            "Epoch: 7/12..  Test Accuracy: 0.817157\n",
            "Epoch: 8/12..  Test Accuracy: 0.859225\n",
            "Epoch: 9/12..  Test Accuracy: 0.828576\n",
            "Epoch: 10/12..  Test Accuracy: 0.872897\n",
            "Epoch: 11/12..  Test Accuracy: 0.856220\n",
            "Epoch: 12/12..  Test Accuracy: 0.883564\n",
            "Teacher 2\n",
            "Epoch: 1/12..  Test Accuracy: 0.230919\n",
            "Epoch: 2/12..  Test Accuracy: 0.449669\n",
            "Epoch: 3/12..  Test Accuracy: 0.609375\n",
            "Epoch: 4/12..  Test Accuracy: 0.711388\n",
            "Epoch: 5/12..  Test Accuracy: 0.760216\n",
            "Epoch: 6/12..  Test Accuracy: 0.787560\n",
            "Epoch: 7/12..  Test Accuracy: 0.864032\n",
            "Epoch: 8/12..  Test Accuracy: 0.875300\n",
            "Epoch: 9/12..  Test Accuracy: 0.863582\n",
            "Epoch: 10/12..  Test Accuracy: 0.885066\n",
            "Epoch: 11/12..  Test Accuracy: 0.896635\n",
            "Epoch: 12/12..  Test Accuracy: 0.886568\n",
            "Teacher 3\n",
            "Epoch: 1/12..  Test Accuracy: 0.114934\n",
            "Epoch: 2/12..  Test Accuracy: 0.386869\n",
            "Epoch: 3/12..  Test Accuracy: 0.566106\n",
            "Epoch: 4/12..  Test Accuracy: 0.749850\n",
            "Epoch: 5/12..  Test Accuracy: 0.777945\n",
            "Epoch: 6/12..  Test Accuracy: 0.815355\n",
            "Epoch: 7/12..  Test Accuracy: 0.837139\n",
            "Epoch: 8/12..  Test Accuracy: 0.864032\n",
            "Epoch: 9/12..  Test Accuracy: 0.867488\n",
            "Epoch: 10/12..  Test Accuracy: 0.885066\n",
            "Epoch: 11/12..  Test Accuracy: 0.891376\n",
            "Epoch: 12/12..  Test Accuracy: 0.900090\n",
            "Teacher 4\n",
            "Epoch: 1/12..  Test Accuracy: 0.127704\n",
            "Epoch: 2/12..  Test Accuracy: 0.580829\n",
            "Epoch: 3/12..  Test Accuracy: 0.725060\n",
            "Epoch: 4/12..  Test Accuracy: 0.761719\n",
            "Epoch: 5/12..  Test Accuracy: 0.779447\n",
            "Epoch: 6/12..  Test Accuracy: 0.817157\n",
            "Epoch: 7/12..  Test Accuracy: 0.839243\n",
            "Epoch: 8/12..  Test Accuracy: 0.864934\n",
            "Epoch: 9/12..  Test Accuracy: 0.872746\n",
            "Epoch: 10/12..  Test Accuracy: 0.876803\n",
            "Epoch: 11/12..  Test Accuracy: 0.888522\n",
            "Epoch: 12/12..  Test Accuracy: 0.897085\n",
            "Teacher 5\n",
            "Epoch: 1/12..  Test Accuracy: 0.281250\n",
            "Epoch: 2/12..  Test Accuracy: 0.349309\n",
            "Epoch: 3/12..  Test Accuracy: 0.541767\n",
            "Epoch: 4/12..  Test Accuracy: 0.650691\n",
            "Epoch: 5/12..  Test Accuracy: 0.763522\n",
            "Epoch: 6/12..  Test Accuracy: 0.823017\n",
            "Epoch: 7/12..  Test Accuracy: 0.849309\n",
            "Epoch: 8/12..  Test Accuracy: 0.858774\n",
            "Epoch: 9/12..  Test Accuracy: 0.872446\n",
            "Epoch: 10/12..  Test Accuracy: 0.875751\n",
            "Epoch: 11/12..  Test Accuracy: 0.890024\n",
            "Epoch: 12/12..  Test Accuracy: 0.894682\n",
            "Teacher 6\n",
            "Epoch: 1/12..  Test Accuracy: 0.215895\n",
            "Epoch: 2/12..  Test Accuracy: 0.477013\n",
            "Epoch: 3/12..  Test Accuracy: 0.485577\n",
            "Epoch: 4/12..  Test Accuracy: 0.602764\n",
            "Epoch: 5/12..  Test Accuracy: 0.772386\n",
            "Epoch: 6/12..  Test Accuracy: 0.837740\n",
            "Epoch: 7/12..  Test Accuracy: 0.848558\n",
            "Epoch: 8/12..  Test Accuracy: 0.860577\n",
            "Epoch: 9/12..  Test Accuracy: 0.896635\n",
            "Epoch: 10/12..  Test Accuracy: 0.900541\n",
            "Epoch: 11/12..  Test Accuracy: 0.901593\n",
            "Epoch: 12/12..  Test Accuracy: 0.900240\n",
            "Teacher 7\n",
            "Epoch: 1/12..  Test Accuracy: 0.231971\n",
            "Epoch: 2/12..  Test Accuracy: 0.443359\n",
            "Epoch: 3/12..  Test Accuracy: 0.704026\n",
            "Epoch: 4/12..  Test Accuracy: 0.816857\n",
            "Epoch: 5/12..  Test Accuracy: 0.824068\n",
            "Epoch: 6/12..  Test Accuracy: 0.850811\n",
            "Epoch: 7/12..  Test Accuracy: 0.865986\n",
            "Epoch: 8/12..  Test Accuracy: 0.875751\n",
            "Epoch: 9/12..  Test Accuracy: 0.881460\n",
            "Epoch: 10/12..  Test Accuracy: 0.888371\n",
            "Epoch: 11/12..  Test Accuracy: 0.900541\n",
            "Epoch: 12/12..  Test Accuracy: 0.905950\n",
            "Teacher 8\n",
            "Epoch: 1/12..  Test Accuracy: 0.280649\n",
            "Epoch: 2/12..  Test Accuracy: 0.373798\n",
            "Epoch: 3/12..  Test Accuracy: 0.504357\n",
            "Epoch: 4/12..  Test Accuracy: 0.660307\n",
            "Epoch: 5/12..  Test Accuracy: 0.686148\n",
            "Epoch: 6/12..  Test Accuracy: 0.787560\n",
            "Epoch: 7/12..  Test Accuracy: 0.802133\n",
            "Epoch: 8/12..  Test Accuracy: 0.824068\n",
            "Epoch: 9/12..  Test Accuracy: 0.853816\n",
            "Epoch: 10/12..  Test Accuracy: 0.856671\n",
            "Epoch: 11/12..  Test Accuracy: 0.857121\n",
            "Epoch: 12/12..  Test Accuracy: 0.871244\n",
            "Teacher 9\n",
            "Epoch: 1/12..  Test Accuracy: 0.232422\n",
            "Epoch: 2/12..  Test Accuracy: 0.202374\n",
            "Epoch: 3/12..  Test Accuracy: 0.551082\n",
            "Epoch: 4/12..  Test Accuracy: 0.650541\n",
            "Epoch: 5/12..  Test Accuracy: 0.759766\n",
            "Epoch: 6/12..  Test Accuracy: 0.832933\n",
            "Epoch: 7/12..  Test Accuracy: 0.845703\n",
            "Epoch: 8/12..  Test Accuracy: 0.866887\n",
            "Epoch: 9/12..  Test Accuracy: 0.870343\n",
            "Epoch: 10/12..  Test Accuracy: 0.889273\n",
            "Epoch: 11/12..  Test Accuracy: 0.895282\n",
            "Epoch: 12/12..  Test Accuracy: 0.907903\n",
            "Teacher 10\n",
            "Epoch: 1/12..  Test Accuracy: 0.081881\n",
            "Epoch: 2/12..  Test Accuracy: 0.202674\n",
            "Epoch: 3/12..  Test Accuracy: 0.508714\n",
            "Epoch: 4/12..  Test Accuracy: 0.621544\n",
            "Epoch: 5/12..  Test Accuracy: 0.703726\n",
            "Epoch: 6/12..  Test Accuracy: 0.785006\n",
            "Epoch: 7/12..  Test Accuracy: 0.816707\n",
            "Epoch: 8/12..  Test Accuracy: 0.842097\n",
            "Epoch: 9/12..  Test Accuracy: 0.863131\n",
            "Epoch: 10/12..  Test Accuracy: 0.867037\n",
            "Epoch: 11/12..  Test Accuracy: 0.870793\n",
            "Epoch: 12/12..  Test Accuracy: 0.886418\n",
            "Teacher 11\n",
            "Epoch: 1/12..  Test Accuracy: 0.103365\n",
            "Epoch: 2/12..  Test Accuracy: 0.379056\n",
            "Epoch: 3/12..  Test Accuracy: 0.547776\n",
            "Epoch: 4/12..  Test Accuracy: 0.694862\n",
            "Epoch: 5/12..  Test Accuracy: 0.734225\n",
            "Epoch: 6/12..  Test Accuracy: 0.821064\n",
            "Epoch: 7/12..  Test Accuracy: 0.835787\n",
            "Epoch: 8/12..  Test Accuracy: 0.846004\n",
            "Epoch: 9/12..  Test Accuracy: 0.853365\n",
            "Epoch: 10/12..  Test Accuracy: 0.870343\n",
            "Epoch: 11/12..  Test Accuracy: 0.887019\n",
            "Epoch: 12/12..  Test Accuracy: 0.874700\n",
            "Teacher 12\n",
            "Epoch: 1/12..  Test Accuracy: 0.222806\n",
            "Epoch: 2/12..  Test Accuracy: 0.234826\n",
            "Epoch: 3/12..  Test Accuracy: 0.624249\n",
            "Epoch: 4/12..  Test Accuracy: 0.758413\n",
            "Epoch: 5/12..  Test Accuracy: 0.827825\n",
            "Epoch: 6/12..  Test Accuracy: 0.862981\n",
            "Epoch: 7/12..  Test Accuracy: 0.874249\n",
            "Epoch: 8/12..  Test Accuracy: 0.889874\n",
            "Epoch: 9/12..  Test Accuracy: 0.894682\n",
            "Epoch: 10/12..  Test Accuracy: 0.900541\n",
            "Epoch: 11/12..  Test Accuracy: 0.900090\n",
            "Epoch: 12/12..  Test Accuracy: 0.916166\n",
            "Teacher 13\n",
            "Epoch: 1/12..  Test Accuracy: 0.140775\n",
            "Epoch: 2/12..  Test Accuracy: 0.417518\n",
            "Epoch: 3/12..  Test Accuracy: 0.538462\n",
            "Epoch: 4/12..  Test Accuracy: 0.696965\n",
            "Epoch: 5/12..  Test Accuracy: 0.769081\n",
            "Epoch: 6/12..  Test Accuracy: 0.794171\n",
            "Epoch: 7/12..  Test Accuracy: 0.818359\n",
            "Epoch: 8/12..  Test Accuracy: 0.838792\n",
            "Epoch: 9/12..  Test Accuracy: 0.864483\n",
            "Epoch: 10/12..  Test Accuracy: 0.870793\n",
            "Epoch: 11/12..  Test Accuracy: 0.868389\n",
            "Epoch: 12/12..  Test Accuracy: 0.899038\n",
            "Teacher 14\n",
            "Epoch: 1/12..  Test Accuracy: 0.150992\n",
            "Epoch: 2/12..  Test Accuracy: 0.384014\n",
            "Epoch: 3/12..  Test Accuracy: 0.504958\n",
            "Epoch: 4/12..  Test Accuracy: 0.618840\n",
            "Epoch: 5/12..  Test Accuracy: 0.764724\n",
            "Epoch: 6/12..  Test Accuracy: 0.797626\n",
            "Epoch: 7/12..  Test Accuracy: 0.827374\n",
            "Epoch: 8/12..  Test Accuracy: 0.843900\n",
            "Epoch: 9/12..  Test Accuracy: 0.847656\n",
            "Epoch: 10/12..  Test Accuracy: 0.882212\n",
            "Epoch: 11/12..  Test Accuracy: 0.899639\n",
            "Epoch: 12/12..  Test Accuracy: 0.891376\n",
            "Teacher 15\n",
            "Epoch: 1/12..  Test Accuracy: 0.169772\n",
            "Epoch: 2/12..  Test Accuracy: 0.432692\n",
            "Epoch: 3/12..  Test Accuracy: 0.580228\n",
            "Epoch: 4/12..  Test Accuracy: 0.631010\n",
            "Epoch: 5/12..  Test Accuracy: 0.725811\n",
            "Epoch: 6/12..  Test Accuracy: 0.807392\n",
            "Epoch: 7/12..  Test Accuracy: 0.812200\n",
            "Epoch: 8/12..  Test Accuracy: 0.839393\n",
            "Epoch: 9/12..  Test Accuracy: 0.866887\n",
            "Epoch: 10/12..  Test Accuracy: 0.868840\n",
            "Epoch: 11/12..  Test Accuracy: 0.882963\n",
            "Epoch: 12/12..  Test Accuracy: 0.886268\n",
            "Teacher 16\n",
            "Epoch: 1/12..  Test Accuracy: 0.176833\n",
            "Epoch: 2/12..  Test Accuracy: 0.286959\n",
            "Epoch: 3/12..  Test Accuracy: 0.562350\n",
            "Epoch: 4/12..  Test Accuracy: 0.645433\n",
            "Epoch: 5/12..  Test Accuracy: 0.780048\n",
            "Epoch: 6/12..  Test Accuracy: 0.786659\n",
            "Epoch: 7/12..  Test Accuracy: 0.862530\n",
            "Epoch: 8/12..  Test Accuracy: 0.861178\n",
            "Epoch: 9/12..  Test Accuracy: 0.871394\n",
            "Epoch: 10/12..  Test Accuracy: 0.876202\n",
            "Epoch: 11/12..  Test Accuracy: 0.878756\n",
            "Epoch: 12/12..  Test Accuracy: 0.891376\n",
            "Teacher 17\n",
            "Epoch: 1/12..  Test Accuracy: 0.174429\n",
            "Epoch: 2/12..  Test Accuracy: 0.365084\n",
            "Epoch: 3/12..  Test Accuracy: 0.561599\n",
            "Epoch: 4/12..  Test Accuracy: 0.684044\n",
            "Epoch: 5/12..  Test Accuracy: 0.742037\n",
            "Epoch: 6/12..  Test Accuracy: 0.821064\n",
            "Epoch: 7/12..  Test Accuracy: 0.856220\n",
            "Epoch: 8/12..  Test Accuracy: 0.878456\n",
            "Epoch: 9/12..  Test Accuracy: 0.881611\n",
            "Epoch: 10/12..  Test Accuracy: 0.885367\n",
            "Epoch: 11/12..  Test Accuracy: 0.884165\n",
            "Epoch: 12/12..  Test Accuracy: 0.889123\n",
            "Teacher 18\n",
            "Epoch: 1/12..  Test Accuracy: 0.122746\n",
            "Epoch: 2/12..  Test Accuracy: 0.273738\n",
            "Epoch: 3/12..  Test Accuracy: 0.439453\n",
            "Epoch: 4/12..  Test Accuracy: 0.699369\n",
            "Epoch: 5/12..  Test Accuracy: 0.775240\n",
            "Epoch: 6/12..  Test Accuracy: 0.803185\n",
            "Epoch: 7/12..  Test Accuracy: 0.851412\n",
            "Epoch: 8/12..  Test Accuracy: 0.884465\n",
            "Epoch: 9/12..  Test Accuracy: 0.894682\n",
            "Epoch: 10/12..  Test Accuracy: 0.903546\n",
            "Epoch: 11/12..  Test Accuracy: 0.898588\n",
            "Epoch: 12/12..  Test Accuracy: 0.905349\n",
            "Teacher 19\n",
            "Epoch: 1/12..  Test Accuracy: 0.228215\n",
            "Epoch: 2/12..  Test Accuracy: 0.289964\n",
            "Epoch: 3/12..  Test Accuracy: 0.624549\n",
            "Epoch: 4/12..  Test Accuracy: 0.705078\n",
            "Epoch: 5/12..  Test Accuracy: 0.809495\n",
            "Epoch: 6/12..  Test Accuracy: 0.808443\n",
            "Epoch: 7/12..  Test Accuracy: 0.855769\n",
            "Epoch: 8/12..  Test Accuracy: 0.859075\n",
            "Epoch: 9/12..  Test Accuracy: 0.877254\n",
            "Epoch: 10/12..  Test Accuracy: 0.876953\n",
            "Epoch: 11/12..  Test Accuracy: 0.879958\n",
            "Epoch: 12/12..  Test Accuracy: 0.885968\n",
            "Teacher 20\n",
            "Epoch: 1/12..  Test Accuracy: 0.114483\n",
            "Epoch: 2/12..  Test Accuracy: 0.287560\n",
            "Epoch: 3/12..  Test Accuracy: 0.593450\n",
            "Epoch: 4/12..  Test Accuracy: 0.611328\n",
            "Epoch: 5/12..  Test Accuracy: 0.697266\n",
            "Epoch: 6/12..  Test Accuracy: 0.762620\n",
            "Epoch: 7/12..  Test Accuracy: 0.772085\n",
            "Epoch: 8/12..  Test Accuracy: 0.838642\n",
            "Epoch: 9/12..  Test Accuracy: 0.861629\n",
            "Epoch: 10/12..  Test Accuracy: 0.854267\n",
            "Epoch: 11/12..  Test Accuracy: 0.874850\n",
            "Epoch: 12/12..  Test Accuracy: 0.871995\n",
            "Teacher 21\n",
            "Epoch: 1/12..  Test Accuracy: 0.246094\n",
            "Epoch: 2/12..  Test Accuracy: 0.453726\n",
            "Epoch: 3/12..  Test Accuracy: 0.551833\n",
            "Epoch: 4/12..  Test Accuracy: 0.650090\n",
            "Epoch: 5/12..  Test Accuracy: 0.668419\n",
            "Epoch: 6/12..  Test Accuracy: 0.784105\n",
            "Epoch: 7/12..  Test Accuracy: 0.793419\n",
            "Epoch: 8/12..  Test Accuracy: 0.830379\n",
            "Epoch: 9/12..  Test Accuracy: 0.846004\n",
            "Epoch: 10/12..  Test Accuracy: 0.860577\n",
            "Epoch: 11/12..  Test Accuracy: 0.869892\n",
            "Epoch: 12/12..  Test Accuracy: 0.863432\n",
            "Teacher 22\n",
            "Epoch: 1/12..  Test Accuracy: 0.185246\n",
            "Epoch: 2/12..  Test Accuracy: 0.347055\n",
            "Epoch: 3/12..  Test Accuracy: 0.493690\n",
            "Epoch: 4/12..  Test Accuracy: 0.607873\n",
            "Epoch: 5/12..  Test Accuracy: 0.723107\n",
            "Epoch: 6/12..  Test Accuracy: 0.777344\n",
            "Epoch: 7/12..  Test Accuracy: 0.848257\n",
            "Epoch: 8/12..  Test Accuracy: 0.845403\n",
            "Epoch: 9/12..  Test Accuracy: 0.864032\n",
            "Epoch: 10/12..  Test Accuracy: 0.867488\n",
            "Epoch: 11/12..  Test Accuracy: 0.882963\n",
            "Epoch: 12/12..  Test Accuracy: 0.896635\n",
            "Teacher 23\n",
            "Epoch: 1/12..  Test Accuracy: 0.184796\n",
            "Epoch: 2/12..  Test Accuracy: 0.332482\n",
            "Epoch: 3/12..  Test Accuracy: 0.598858\n",
            "Epoch: 4/12..  Test Accuracy: 0.695312\n",
            "Epoch: 5/12..  Test Accuracy: 0.765024\n",
            "Epoch: 6/12..  Test Accuracy: 0.816256\n",
            "Epoch: 7/12..  Test Accuracy: 0.839093\n",
            "Epoch: 8/12..  Test Accuracy: 0.849008\n",
            "Epoch: 9/12..  Test Accuracy: 0.882963\n",
            "Epoch: 10/12..  Test Accuracy: 0.875751\n",
            "Epoch: 11/12..  Test Accuracy: 0.891376\n",
            "Epoch: 12/12..  Test Accuracy: 0.901142\n",
            "Teacher 24\n",
            "Epoch: 1/12..  Test Accuracy: 0.323017\n",
            "Epoch: 2/12..  Test Accuracy: 0.400541\n",
            "Epoch: 3/12..  Test Accuracy: 0.495643\n",
            "Epoch: 4/12..  Test Accuracy: 0.588792\n",
            "Epoch: 5/12..  Test Accuracy: 0.721755\n",
            "Epoch: 6/12..  Test Accuracy: 0.777344\n",
            "Epoch: 7/12..  Test Accuracy: 0.799880\n",
            "Epoch: 8/12..  Test Accuracy: 0.837290\n",
            "Epoch: 9/12..  Test Accuracy: 0.850511\n",
            "Epoch: 10/12..  Test Accuracy: 0.843149\n",
            "Epoch: 11/12..  Test Accuracy: 0.875300\n",
            "Epoch: 12/12..  Test Accuracy: 0.884916\n",
            "Teacher 25\n",
            "Epoch: 1/12..  Test Accuracy: 0.281100\n",
            "Epoch: 2/12..  Test Accuracy: 0.400992\n",
            "Epoch: 3/12..  Test Accuracy: 0.473107\n",
            "Epoch: 4/12..  Test Accuracy: 0.716647\n",
            "Epoch: 5/12..  Test Accuracy: 0.733474\n",
            "Epoch: 6/12..  Test Accuracy: 0.800932\n",
            "Epoch: 7/12..  Test Accuracy: 0.818810\n",
            "Epoch: 8/12..  Test Accuracy: 0.834285\n",
            "Epoch: 9/12..  Test Accuracy: 0.876653\n",
            "Epoch: 10/12..  Test Accuracy: 0.887470\n",
            "Epoch: 11/12..  Test Accuracy: 0.882963\n",
            "Epoch: 12/12..  Test Accuracy: 0.897236\n",
            "Teacher 26\n",
            "Epoch: 1/12..  Test Accuracy: 0.135968\n",
            "Epoch: 2/12..  Test Accuracy: 0.385367\n",
            "Epoch: 3/12..  Test Accuracy: 0.554688\n",
            "Epoch: 4/12..  Test Accuracy: 0.702374\n",
            "Epoch: 5/12..  Test Accuracy: 0.790865\n",
            "Epoch: 6/12..  Test Accuracy: 0.803185\n",
            "Epoch: 7/12..  Test Accuracy: 0.831881\n",
            "Epoch: 8/12..  Test Accuracy: 0.847356\n",
            "Epoch: 9/12..  Test Accuracy: 0.860577\n",
            "Epoch: 10/12..  Test Accuracy: 0.875300\n",
            "Epoch: 11/12..  Test Accuracy: 0.879507\n",
            "Epoch: 12/12..  Test Accuracy: 0.885517\n",
            "Teacher 27\n",
            "Epoch: 1/12..  Test Accuracy: 0.168570\n",
            "Epoch: 2/12..  Test Accuracy: 0.330228\n",
            "Epoch: 3/12..  Test Accuracy: 0.634766\n",
            "Epoch: 4/12..  Test Accuracy: 0.697266\n",
            "Epoch: 5/12..  Test Accuracy: 0.731971\n",
            "Epoch: 6/12..  Test Accuracy: 0.794922\n",
            "Epoch: 7/12..  Test Accuracy: 0.837290\n",
            "Epoch: 8/12..  Test Accuracy: 0.856070\n",
            "Epoch: 9/12..  Test Accuracy: 0.856671\n",
            "Epoch: 10/12..  Test Accuracy: 0.883564\n",
            "Epoch: 11/12..  Test Accuracy: 0.889423\n",
            "Epoch: 12/12..  Test Accuracy: 0.889874\n",
            "Teacher 28\n",
            "Epoch: 1/12..  Test Accuracy: 0.129056\n",
            "Epoch: 2/12..  Test Accuracy: 0.550331\n",
            "Epoch: 3/12..  Test Accuracy: 0.607722\n",
            "Epoch: 4/12..  Test Accuracy: 0.667218\n",
            "Epoch: 5/12..  Test Accuracy: 0.789964\n",
            "Epoch: 6/12..  Test Accuracy: 0.846454\n",
            "Epoch: 7/12..  Test Accuracy: 0.854267\n",
            "Epoch: 8/12..  Test Accuracy: 0.872296\n",
            "Epoch: 9/12..  Test Accuracy: 0.875300\n",
            "Epoch: 10/12..  Test Accuracy: 0.882061\n",
            "Epoch: 11/12..  Test Accuracy: 0.878155\n",
            "Epoch: 12/12..  Test Accuracy: 0.895132\n",
            "Teacher 29\n",
            "Epoch: 1/12..  Test Accuracy: 0.151142\n",
            "Epoch: 2/12..  Test Accuracy: 0.286058\n",
            "Epoch: 3/12..  Test Accuracy: 0.437500\n",
            "Epoch: 4/12..  Test Accuracy: 0.598858\n",
            "Epoch: 5/12..  Test Accuracy: 0.664213\n",
            "Epoch: 6/12..  Test Accuracy: 0.749850\n",
            "Epoch: 7/12..  Test Accuracy: 0.819261\n",
            "Epoch: 8/12..  Test Accuracy: 0.864633\n",
            "Epoch: 9/12..  Test Accuracy: 0.846154\n",
            "Epoch: 10/12..  Test Accuracy: 0.888822\n",
            "Epoch: 11/12..  Test Accuracy: 0.898738\n",
            "Epoch: 12/12..  Test Accuracy: 0.887921\n",
            "Teacher 30\n",
            "Epoch: 1/12..  Test Accuracy: 0.100962\n",
            "Epoch: 2/12..  Test Accuracy: 0.370793\n",
            "Epoch: 3/12..  Test Accuracy: 0.516977\n",
            "Epoch: 4/12..  Test Accuracy: 0.673528\n",
            "Epoch: 5/12..  Test Accuracy: 0.778696\n",
            "Epoch: 6/12..  Test Accuracy: 0.841046\n",
            "Epoch: 7/12..  Test Accuracy: 0.827524\n",
            "Epoch: 8/12..  Test Accuracy: 0.861178\n",
            "Epoch: 9/12..  Test Accuracy: 0.868840\n",
            "Epoch: 10/12..  Test Accuracy: 0.885517\n",
            "Epoch: 11/12..  Test Accuracy: 0.892728\n",
            "Epoch: 12/12..  Test Accuracy: 0.903546\n",
            "Teacher 31\n",
            "Epoch: 1/12..  Test Accuracy: 0.245192\n",
            "Epoch: 2/12..  Test Accuracy: 0.559796\n",
            "Epoch: 3/12..  Test Accuracy: 0.591496\n",
            "Epoch: 4/12..  Test Accuracy: 0.718750\n",
            "Epoch: 5/12..  Test Accuracy: 0.813852\n",
            "Epoch: 6/12..  Test Accuracy: 0.846004\n",
            "Epoch: 7/12..  Test Accuracy: 0.859075\n",
            "Epoch: 8/12..  Test Accuracy: 0.900240\n",
            "Epoch: 9/12..  Test Accuracy: 0.910307\n",
            "Epoch: 10/12..  Test Accuracy: 0.899940\n",
            "Epoch: 11/12..  Test Accuracy: 0.916166\n",
            "Epoch: 12/12..  Test Accuracy: 0.922025\n",
            "Teacher 32\n",
            "Epoch: 1/12..  Test Accuracy: 0.140475\n",
            "Epoch: 2/12..  Test Accuracy: 0.363882\n",
            "Epoch: 3/12..  Test Accuracy: 0.461088\n",
            "Epoch: 4/12..  Test Accuracy: 0.645132\n",
            "Epoch: 5/12..  Test Accuracy: 0.704177\n",
            "Epoch: 6/12..  Test Accuracy: 0.763672\n",
            "Epoch: 7/12..  Test Accuracy: 0.792818\n",
            "Epoch: 8/12..  Test Accuracy: 0.855619\n",
            "Epoch: 9/12..  Test Accuracy: 0.835637\n",
            "Epoch: 10/12..  Test Accuracy: 0.875751\n",
            "Epoch: 11/12..  Test Accuracy: 0.891526\n",
            "Epoch: 12/12..  Test Accuracy: 0.878005\n",
            "Teacher 33\n",
            "Epoch: 1/12..  Test Accuracy: 0.095553\n",
            "Epoch: 2/12..  Test Accuracy: 0.335036\n",
            "Epoch: 3/12..  Test Accuracy: 0.521034\n",
            "Epoch: 4/12..  Test Accuracy: 0.584285\n",
            "Epoch: 5/12..  Test Accuracy: 0.695012\n",
            "Epoch: 6/12..  Test Accuracy: 0.764273\n",
            "Epoch: 7/12..  Test Accuracy: 0.809946\n",
            "Epoch: 8/12..  Test Accuracy: 0.835787\n",
            "Epoch: 9/12..  Test Accuracy: 0.854718\n",
            "Epoch: 10/12..  Test Accuracy: 0.868990\n",
            "Epoch: 11/12..  Test Accuracy: 0.881611\n",
            "Epoch: 12/12..  Test Accuracy: 0.885968\n",
            "Teacher 34\n",
            "Epoch: 1/12..  Test Accuracy: 0.246995\n",
            "Epoch: 2/12..  Test Accuracy: 0.306641\n",
            "Epoch: 3/12..  Test Accuracy: 0.714393\n",
            "Epoch: 4/12..  Test Accuracy: 0.776743\n",
            "Epoch: 5/12..  Test Accuracy: 0.832933\n",
            "Epoch: 6/12..  Test Accuracy: 0.867338\n",
            "Epoch: 7/12..  Test Accuracy: 0.897085\n",
            "Epoch: 8/12..  Test Accuracy: 0.894231\n",
            "Epoch: 9/12..  Test Accuracy: 0.899038\n",
            "Epoch: 10/12..  Test Accuracy: 0.895733\n",
            "Epoch: 11/12..  Test Accuracy: 0.906851\n",
            "Epoch: 12/12..  Test Accuracy: 0.903546\n",
            "Teacher 35\n",
            "Epoch: 1/12..  Test Accuracy: 0.255859\n",
            "Epoch: 2/12..  Test Accuracy: 0.355769\n",
            "Epoch: 3/12..  Test Accuracy: 0.415565\n",
            "Epoch: 4/12..  Test Accuracy: 0.560246\n",
            "Epoch: 5/12..  Test Accuracy: 0.742788\n",
            "Epoch: 6/12..  Test Accuracy: 0.822566\n",
            "Epoch: 7/12..  Test Accuracy: 0.835337\n",
            "Epoch: 8/12..  Test Accuracy: 0.882963\n",
            "Epoch: 9/12..  Test Accuracy: 0.878606\n",
            "Epoch: 10/12..  Test Accuracy: 0.905950\n",
            "Epoch: 11/12..  Test Accuracy: 0.903395\n",
            "Epoch: 12/12..  Test Accuracy: 0.901593\n",
            "Teacher 36\n",
            "Epoch: 1/12..  Test Accuracy: 0.252404\n",
            "Epoch: 2/12..  Test Accuracy: 0.327524\n",
            "Epoch: 3/12..  Test Accuracy: 0.448918\n",
            "Epoch: 4/12..  Test Accuracy: 0.756761\n",
            "Epoch: 5/12..  Test Accuracy: 0.805138\n",
            "Epoch: 6/12..  Test Accuracy: 0.829778\n",
            "Epoch: 7/12..  Test Accuracy: 0.859675\n",
            "Epoch: 8/12..  Test Accuracy: 0.857873\n",
            "Epoch: 9/12..  Test Accuracy: 0.885517\n",
            "Epoch: 10/12..  Test Accuracy: 0.880559\n",
            "Epoch: 11/12..  Test Accuracy: 0.894231\n",
            "Epoch: 12/12..  Test Accuracy: 0.885517\n",
            "Teacher 37\n",
            "Epoch: 1/12..  Test Accuracy: 0.261268\n",
            "Epoch: 2/12..  Test Accuracy: 0.538762\n",
            "Epoch: 3/12..  Test Accuracy: 0.646034\n",
            "Epoch: 4/12..  Test Accuracy: 0.749399\n",
            "Epoch: 5/12..  Test Accuracy: 0.835337\n",
            "Epoch: 6/12..  Test Accuracy: 0.854267\n",
            "Epoch: 7/12..  Test Accuracy: 0.852464\n",
            "Epoch: 8/12..  Test Accuracy: 0.890925\n",
            "Epoch: 9/12..  Test Accuracy: 0.887770\n",
            "Epoch: 10/12..  Test Accuracy: 0.901593\n",
            "Epoch: 11/12..  Test Accuracy: 0.902494\n",
            "Epoch: 12/12..  Test Accuracy: 0.913161\n",
            "Teacher 38\n",
            "Epoch: 1/12..  Test Accuracy: 0.247446\n",
            "Epoch: 2/12..  Test Accuracy: 0.293870\n",
            "Epoch: 3/12..  Test Accuracy: 0.547776\n",
            "Epoch: 4/12..  Test Accuracy: 0.703275\n",
            "Epoch: 5/12..  Test Accuracy: 0.748498\n",
            "Epoch: 6/12..  Test Accuracy: 0.840595\n",
            "Epoch: 7/12..  Test Accuracy: 0.864483\n",
            "Epoch: 8/12..  Test Accuracy: 0.866587\n",
            "Epoch: 9/12..  Test Accuracy: 0.880709\n",
            "Epoch: 10/12..  Test Accuracy: 0.889423\n",
            "Epoch: 11/12..  Test Accuracy: 0.897236\n",
            "Epoch: 12/12..  Test Accuracy: 0.906400\n",
            "Teacher 39\n",
            "Epoch: 1/12..  Test Accuracy: 0.271484\n",
            "Epoch: 2/12..  Test Accuracy: 0.450270\n",
            "Epoch: 3/12..  Test Accuracy: 0.592999\n",
            "Epoch: 4/12..  Test Accuracy: 0.690505\n",
            "Epoch: 5/12..  Test Accuracy: 0.780649\n",
            "Epoch: 6/12..  Test Accuracy: 0.827975\n",
            "Epoch: 7/12..  Test Accuracy: 0.857722\n",
            "Epoch: 8/12..  Test Accuracy: 0.859675\n",
            "Epoch: 9/12..  Test Accuracy: 0.867488\n",
            "Epoch: 10/12..  Test Accuracy: 0.888822\n",
            "Epoch: 11/12..  Test Accuracy: 0.888371\n",
            "Epoch: 12/12..  Test Accuracy: 0.903996\n",
            "Teacher 40\n",
            "Epoch: 1/12..  Test Accuracy: 0.212590\n",
            "Epoch: 2/12..  Test Accuracy: 0.335186\n",
            "Epoch: 3/12..  Test Accuracy: 0.508714\n",
            "Epoch: 4/12..  Test Accuracy: 0.621995\n",
            "Epoch: 5/12..  Test Accuracy: 0.734525\n",
            "Epoch: 6/12..  Test Accuracy: 0.777644\n",
            "Epoch: 7/12..  Test Accuracy: 0.839243\n",
            "Epoch: 8/12..  Test Accuracy: 0.841046\n",
            "Epoch: 9/12..  Test Accuracy: 0.868990\n",
            "Epoch: 10/12..  Test Accuracy: 0.879657\n",
            "Epoch: 11/12..  Test Accuracy: 0.887019\n",
            "Epoch: 12/12..  Test Accuracy: 0.891677\n",
            "Teacher 41\n",
            "Epoch: 1/12..  Test Accuracy: 0.291016\n",
            "Epoch: 2/12..  Test Accuracy: 0.327073\n",
            "Epoch: 3/12..  Test Accuracy: 0.537560\n",
            "Epoch: 4/12..  Test Accuracy: 0.634465\n",
            "Epoch: 5/12..  Test Accuracy: 0.782602\n",
            "Epoch: 6/12..  Test Accuracy: 0.782151\n",
            "Epoch: 7/12..  Test Accuracy: 0.855168\n",
            "Epoch: 8/12..  Test Accuracy: 0.831130\n",
            "Epoch: 9/12..  Test Accuracy: 0.881160\n",
            "Epoch: 10/12..  Test Accuracy: 0.875601\n",
            "Epoch: 11/12..  Test Accuracy: 0.886418\n",
            "Epoch: 12/12..  Test Accuracy: 0.883564\n",
            "Teacher 42\n",
            "Epoch: 1/12..  Test Accuracy: 0.080980\n",
            "Epoch: 2/12..  Test Accuracy: 0.149940\n",
            "Epoch: 3/12..  Test Accuracy: 0.376653\n",
            "Epoch: 4/12..  Test Accuracy: 0.546725\n",
            "Epoch: 5/12..  Test Accuracy: 0.647236\n",
            "Epoch: 6/12..  Test Accuracy: 0.783053\n",
            "Epoch: 7/12..  Test Accuracy: 0.793720\n",
            "Epoch: 8/12..  Test Accuracy: 0.836238\n",
            "Epoch: 9/12..  Test Accuracy: 0.859675\n",
            "Epoch: 10/12..  Test Accuracy: 0.880709\n",
            "Epoch: 11/12..  Test Accuracy: 0.885517\n",
            "Epoch: 12/12..  Test Accuracy: 0.880108\n",
            "Teacher 43\n",
            "Epoch: 1/12..  Test Accuracy: 0.135517\n",
            "Epoch: 2/12..  Test Accuracy: 0.292368\n",
            "Epoch: 3/12..  Test Accuracy: 0.485427\n",
            "Epoch: 4/12..  Test Accuracy: 0.673678\n",
            "Epoch: 5/12..  Test Accuracy: 0.790415\n",
            "Epoch: 6/12..  Test Accuracy: 0.818209\n",
            "Epoch: 7/12..  Test Accuracy: 0.840144\n",
            "Epoch: 8/12..  Test Accuracy: 0.846004\n",
            "Epoch: 9/12..  Test Accuracy: 0.849309\n",
            "Epoch: 10/12..  Test Accuracy: 0.859225\n",
            "Epoch: 11/12..  Test Accuracy: 0.869742\n",
            "Epoch: 12/12..  Test Accuracy: 0.880108\n",
            "Teacher 44\n",
            "Epoch: 1/12..  Test Accuracy: 0.128756\n",
            "Epoch: 2/12..  Test Accuracy: 0.438702\n",
            "Epoch: 3/12..  Test Accuracy: 0.586238\n",
            "Epoch: 4/12..  Test Accuracy: 0.661809\n",
            "Epoch: 5/12..  Test Accuracy: 0.789062\n",
            "Epoch: 6/12..  Test Accuracy: 0.751953\n",
            "Epoch: 7/12..  Test Accuracy: 0.837740\n",
            "Epoch: 8/12..  Test Accuracy: 0.821665\n",
            "Epoch: 9/12..  Test Accuracy: 0.852314\n",
            "Epoch: 10/12..  Test Accuracy: 0.877254\n",
            "Epoch: 11/12..  Test Accuracy: 0.881010\n",
            "Epoch: 12/12..  Test Accuracy: 0.883864\n",
            "Teacher 45\n",
            "Epoch: 1/12..  Test Accuracy: 0.235877\n",
            "Epoch: 2/12..  Test Accuracy: 0.179838\n",
            "Epoch: 3/12..  Test Accuracy: 0.571665\n",
            "Epoch: 4/12..  Test Accuracy: 0.736328\n",
            "Epoch: 5/12..  Test Accuracy: 0.796875\n",
            "Epoch: 6/12..  Test Accuracy: 0.830980\n",
            "Epoch: 7/12..  Test Accuracy: 0.830529\n",
            "Epoch: 8/12..  Test Accuracy: 0.868239\n",
            "Epoch: 9/12..  Test Accuracy: 0.885517\n",
            "Epoch: 10/12..  Test Accuracy: 0.896785\n",
            "Epoch: 11/12..  Test Accuracy: 0.901593\n",
            "Epoch: 12/12..  Test Accuracy: 0.895132\n",
            "Teacher 46\n",
            "Epoch: 1/12..  Test Accuracy: 0.178786\n",
            "Epoch: 2/12..  Test Accuracy: 0.337590\n",
            "Epoch: 3/12..  Test Accuracy: 0.501502\n",
            "Epoch: 4/12..  Test Accuracy: 0.678335\n",
            "Epoch: 5/12..  Test Accuracy: 0.727614\n",
            "Epoch: 6/12..  Test Accuracy: 0.809946\n",
            "Epoch: 7/12..  Test Accuracy: 0.839093\n",
            "Epoch: 8/12..  Test Accuracy: 0.859075\n",
            "Epoch: 9/12..  Test Accuracy: 0.859525\n",
            "Epoch: 10/12..  Test Accuracy: 0.881010\n",
            "Epoch: 11/12..  Test Accuracy: 0.869892\n",
            "Epoch: 12/12..  Test Accuracy: 0.884916\n",
            "Teacher 47\n",
            "Epoch: 1/12..  Test Accuracy: 0.191857\n",
            "Epoch: 2/12..  Test Accuracy: 0.448768\n",
            "Epoch: 3/12..  Test Accuracy: 0.525691\n",
            "Epoch: 4/12..  Test Accuracy: 0.608173\n",
            "Epoch: 5/12..  Test Accuracy: 0.737380\n",
            "Epoch: 6/12..  Test Accuracy: 0.812800\n",
            "Epoch: 7/12..  Test Accuracy: 0.833383\n",
            "Epoch: 8/12..  Test Accuracy: 0.856370\n",
            "Epoch: 9/12..  Test Accuracy: 0.866436\n",
            "Epoch: 10/12..  Test Accuracy: 0.882512\n",
            "Epoch: 11/12..  Test Accuracy: 0.885517\n",
            "Epoch: 12/12..  Test Accuracy: 0.895282\n",
            "Teacher 48\n",
            "Epoch: 1/12..  Test Accuracy: 0.196514\n",
            "Epoch: 2/12..  Test Accuracy: 0.458534\n",
            "Epoch: 3/12..  Test Accuracy: 0.595102\n",
            "Epoch: 4/12..  Test Accuracy: 0.712891\n",
            "Epoch: 5/12..  Test Accuracy: 0.834886\n",
            "Epoch: 6/12..  Test Accuracy: 0.867338\n",
            "Epoch: 7/12..  Test Accuracy: 0.879207\n",
            "Epoch: 8/12..  Test Accuracy: 0.908053\n",
            "Epoch: 9/12..  Test Accuracy: 0.914663\n",
            "Epoch: 10/12..  Test Accuracy: 0.920974\n",
            "Epoch: 11/12..  Test Accuracy: 0.926382\n",
            "Epoch: 12/12..  Test Accuracy: 0.925481\n",
            "Teacher 49\n",
            "Epoch: 1/12..  Test Accuracy: 0.261719\n",
            "Epoch: 2/12..  Test Accuracy: 0.573017\n",
            "Epoch: 3/12..  Test Accuracy: 0.686148\n",
            "Epoch: 4/12..  Test Accuracy: 0.755859\n",
            "Epoch: 5/12..  Test Accuracy: 0.809495\n",
            "Epoch: 6/12..  Test Accuracy: 0.837740\n",
            "Epoch: 7/12..  Test Accuracy: 0.860727\n",
            "Epoch: 8/12..  Test Accuracy: 0.863432\n",
            "Epoch: 9/12..  Test Accuracy: 0.879207\n",
            "Epoch: 10/12..  Test Accuracy: 0.881611\n",
            "Epoch: 11/12..  Test Accuracy: 0.900992\n",
            "Epoch: 12/12..  Test Accuracy: 0.909405\n",
            "Teacher 50\n",
            "Epoch: 1/12..  Test Accuracy: 0.093149\n",
            "Epoch: 2/12..  Test Accuracy: 0.293419\n",
            "Epoch: 3/12..  Test Accuracy: 0.508413\n",
            "Epoch: 4/12..  Test Accuracy: 0.672175\n",
            "Epoch: 5/12..  Test Accuracy: 0.789062\n",
            "Epoch: 6/12..  Test Accuracy: 0.844050\n",
            "Epoch: 7/12..  Test Accuracy: 0.862680\n",
            "Epoch: 8/12..  Test Accuracy: 0.877254\n",
            "Epoch: 9/12..  Test Accuracy: 0.878606\n",
            "Epoch: 10/12..  Test Accuracy: 0.877554\n",
            "Epoch: 11/12..  Test Accuracy: 0.901743\n",
            "Epoch: 12/12..  Test Accuracy: 0.905950\n",
            "Teacher 51\n",
            "Epoch: 1/12..  Test Accuracy: 0.223107\n",
            "Epoch: 2/12..  Test Accuracy: 0.284555\n",
            "Epoch: 3/12..  Test Accuracy: 0.392428\n",
            "Epoch: 4/12..  Test Accuracy: 0.543570\n",
            "Epoch: 5/12..  Test Accuracy: 0.697566\n",
            "Epoch: 6/12..  Test Accuracy: 0.738281\n",
            "Epoch: 7/12..  Test Accuracy: 0.788462\n",
            "Epoch: 8/12..  Test Accuracy: 0.855318\n",
            "Epoch: 9/12..  Test Accuracy: 0.837740\n",
            "Epoch: 10/12..  Test Accuracy: 0.862680\n",
            "Epoch: 11/12..  Test Accuracy: 0.871244\n",
            "Epoch: 12/12..  Test Accuracy: 0.889273\n",
            "Teacher 52\n",
            "Epoch: 1/12..  Test Accuracy: 0.186599\n",
            "Epoch: 2/12..  Test Accuracy: 0.113582\n",
            "Epoch: 3/12..  Test Accuracy: 0.350511\n",
            "Epoch: 4/12..  Test Accuracy: 0.543570\n",
            "Epoch: 5/12..  Test Accuracy: 0.640024\n",
            "Epoch: 6/12..  Test Accuracy: 0.770583\n",
            "Epoch: 7/12..  Test Accuracy: 0.805138\n",
            "Epoch: 8/12..  Test Accuracy: 0.831881\n",
            "Epoch: 9/12..  Test Accuracy: 0.841647\n",
            "Epoch: 10/12..  Test Accuracy: 0.869892\n",
            "Epoch: 11/12..  Test Accuracy: 0.859675\n",
            "Epoch: 12/12..  Test Accuracy: 0.882512\n",
            "Teacher 53\n",
            "Epoch: 1/12..  Test Accuracy: 0.110727\n",
            "Epoch: 2/12..  Test Accuracy: 0.269531\n",
            "Epoch: 3/12..  Test Accuracy: 0.523438\n",
            "Epoch: 4/12..  Test Accuracy: 0.649038\n",
            "Epoch: 5/12..  Test Accuracy: 0.757662\n",
            "Epoch: 6/12..  Test Accuracy: 0.822566\n",
            "Epoch: 7/12..  Test Accuracy: 0.834886\n",
            "Epoch: 8/12..  Test Accuracy: 0.864483\n",
            "Epoch: 9/12..  Test Accuracy: 0.867939\n",
            "Epoch: 10/12..  Test Accuracy: 0.874249\n",
            "Epoch: 11/12..  Test Accuracy: 0.879507\n",
            "Epoch: 12/12..  Test Accuracy: 0.882512\n",
            "Teacher 54\n",
            "Epoch: 1/12..  Test Accuracy: 0.272987\n",
            "Epoch: 2/12..  Test Accuracy: 0.408504\n",
            "Epoch: 3/12..  Test Accuracy: 0.617338\n",
            "Epoch: 4/12..  Test Accuracy: 0.671274\n",
            "Epoch: 5/12..  Test Accuracy: 0.716797\n",
            "Epoch: 6/12..  Test Accuracy: 0.801232\n",
            "Epoch: 7/12..  Test Accuracy: 0.817157\n",
            "Epoch: 8/12..  Test Accuracy: 0.827073\n",
            "Epoch: 9/12..  Test Accuracy: 0.842097\n",
            "Epoch: 10/12..  Test Accuracy: 0.865685\n",
            "Epoch: 11/12..  Test Accuracy: 0.866887\n",
            "Epoch: 12/12..  Test Accuracy: 0.871845\n",
            "Teacher 55\n",
            "Epoch: 1/12..  Test Accuracy: 0.090745\n",
            "Epoch: 2/12..  Test Accuracy: 0.312350\n",
            "Epoch: 3/12..  Test Accuracy: 0.523588\n",
            "Epoch: 4/12..  Test Accuracy: 0.748347\n",
            "Epoch: 5/12..  Test Accuracy: 0.776292\n",
            "Epoch: 6/12..  Test Accuracy: 0.828876\n",
            "Epoch: 7/12..  Test Accuracy: 0.846454\n",
            "Epoch: 8/12..  Test Accuracy: 0.847506\n",
            "Epoch: 9/12..  Test Accuracy: 0.874549\n",
            "Epoch: 10/12..  Test Accuracy: 0.892728\n",
            "Epoch: 11/12..  Test Accuracy: 0.873197\n",
            "Epoch: 12/12..  Test Accuracy: 0.883413\n",
            "Teacher 56\n",
            "Epoch: 1/12..  Test Accuracy: 0.150541\n",
            "Epoch: 2/12..  Test Accuracy: 0.368540\n",
            "Epoch: 3/12..  Test Accuracy: 0.511869\n",
            "Epoch: 4/12..  Test Accuracy: 0.658804\n",
            "Epoch: 5/12..  Test Accuracy: 0.740835\n",
            "Epoch: 6/12..  Test Accuracy: 0.812800\n",
            "Epoch: 7/12..  Test Accuracy: 0.817308\n",
            "Epoch: 8/12..  Test Accuracy: 0.848407\n",
            "Epoch: 9/12..  Test Accuracy: 0.833383\n",
            "Epoch: 10/12..  Test Accuracy: 0.877704\n",
            "Epoch: 11/12..  Test Accuracy: 0.875300\n",
            "Epoch: 12/12..  Test Accuracy: 0.874850\n",
            "Teacher 57\n",
            "Epoch: 1/12..  Test Accuracy: 0.124700\n",
            "Epoch: 2/12..  Test Accuracy: 0.376052\n",
            "Epoch: 3/12..  Test Accuracy: 0.502554\n",
            "Epoch: 4/12..  Test Accuracy: 0.652644\n",
            "Epoch: 5/12..  Test Accuracy: 0.741436\n",
            "Epoch: 6/12..  Test Accuracy: 0.827073\n",
            "Epoch: 7/12..  Test Accuracy: 0.860276\n",
            "Epoch: 8/12..  Test Accuracy: 0.872296\n",
            "Epoch: 9/12..  Test Accuracy: 0.883564\n",
            "Epoch: 10/12..  Test Accuracy: 0.888972\n",
            "Epoch: 11/12..  Test Accuracy: 0.895733\n",
            "Epoch: 12/12..  Test Accuracy: 0.896785\n",
            "Teacher 58\n",
            "Epoch: 1/12..  Test Accuracy: 0.142428\n",
            "Epoch: 2/12..  Test Accuracy: 0.395733\n",
            "Epoch: 3/12..  Test Accuracy: 0.451773\n",
            "Epoch: 4/12..  Test Accuracy: 0.674429\n",
            "Epoch: 5/12..  Test Accuracy: 0.783654\n",
            "Epoch: 6/12..  Test Accuracy: 0.812800\n",
            "Epoch: 7/12..  Test Accuracy: 0.863131\n",
            "Epoch: 8/12..  Test Accuracy: 0.872746\n",
            "Epoch: 9/12..  Test Accuracy: 0.893179\n",
            "Epoch: 10/12..  Test Accuracy: 0.901442\n",
            "Epoch: 11/12..  Test Accuracy: 0.895733\n",
            "Epoch: 12/12..  Test Accuracy: 0.912861\n",
            "Teacher 59\n",
            "Epoch: 1/12..  Test Accuracy: 0.126803\n",
            "Epoch: 2/12..  Test Accuracy: 0.510667\n",
            "Epoch: 3/12..  Test Accuracy: 0.516076\n",
            "Epoch: 4/12..  Test Accuracy: 0.733323\n",
            "Epoch: 5/12..  Test Accuracy: 0.789814\n",
            "Epoch: 6/12..  Test Accuracy: 0.825421\n",
            "Epoch: 7/12..  Test Accuracy: 0.860577\n",
            "Epoch: 8/12..  Test Accuracy: 0.874249\n",
            "Epoch: 9/12..  Test Accuracy: 0.895733\n",
            "Epoch: 10/12..  Test Accuracy: 0.888972\n",
            "Epoch: 11/12..  Test Accuracy: 0.902494\n",
            "Epoch: 12/12..  Test Accuracy: 0.912260\n",
            "Teacher 60\n",
            "Epoch: 1/12..  Test Accuracy: 0.109976\n",
            "Epoch: 2/12..  Test Accuracy: 0.394681\n",
            "Epoch: 3/12..  Test Accuracy: 0.587590\n",
            "Epoch: 4/12..  Test Accuracy: 0.730469\n",
            "Epoch: 5/12..  Test Accuracy: 0.788011\n",
            "Epoch: 6/12..  Test Accuracy: 0.856821\n",
            "Epoch: 7/12..  Test Accuracy: 0.871394\n",
            "Epoch: 8/12..  Test Accuracy: 0.871394\n",
            "Epoch: 9/12..  Test Accuracy: 0.888522\n",
            "Epoch: 10/12..  Test Accuracy: 0.884465\n",
            "Epoch: 11/12..  Test Accuracy: 0.896635\n",
            "Epoch: 12/12..  Test Accuracy: 0.911208\n",
            "Teacher 61\n",
            "Epoch: 1/12..  Test Accuracy: 0.172927\n",
            "Epoch: 2/12..  Test Accuracy: 0.302885\n",
            "Epoch: 3/12..  Test Accuracy: 0.501502\n",
            "Epoch: 4/12..  Test Accuracy: 0.597957\n",
            "Epoch: 5/12..  Test Accuracy: 0.715745\n",
            "Epoch: 6/12..  Test Accuracy: 0.769982\n",
            "Epoch: 7/12..  Test Accuracy: 0.821514\n",
            "Epoch: 8/12..  Test Accuracy: 0.867939\n",
            "Epoch: 9/12..  Test Accuracy: 0.886869\n",
            "Epoch: 10/12..  Test Accuracy: 0.881911\n",
            "Epoch: 11/12..  Test Accuracy: 0.902043\n",
            "Epoch: 12/12..  Test Accuracy: 0.905048\n",
            "Teacher 62\n",
            "Epoch: 1/12..  Test Accuracy: 0.172175\n",
            "Epoch: 2/12..  Test Accuracy: 0.407001\n",
            "Epoch: 3/12..  Test Accuracy: 0.510517\n",
            "Epoch: 4/12..  Test Accuracy: 0.680288\n",
            "Epoch: 5/12..  Test Accuracy: 0.774639\n",
            "Epoch: 6/12..  Test Accuracy: 0.824669\n",
            "Epoch: 7/12..  Test Accuracy: 0.836839\n",
            "Epoch: 8/12..  Test Accuracy: 0.869441\n",
            "Epoch: 9/12..  Test Accuracy: 0.860877\n",
            "Epoch: 10/12..  Test Accuracy: 0.894231\n",
            "Epoch: 11/12..  Test Accuracy: 0.890325\n",
            "Epoch: 12/12..  Test Accuracy: 0.908954\n",
            "Teacher 63\n",
            "Epoch: 1/12..  Test Accuracy: 0.268930\n",
            "Epoch: 2/12..  Test Accuracy: 0.377855\n",
            "Epoch: 3/12..  Test Accuracy: 0.421424\n",
            "Epoch: 4/12..  Test Accuracy: 0.478666\n",
            "Epoch: 5/12..  Test Accuracy: 0.610877\n",
            "Epoch: 6/12..  Test Accuracy: 0.725210\n",
            "Epoch: 7/12..  Test Accuracy: 0.785457\n",
            "Epoch: 8/12..  Test Accuracy: 0.806941\n",
            "Epoch: 9/12..  Test Accuracy: 0.850811\n",
            "Epoch: 10/12..  Test Accuracy: 0.872296\n",
            "Epoch: 11/12..  Test Accuracy: 0.877704\n",
            "Epoch: 12/12..  Test Accuracy: 0.884916\n",
            "Teacher 64\n",
            "Epoch: 1/12..  Test Accuracy: 0.227915\n",
            "Epoch: 2/12..  Test Accuracy: 0.388972\n",
            "Epoch: 3/12..  Test Accuracy: 0.550331\n",
            "Epoch: 4/12..  Test Accuracy: 0.657452\n",
            "Epoch: 5/12..  Test Accuracy: 0.754357\n",
            "Epoch: 6/12..  Test Accuracy: 0.812951\n",
            "Epoch: 7/12..  Test Accuracy: 0.815355\n",
            "Epoch: 8/12..  Test Accuracy: 0.855919\n",
            "Epoch: 9/12..  Test Accuracy: 0.873197\n",
            "Epoch: 10/12..  Test Accuracy: 0.886869\n",
            "Epoch: 11/12..  Test Accuracy: 0.873948\n",
            "Epoch: 12/12..  Test Accuracy: 0.893780\n",
            "Teacher 65\n",
            "Epoch: 1/12..  Test Accuracy: 0.180288\n",
            "Epoch: 2/12..  Test Accuracy: 0.367338\n",
            "Epoch: 3/12..  Test Accuracy: 0.530649\n",
            "Epoch: 4/12..  Test Accuracy: 0.569261\n",
            "Epoch: 5/12..  Test Accuracy: 0.710487\n",
            "Epoch: 6/12..  Test Accuracy: 0.769531\n",
            "Epoch: 7/12..  Test Accuracy: 0.804988\n",
            "Epoch: 8/12..  Test Accuracy: 0.838642\n",
            "Epoch: 9/12..  Test Accuracy: 0.854718\n",
            "Epoch: 10/12..  Test Accuracy: 0.870343\n",
            "Epoch: 11/12..  Test Accuracy: 0.867939\n",
            "Epoch: 12/12..  Test Accuracy: 0.887921\n",
            "Teacher 66\n",
            "Epoch: 1/12..  Test Accuracy: 0.215745\n",
            "Epoch: 2/12..  Test Accuracy: 0.238582\n",
            "Epoch: 3/12..  Test Accuracy: 0.428035\n",
            "Epoch: 4/12..  Test Accuracy: 0.625300\n",
            "Epoch: 5/12..  Test Accuracy: 0.739333\n",
            "Epoch: 6/12..  Test Accuracy: 0.769081\n",
            "Epoch: 7/12..  Test Accuracy: 0.806340\n",
            "Epoch: 8/12..  Test Accuracy: 0.824068\n",
            "Epoch: 9/12..  Test Accuracy: 0.840144\n",
            "Epoch: 10/12..  Test Accuracy: 0.858173\n",
            "Epoch: 11/12..  Test Accuracy: 0.886418\n",
            "Epoch: 12/12..  Test Accuracy: 0.892879\n",
            "Teacher 67\n",
            "Epoch: 1/12..  Test Accuracy: 0.239183\n",
            "Epoch: 2/12..  Test Accuracy: 0.317758\n",
            "Epoch: 3/12..  Test Accuracy: 0.577073\n",
            "Epoch: 4/12..  Test Accuracy: 0.645132\n",
            "Epoch: 5/12..  Test Accuracy: 0.721304\n",
            "Epoch: 6/12..  Test Accuracy: 0.750451\n",
            "Epoch: 7/12..  Test Accuracy: 0.800331\n",
            "Epoch: 8/12..  Test Accuracy: 0.790865\n",
            "Epoch: 9/12..  Test Accuracy: 0.852163\n",
            "Epoch: 10/12..  Test Accuracy: 0.847957\n",
            "Epoch: 11/12..  Test Accuracy: 0.855769\n",
            "Epoch: 12/12..  Test Accuracy: 0.883113\n",
            "Teacher 68\n",
            "Epoch: 1/12..  Test Accuracy: 0.084886\n",
            "Epoch: 2/12..  Test Accuracy: 0.164213\n",
            "Epoch: 3/12..  Test Accuracy: 0.410306\n",
            "Epoch: 4/12..  Test Accuracy: 0.669020\n",
            "Epoch: 5/12..  Test Accuracy: 0.750901\n",
            "Epoch: 6/12..  Test Accuracy: 0.806791\n",
            "Epoch: 7/12..  Test Accuracy: 0.821665\n",
            "Epoch: 8/12..  Test Accuracy: 0.834736\n",
            "Epoch: 9/12..  Test Accuracy: 0.845553\n",
            "Epoch: 10/12..  Test Accuracy: 0.866887\n",
            "Epoch: 11/12..  Test Accuracy: 0.865835\n",
            "Epoch: 12/12..  Test Accuracy: 0.868990\n",
            "Teacher 69\n",
            "Epoch: 1/12..  Test Accuracy: 0.273287\n",
            "Epoch: 2/12..  Test Accuracy: 0.397386\n",
            "Epoch: 3/12..  Test Accuracy: 0.573167\n",
            "Epoch: 4/12..  Test Accuracy: 0.643179\n",
            "Epoch: 5/12..  Test Accuracy: 0.718600\n",
            "Epoch: 6/12..  Test Accuracy: 0.774940\n",
            "Epoch: 7/12..  Test Accuracy: 0.830379\n",
            "Epoch: 8/12..  Test Accuracy: 0.872446\n",
            "Epoch: 9/12..  Test Accuracy: 0.861629\n",
            "Epoch: 10/12..  Test Accuracy: 0.895132\n",
            "Epoch: 11/12..  Test Accuracy: 0.879207\n",
            "Epoch: 12/12..  Test Accuracy: 0.893179\n",
            "Teacher 70\n",
            "Epoch: 1/12..  Test Accuracy: 0.191556\n",
            "Epoch: 2/12..  Test Accuracy: 0.323618\n",
            "Epoch: 3/12..  Test Accuracy: 0.544321\n",
            "Epoch: 4/12..  Test Accuracy: 0.649639\n",
            "Epoch: 5/12..  Test Accuracy: 0.719802\n",
            "Epoch: 6/12..  Test Accuracy: 0.791016\n",
            "Epoch: 7/12..  Test Accuracy: 0.805138\n",
            "Epoch: 8/12..  Test Accuracy: 0.819712\n",
            "Epoch: 9/12..  Test Accuracy: 0.832332\n",
            "Epoch: 10/12..  Test Accuracy: 0.855769\n",
            "Epoch: 11/12..  Test Accuracy: 0.868389\n",
            "Epoch: 12/12..  Test Accuracy: 0.866436\n",
            "Teacher 71\n",
            "Epoch: 1/12..  Test Accuracy: 0.089694\n",
            "Epoch: 2/12..  Test Accuracy: 0.338942\n",
            "Epoch: 3/12..  Test Accuracy: 0.452825\n",
            "Epoch: 4/12..  Test Accuracy: 0.624399\n",
            "Epoch: 5/12..  Test Accuracy: 0.698317\n",
            "Epoch: 6/12..  Test Accuracy: 0.740685\n",
            "Epoch: 7/12..  Test Accuracy: 0.823017\n",
            "Epoch: 8/12..  Test Accuracy: 0.842097\n",
            "Epoch: 9/12..  Test Accuracy: 0.844651\n",
            "Epoch: 10/12..  Test Accuracy: 0.879057\n",
            "Epoch: 11/12..  Test Accuracy: 0.875300\n",
            "Epoch: 12/12..  Test Accuracy: 0.888822\n",
            "Teacher 72\n",
            "Epoch: 1/12..  Test Accuracy: 0.247145\n",
            "Epoch: 2/12..  Test Accuracy: 0.458984\n",
            "Epoch: 3/12..  Test Accuracy: 0.630108\n",
            "Epoch: 4/12..  Test Accuracy: 0.655499\n",
            "Epoch: 5/12..  Test Accuracy: 0.751352\n",
            "Epoch: 6/12..  Test Accuracy: 0.810246\n",
            "Epoch: 7/12..  Test Accuracy: 0.844050\n",
            "Epoch: 8/12..  Test Accuracy: 0.853966\n",
            "Epoch: 9/12..  Test Accuracy: 0.857873\n",
            "Epoch: 10/12..  Test Accuracy: 0.878155\n",
            "Epoch: 11/12..  Test Accuracy: 0.888972\n",
            "Epoch: 12/12..  Test Accuracy: 0.888522\n",
            "Teacher 73\n",
            "Epoch: 1/12..  Test Accuracy: 0.347506\n",
            "Epoch: 2/12..  Test Accuracy: 0.469802\n",
            "Epoch: 3/12..  Test Accuracy: 0.613582\n",
            "Epoch: 4/12..  Test Accuracy: 0.685697\n",
            "Epoch: 5/12..  Test Accuracy: 0.731971\n",
            "Epoch: 6/12..  Test Accuracy: 0.803636\n",
            "Epoch: 7/12..  Test Accuracy: 0.827374\n",
            "Epoch: 8/12..  Test Accuracy: 0.847656\n",
            "Epoch: 9/12..  Test Accuracy: 0.867939\n",
            "Epoch: 10/12..  Test Accuracy: 0.875300\n",
            "Epoch: 11/12..  Test Accuracy: 0.873798\n",
            "Epoch: 12/12..  Test Accuracy: 0.894231\n",
            "Teacher 74\n",
            "Epoch: 1/12..  Test Accuracy: 0.286208\n",
            "Epoch: 2/12..  Test Accuracy: 0.350811\n",
            "Epoch: 3/12..  Test Accuracy: 0.615535\n",
            "Epoch: 4/12..  Test Accuracy: 0.665415\n",
            "Epoch: 5/12..  Test Accuracy: 0.756310\n",
            "Epoch: 6/12..  Test Accuracy: 0.827073\n",
            "Epoch: 7/12..  Test Accuracy: 0.835186\n",
            "Epoch: 8/12..  Test Accuracy: 0.859225\n",
            "Epoch: 9/12..  Test Accuracy: 0.855619\n",
            "Epoch: 10/12..  Test Accuracy: 0.870343\n",
            "Epoch: 11/12..  Test Accuracy: 0.875751\n",
            "Epoch: 12/12..  Test Accuracy: 0.889874\n",
            "Teacher 75\n",
            "Epoch: 1/12..  Test Accuracy: 0.196815\n",
            "Epoch: 2/12..  Test Accuracy: 0.308293\n",
            "Epoch: 3/12..  Test Accuracy: 0.450721\n",
            "Epoch: 4/12..  Test Accuracy: 0.558894\n",
            "Epoch: 5/12..  Test Accuracy: 0.718299\n",
            "Epoch: 6/12..  Test Accuracy: 0.781100\n",
            "Epoch: 7/12..  Test Accuracy: 0.861178\n",
            "Epoch: 8/12..  Test Accuracy: 0.874249\n",
            "Epoch: 9/12..  Test Accuracy: 0.890925\n",
            "Epoch: 10/12..  Test Accuracy: 0.878155\n",
            "Epoch: 11/12..  Test Accuracy: 0.890775\n",
            "Epoch: 12/12..  Test Accuracy: 0.905649\n",
            "Teacher 76\n",
            "Epoch: 1/12..  Test Accuracy: 0.100811\n",
            "Epoch: 2/12..  Test Accuracy: 0.447416\n",
            "Epoch: 3/12..  Test Accuracy: 0.560847\n",
            "Epoch: 4/12..  Test Accuracy: 0.708534\n",
            "Epoch: 5/12..  Test Accuracy: 0.766076\n",
            "Epoch: 6/12..  Test Accuracy: 0.862981\n",
            "Epoch: 7/12..  Test Accuracy: 0.856671\n",
            "Epoch: 8/12..  Test Accuracy: 0.865685\n",
            "Epoch: 9/12..  Test Accuracy: 0.879057\n",
            "Epoch: 10/12..  Test Accuracy: 0.889423\n",
            "Epoch: 11/12..  Test Accuracy: 0.887470\n",
            "Epoch: 12/12..  Test Accuracy: 0.896184\n",
            "Teacher 77\n",
            "Epoch: 1/12..  Test Accuracy: 0.145282\n",
            "Epoch: 2/12..  Test Accuracy: 0.477163\n",
            "Epoch: 3/12..  Test Accuracy: 0.697716\n",
            "Epoch: 4/12..  Test Accuracy: 0.727614\n",
            "Epoch: 5/12..  Test Accuracy: 0.772837\n",
            "Epoch: 6/12..  Test Accuracy: 0.812800\n",
            "Epoch: 7/12..  Test Accuracy: 0.850361\n",
            "Epoch: 8/12..  Test Accuracy: 0.842248\n",
            "Epoch: 9/12..  Test Accuracy: 0.864032\n",
            "Epoch: 10/12..  Test Accuracy: 0.884014\n",
            "Epoch: 11/12..  Test Accuracy: 0.878606\n",
            "Epoch: 12/12..  Test Accuracy: 0.878606\n",
            "Teacher 78\n",
            "Epoch: 1/12..  Test Accuracy: 0.109675\n",
            "Epoch: 2/12..  Test Accuracy: 0.206731\n",
            "Epoch: 3/12..  Test Accuracy: 0.454778\n",
            "Epoch: 4/12..  Test Accuracy: 0.660907\n",
            "Epoch: 5/12..  Test Accuracy: 0.788011\n",
            "Epoch: 6/12..  Test Accuracy: 0.811749\n",
            "Epoch: 7/12..  Test Accuracy: 0.843149\n",
            "Epoch: 8/12..  Test Accuracy: 0.853215\n",
            "Epoch: 9/12..  Test Accuracy: 0.860727\n",
            "Epoch: 10/12..  Test Accuracy: 0.874700\n",
            "Epoch: 11/12..  Test Accuracy: 0.895733\n",
            "Epoch: 12/12..  Test Accuracy: 0.893179\n",
            "Teacher 79\n",
            "Epoch: 1/12..  Test Accuracy: 0.232873\n",
            "Epoch: 2/12..  Test Accuracy: 0.376352\n",
            "Epoch: 3/12..  Test Accuracy: 0.515475\n",
            "Epoch: 4/12..  Test Accuracy: 0.645282\n",
            "Epoch: 5/12..  Test Accuracy: 0.711538\n",
            "Epoch: 6/12..  Test Accuracy: 0.776442\n",
            "Epoch: 7/12..  Test Accuracy: 0.826472\n",
            "Epoch: 8/12..  Test Accuracy: 0.841647\n",
            "Epoch: 9/12..  Test Accuracy: 0.862079\n",
            "Epoch: 10/12..  Test Accuracy: 0.853966\n",
            "Epoch: 11/12..  Test Accuracy: 0.865535\n",
            "Epoch: 12/12..  Test Accuracy: 0.890174\n",
            "All 80 Teachers  has been trained\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgihRYmu2kkD",
        "colab_type": "code",
        "outputId": "71bdf955-a2fa-462d-d9ce-ed19721c67f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(labs),len(imgs))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9500 9500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YGDRpv05LEP",
        "colab_type": "code",
        "outputId": "53ea9265-a86a-4d8c-d92c-45354f34d8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "np.transpose(labs[:100])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMR3k4GU0O3T",
        "colab_type": "code",
        "outputId": "5daae535-207d-4c47-8513-4079e92e8e40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"len(mnist_labels) {len(mnist_labels)}\",f\"len(true_labels) {len(true_labels)}\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(mnist_labels) 80 len(true_labels) 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMX4BByqIAOR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1274c51d-a407-4517-a730-9d64ba82eefd"
      },
      "source": [
        "mnist_labels[0][:100]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7., 2., 1., 0., 4., 1., 4., 9., 5., 9., 0., 6., 9., 0., 1., 5., 9., 7.,\n",
              "        3., 4., 9., 6., 6., 5., 4., 0., 7., 4., 0., 1., 3., 1., 3., 4., 7., 2.,\n",
              "        7., 1., 2., 1., 1., 7., 4., 2., 3., 5., 1., 2., 4., 4., 6., 3., 5., 5.,\n",
              "        6., 0., 4., 1., 9., 5., 7., 8., 9., 3., 7., 4., 6., 4., 3., 0., 7., 0.,\n",
              "        2., 9., 1., 7., 3., 2., 9., 7., 7., 6., 2., 7., 8., 4., 7., 3., 6., 1.,\n",
              "        3., 6., 9., 3., 1., 4., 1., 7., 6., 9.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMa--HY0I3dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_label = np.array( [ item.int() for item in mnist_labels[0].flatten() ] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqQgpXigJkWP",
        "colab_type": "code",
        "outputId": "daaff191-26c8-4be9-ac38-1b82ccb45d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(mnist_label)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzQtuIqdt2EL",
        "colab_type": "code",
        "outputId": "dcc8a4c5-7866-43ad-a64c-74441d272efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "mnist_label.shape\n",
        "mnist_label[:100]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhwbaicNJqwv",
        "colab_type": "code",
        "outputId": "a82a8a95-131d-4bfc-bfe7-44acdffde83f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(true_labels)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gln7xCkSKOu3",
        "colab_type": "code",
        "outputId": "020fd408-2513-4aa9-b79e-774690ef38d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "true_labels[12][:100]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7., 2., 1., 0., 4., 1., 4., 4., 5., 9., 0., 6., 9., 0., 1., 5., 9., 7.,\n",
              "        3., 4., 9., 6., 6., 5., 4., 0., 7., 4., 0., 1., 3., 1., 3., 0., 7., 2.,\n",
              "        7., 1., 2., 1., 1., 7., 4., 2., 3., 5., 3., 2., 4., 4., 6., 3., 5., 5.,\n",
              "        6., 0., 4., 1., 9., 5., 7., 8., 4., 3., 7., 4., 0., 4., 3., 0., 7., 0.,\n",
              "        2., 8., 1., 7., 3., 7., 9., 7., 7., 6., 2., 7., 8., 4., 7., 3., 6., 1.,\n",
              "        3., 6., 4., 3., 1., 4., 3., 1., 6., 9.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D3IO5fhtsGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_label = [item.int() for sublist in true_labels for item in sublist.flatten()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtrnxQvzt6kv",
        "colab_type": "code",
        "outputId": "609f8fb3-bace-44ef-ec77-7ab3b1fc7797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(true_label)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "760000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDKNvbchuOzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictions from num_teachers on test_train (9500)\n",
        "preds = np.array(true_label).reshape(num_teachers,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uus_01xMAGhL",
        "colab_type": "code",
        "outputId": "cc99a06e-dcfe-4d61-8a55-889c98241418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(mnist_label.shape,preds.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9500,) (80, 9500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ZitwWXJHKX",
        "colab_type": "code",
        "outputId": "c00ad053-2cd7-4be7-b0ca-3268672c9445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc_teacher/num_teachers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7457)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjKjyZ_aXPRo",
        "colab_type": "text"
      },
      "source": [
        "#Differential Privacy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QM5YdC3jVBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        },
        "outputId": "8fee992a-210a-4f09-96ba-6190aaedbd10"
      },
      "source": [
        "!pip install syft\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg (0.1.25a1)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.1)\n",
            "Requirement already satisfied: flask_socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages/Flask_SocketIO-4.2.1-py3.6.egg (from syft) (4.2.1)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Collecting msgpack>=0.6.1 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/7e/ae9e91c1bb8d846efafd1f353476e3fd7309778b582d2fb4cea4cc15b9a2/msgpack-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (248kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20kB 30.8MB/s eta 0:00:01\r\u001b[K     |████                            | 30kB 38.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 51kB 33.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 61kB 38.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 71kB 31.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 81kB 33.3MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 92kB 35.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 102kB 33.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 112kB 33.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 122kB 33.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 133kB 33.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 143kB 33.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 153kB 33.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 163kB 33.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 174kB 33.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 184kB 33.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 194kB 33.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 204kB 33.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 215kB 33.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 225kB 33.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 235kB 33.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 245kB 33.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256kB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.3)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: tf_encrypted!=0.5.7,>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.8)\n",
            "Requirement already satisfied: torch==1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: torchvision==0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: websocket_client>=0.56.0 in /usr/local/lib/python3.6/dist-packages/websocket_client-0.56.0-py3.6.egg (from syft) (0.56.0)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.0.2)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.3.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.5)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: python-socketio>=4.3.0 in /usr/local/lib/python3.6/dist-packages/python_socketio-4.3.1-py3.6.egg (from flask_socketio>=3.3.2->syft) (4.3.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf_encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf_encrypted!=0.5.7,>=0.5.4->syft) (5.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: python-engineio>=3.9.0 in /usr/local/lib/python3.6/dist-packages/python_engineio-3.9.3-py3.6.egg (from python-socketio>=4.3.0->flask_socketio>=3.3.2->syft) (3.9.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision==0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (41.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf_encrypted!=0.5.7,>=0.5.4->syft) (3.1.1)\n",
            "Installing collected packages: msgpack\n",
            "  Found existing installation: msgpack 0.5.6\n",
            "    Uninstalling msgpack-0.5.6:\n",
            "      Successfully uninstalled msgpack-0.5.6\n",
            "Successfully installed msgpack-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgzXenrNnG9p",
        "colab_type": "code",
        "outputId": "4eed68a2-6537-4a10-c558-0f9afd277c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "import syft as sy\n",
        "from syft.frameworks.torch.differential_privacy import pate"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0830 23:44:23.766313 140326995666816 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0830 23:44:23.779908 140326995666816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVZPCSvZ7aQU",
        "colab_type": "code",
        "outputId": "ccba640d-c7d1-4310-d1fa-f57b8da374d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mnist_label[:]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 9, 0, 1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkCuJVRVwtEe",
        "colab_type": "code",
        "outputId": "53c86e66-5c0e-4193-c7c2-19f6ef5be60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "preds[:,0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo-23B7zw5oY",
        "colab_type": "code",
        "outputId": "43fb5566-a28f-4715-f1a9-3279e25e16ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_labels = 10\n",
        "label_counts = np.bincount(preds[:,0], minlength=num_labels)\n",
        "label_counts"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0, 80,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKe7YGSKxGhI",
        "colab_type": "code",
        "outputId": "d81b8513-b223-4ad9-831d-feb4645cb70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_label = np.argmax(label_counts)\n",
        "new_label"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQSW3qKx2NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#iterate through all images\n",
        "num_labels = 10\n",
        "\n",
        "new_labels = list()\n",
        "for an_image in np.transpose(preds):\n",
        "\n",
        "    label_counts = np.bincount(an_image, minlength=num_labels)\n",
        "\n",
        "    new_label = np.argmax(label_counts)\n",
        "    \n",
        "    new_labels.append(new_label)\n",
        "    \n",
        "new_labels = np.array(new_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPVi1LVoOjaf",
        "colab_type": "code",
        "outputId": "46b63959-5b73-4a5e-8dc5-18e987c0d5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_labels.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZpCL_eGyBsR",
        "colab_type": "code",
        "outputId": "e6ec5ed9-35ec-45d8-e683-1e3a832eff75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(new_labels[:100])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 6 7 2 7\n",
            " 1 2 1 1 7 4 2 3 5 3 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 4 3 7 4 6 4 3 0 7 0 2 7\n",
            " 1 7 3 7 9 7 7 6 2 7 8 4 7 3 6 1 3 6 4 3 1 4 1 7 6 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IO6QR7FGPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dp_labels\n",
        "#iterate through 9500 images\n",
        "\n",
        "num_labels = 10\n",
        "\n",
        "dp_labels = list()\n",
        "for item in np.transpose(preds):\n",
        "\n",
        "    label_counts = np.bincount(item, minlength=num_labels)\n",
        "\n",
        "    epsilon = 0.1     #131.5/3000 = 0.044 68.77/3000 = 0.023\n",
        "    beta = 1 / epsilon\n",
        "\n",
        "    for i in range(len(label_counts)):\n",
        "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "    dp_label = np.argmax(label_counts)\n",
        "    \n",
        "    dp_labels.append(new_label)\n",
        "    \n",
        "dp_labels = np.array(new_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw0kHlEnVNIo",
        "colab_type": "code",
        "outputId": "27425e1e-d604-43a6-b0cd-615318f3c274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dp_labels.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HfGxX6UyWYA",
        "colab_type": "code",
        "outputId": "8fba0a58-b616-4722-86ba-a5e33b59719c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "mnist_label[0:100]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_RDhV_sBc8",
        "colab_type": "code",
        "outputId": "c57fac89-0e11-4e01-aeb5-212ac158e935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "new_labels[0:100]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 7, 1, 7, 3, 7, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 4, 3, 1, 4, 1, 7, 6, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wirnQKvnF_nW",
        "colab_type": "code",
        "outputId": "c28d02d6-8d6d-4f2e-8f93-386d5660fa5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dp_labels[0:100]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 7, 1, 7, 3, 7, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 4, 3, 1, 4, 1, 7, 6, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D6U28_DSROM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fed_accuracy(alt_labels, true_labels):\n",
        "    x =np.array( [(alt_labels[i] == true_labels[i]) for i in new_labels]).astype(int)\n",
        "    fed_accuracy = sum(x)/len(new_labels)\n",
        "    print(fed_accuracy)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hksa2XnqyBlT",
        "colab_type": "code",
        "outputId": "2b26689b-3cc3-44ab-c4db-c8626f52f67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# test_dataset(10000)\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=dp_labels, noise_eps=0.1, delta=1e-5) #,moments =20)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 411.5129254649703\n",
            "Data Dependent Epsilon: 93.89830871784949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOqtsDWrq_U",
        "colab_type": "code",
        "outputId": "33788061-d418-4b88-efc7-30734fbd2c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "fed_accuracy(dp_labels, mnist_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9151578947368421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_EYd406QvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY4sY-M9SuYx",
        "colab_type": "code",
        "outputId": "23ffff38-84ba-4ba9-9dc3-dd41a64652e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=dp_labels, noise_eps=0.1, delta=1e-5) #,moments =20)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 131.51292546497027\n",
            "Data Dependent Epsilon: 68.7707820260897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6fhFy5NyPH3",
        "colab_type": "code",
        "outputId": "3d31916d-6463-49a0-9cfe-9af15ff7f76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#data_ind_eps = 0.044\n",
        "fed_accuracy(dp_labels, mnist_label)\n",
        "#0.7833"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7833333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzRpa7z7Su9Q",
        "colab_type": "code",
        "outputId": "d374736a-56aa-4644-fd4a-1b092c86ca1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#data_ind_eps = 0.023\n",
        "fed_accuracy(dp_labels, mnist_label)\n",
        "#0.7833"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7833333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OakWL1IVd-VP",
        "colab_type": "text"
      },
      "source": [
        "#create new dp_dataset = images+dp_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_N6cYrQOgv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "722f21bb-547c-4bf1-9908-3af1d03b0ec1"
      },
      "source": [
        "new_ds = zip(dp_labels, imgs)\n",
        "\n",
        "result = list(new_ds)\n",
        "print(result[0])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7, tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255,\n",
            "          0.6235, 0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961,\n",
            "          0.9961, 0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765,\n",
            "          0.7765, 0.7765, 0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471,\n",
            "          0.2824, 0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961,\n",
            "          0.9961, 0.9804, 0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627,\n",
            "          0.2627, 0.2314, 0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0863, 0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.5059, 0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.2314, 0.9765, 0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.5216, 0.9961, 0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
            "          0.8039, 0.9725, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941,\n",
            "          0.9961, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843,\n",
            "          0.9412, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961,\n",
            "          0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588,\n",
            "          0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ--sKEorMi8",
        "colab_type": "code",
        "outputId": "fe92008b-d2e3-40b5-cbf9-19db5e881c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x=np.array([sublist[0] for sublist in result])\n",
        "x[:64].transpose()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDISGUV1U3bs",
        "colab_type": "code",
        "outputId": "a1edb00c-b795-465d-f633-53f19e5ec0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "imgs_arr = [item.numpy() for item in imgs]\n",
        "\n",
        "len(imgs_arr)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rVQzh45oYi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset = result,  batch_size=64,sampler = torch.utils.data.SequentialSampler(result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_KjpnpowST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlb = []\n",
        "for label,image in trainloader:\n",
        "  nlb.append(label)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW9RleKTTtYf",
        "colab_type": "code",
        "outputId": "c384c1c8-affd-4c0b-a4e9-c07a90c1a7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nlb[0]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
              "        4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 3, 2,\n",
              "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sCKNfmspbYf",
        "colab_type": "code",
        "outputId": "d784e9b2-8318-458d-859c-18227d03bdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dp_labels[:64]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWhPap0pO0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(nlb[:64]).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnoZsZYN1M4s",
        "colab_type": "text"
      },
      "source": [
        "#train local model on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrP89c73me32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainloader = valid_loader\n",
        "testloader = test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BejOj4x0n2NX",
        "colab_type": "code",
        "outputId": "991ea4af-aa50-45cd-8d78-2eed595c37b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "labels,images =next(iter(trainloader))\n",
        "print(images.shape, labels.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64]) torch.Size([64, 1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHGVDL04oO4",
        "colab_type": "code",
        "outputId": "c4b53cbb-5520-48b1-b435-67508e6788b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "local_model = Conv()\n",
        "local_model"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmzzrcLV8PaX",
        "colab_type": "code",
        "outputId": "b20bd174-49b2-4985-aedc-c97a64bde79c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reset_model_conv(local_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMhJfvTJ4U8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_1(device,criterion,optimizer):\n",
        "    \n",
        "    #device = device\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels  in testloader:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "            \n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            \n",
        "            #print(f\"top_p: {top_p}\")\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            #print(f\"equals: {equals}\")\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "            #print(f\"acc: {acc}\")\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r051uvRO4cny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_1():\n",
        "    \n",
        "    device = 'cuda'\n",
        "    epochs = 15\n",
        "    count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
        "    optimizer = optim.SGD(model.parameters(),lr= 0.01, momentum = 0.5)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for labels, images in trainloader:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Training pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #output = model(images)\n",
        "            output = model.forward(images)\n",
        "\n",
        "            #print(output.shape, labels.shape)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #print(train_count)\n",
        "            train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_1(device,criterion,optimizer)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader))\n",
        "            test_losses.append(test_loss/len(testloader))\n",
        "            accuracy.append(acc/len(testloader))\n",
        "        \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "                  \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                  )\n",
        "          \n",
        "          \n",
        "    return -1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd9Skf1wHNZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_count = 0\n",
        "for images, labels in trainloader:\n",
        "    \n",
        "    if train_count == int(len(test_train_ds)/batch_size):\n",
        "        print(f\"Num_samples = {train_count*batch_size}\")\n",
        "        break\n",
        "    train_count+=1  \n",
        "    print(train_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6d1mr8L4lb1",
        "colab_type": "code",
        "outputId": "b03a171c-0773-473a-aa28-2645689fa7d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        " #train local_model on test_ds\n",
        "  train_model_1()\n",
        "  \n",
        " "
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/15..  Training Loss: 0.462205..  Test Loss: 0.347439..  Test Accuracy: 0.894832\n",
            "Epoch: 2/15..  Training Loss: 0.438328..  Test Loss: 0.337567..  Test Accuracy: 0.902644\n",
            "Epoch: 3/15..  Training Loss: 0.403713..  Test Loss: 0.334742..  Test Accuracy: 0.892879\n",
            "Epoch: 4/15..  Training Loss: 0.369904..  Test Loss: 0.333914..  Test Accuracy: 0.898738\n",
            "Epoch: 5/15..  Training Loss: 0.343507..  Test Loss: 0.335415..  Test Accuracy: 0.900691\n",
            "Epoch: 6/15..  Training Loss: 0.338421..  Test Loss: 0.332802..  Test Accuracy: 0.906550\n",
            "Epoch: 7/15..  Training Loss: 0.320911..  Test Loss: 0.334526..  Test Accuracy: 0.902644\n",
            "Epoch: 8/15..  Training Loss: 0.312846..  Test Loss: 0.327000..  Test Accuracy: 0.894832\n",
            "Epoch: 9/15..  Training Loss: 0.319812..  Test Loss: 0.322322..  Test Accuracy: 0.906550\n",
            "Epoch: 10/15..  Training Loss: 0.309299..  Test Loss: 0.323379..  Test Accuracy: 0.908504\n",
            "Epoch: 11/15..  Training Loss: 0.295227..  Test Loss: 0.333967..  Test Accuracy: 0.906550\n",
            "Epoch: 12/15..  Training Loss: 0.295893..  Test Loss: 0.322825..  Test Accuracy: 0.906550\n",
            "Epoch: 13/15..  Training Loss: 0.289822..  Test Loss: 0.321939..  Test Accuracy: 0.900691\n",
            "Epoch: 14/15..  Training Loss: 0.290428..  Test Loss: 0.320111..  Test Accuracy: 0.908504\n",
            "Epoch: 15/15..  Training Loss: 0.282330..  Test Loss: 0.357445..  Test Accuracy: 0.900691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtx6uVHIRdmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Epoch: 15/15..  Training Loss: 0.282330..  Test Loss: 0.357445..  Test Accuracy: 0.900691\n",
        "#Epoch: 15/15..  Training Loss: 0.272720..  Test Loss: 0.347765..  Test Accuracy: 0.904147"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}