{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_ Differential_Privacy_Project_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "EOVp2h8Yr0Ws",
        "UnXn4-vlhxCr"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellen008/DeepL/blob/master/Final__Differential_Privacy_Project_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXCUUEMJ6-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "      #++++++++++++\n",
        "      #+++++++++split MNIST train dataset on train.train/train.valid\n",
        "\n",
        "      #+++++++++create 10 teachers' datasets from train.train //train_ds//\n",
        "      #+++++++++split 10 teachers' train.train dataset in train/valid\n",
        "      #output:\n",
        "      #teacher_train_loader = [1 ... 10]\n",
        "      #teacher_valid_loader = [1 ... 10]\n",
        "\n",
        "      #++++++++++++++++++++++++++\n",
        "      #+++++++++train_with_teacher() 10 models on 10 splitted trainsets\n",
        "\n",
        "      #++++++++++++++++++++++++++++   \n",
        "      #10 teachers model.forward(train.valid_dataset) //valid_ds//\n",
        "      #return 10 sets of true_labels = []  and mnist_labels = []\n",
        "\n",
        "      #+++++++++++++++++\n",
        "      #PATE analysis to figure out epsilon\n",
        "      #dp_labels = true_labels + laplacianM(epsilon, beta)\n",
        "\n",
        "      #++++++++++++++++++++++++++++++++\n",
        "      #split train.valid_dataset and true_labels on train/valid\n",
        "      #train.valid.train_loader\n",
        "      #train.valid.valid_loader\n",
        "\n",
        "      #+++++++++++++++++\n",
        "      #train_local_model(train.valid_dataset + dp_labels)\n",
        "\n",
        "      #++++++++++++++++++\n",
        "      #predict(MNIST test_dataset)\n",
        "      #compare predictions accuracy for true labels and dp_labels\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4-CKjx_4bix",
        "colab_type": "text"
      },
      "source": [
        "#Download and transform data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgnjb0HRJ69b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, SubsetRandomSampler, SequentialSampler\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQrGH00KJ69m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download the data\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),download = True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "\n",
        "#mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5icqaUjJ69v",
        "colab_type": "text"
      },
      "source": [
        "# Datasets (data + labels):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNC-rhLyJ69y",
        "colab_type": "code",
        "outputId": "6c3aa142-c6e3-4372-dad7-3f2f5fabc0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: ./data\n",
              "    Split: Train"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7YIf6sTJ697",
        "colab_type": "code",
        "outputId": "405dcb0d-4b77-4467-bd94-f17e9eb07db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8ZvacWKmHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_dataset(dataset, ratio):\n",
        "  \n",
        "    train_size = int(ratio * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "\n",
        "    train_ds, valid_ds = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "  \n",
        "    return train_ds,valid_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adxQOswdXHog",
        "colab_type": "code",
        "outputId": "5be47793-6a5f-43b0-d9fa-d60889f3c94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#split train dataset into train.train = train_ds/ train.valid = valid_ds\n",
        "\n",
        "test_train_ds, test_valid_ds = split_dataset(test_dataset,0.95)\n",
        "\n",
        "print(f\"test_train dataset = {len(test_train_ds)}\",\n",
        "      f\"\\ntest_valid dataset= {len(test_valid_ds)}\")\n",
        "print()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_train dataset = 9500 \n",
            "test_valid dataset= 500\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOVp2h8Yr0Ws",
        "colab_type": "text"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZaxYDGnl7Ro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,kernel_size = 5)\n",
        "        self.conv2 = nn.Conv2d(10,20, kernel_size = 5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        \n",
        "        x = F.max_pool2d(self.conv1(x),2)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(self.conv2_drop(self.conv2(x)),2)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(-1,320)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training = self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x,dim = 1)\n",
        "        \n",
        "        return x\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkfaP_C0J6-1",
        "colab_type": "code",
        "outputId": "f3035e1f-72b3-47cd-ff82-35370acf5025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "model = Conv()\n",
        "#local_model = Conv()\n",
        "model"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJg6k6He6hHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset the model\n",
        "def reset_model_conv(model):\n",
        "    model.conv1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.conv1.bias.data.fill_(0)\n",
        "    model.conv2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.conv2.bias.data.fill_(0)\n",
        "    model.fc1.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc1.bias.data.fill_(0)\n",
        "    model.fc2.weight.data.uniform_(0.0, 1.0)\n",
        "    model.fc2.bias.data.fill_(0)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vpljVdZbxri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYxMJPU8bxdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnXn4-vlhxCr",
        "colab_type": "text"
      },
      "source": [
        "# train model_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_QnLjVZ4x6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loaders for training a single model on\n",
        "#train.valid = valid_ds\n",
        "\n",
        "\n",
        "#trainloader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=64,shuffle = True)\n",
        "#testloader = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=64,shuffle = True)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset = p_train_ds,  batch_size=64,shuffle = True)\n",
        "testloader = torch.utils.data.DataLoader(dataset = p_valid_ds, batch_size=64,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeOlpKPcwrlN",
        "colab_type": "code",
        "outputId": "bc5118d7-ce9f-4dfc-c7ec-ef5c5754c79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#reset conv network:\n",
        "reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ2qaE-NJoY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_1(device,criterion,optimizer):\n",
        "    \n",
        "    #device = device\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels in testloader:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "            \n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            \n",
        "            #print(f\"top_p: {top_p}\")\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            #print(f\"equals: {equals}\")\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "            #print(f\"acc: {acc}\")\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2Sx4KD-Cgwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_1():\n",
        "    \n",
        "    device = 'cuda'\n",
        "    epochs = 10\n",
        "    count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
        "    optimizer = optim.SGD(model.parameters(),lr= 0.01, momentum = 0.5)\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader:\n",
        "            # Flatten MNIST images into a 784 long vector\n",
        "            #images = images.view(images.shape[0], -1)\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Training pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #output = model(images)\n",
        "            output = model.forward(images)\n",
        "\n",
        "            #print(output.shape, labels.shape)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #print(train_count)\n",
        "            train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_1(device,criterion,optimizer)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader))\n",
        "            test_losses.append(test_loss/len(testloader))\n",
        "            accuracy.append(acc/len(testloader))\n",
        "        \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "                  \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                  )\n",
        "            \n",
        "        \n",
        "    return -1\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur2iZ7UyI9_L",
        "colab_type": "code",
        "outputId": "8e67a081-dd2e-4458-d41d-831dfade7dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        " train_model_1()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10..  Training Loss: 2.295185..  Test Loss: 2.275266..  Test Accuracy: 0.251919\n",
            "Epoch: 2/10..  Training Loss: 2.201518..  Test Loss: 1.952294..  Test Accuracy: 0.651590\n",
            "Epoch: 3/10..  Training Loss: 1.529691..  Test Loss: 0.764752..  Test Accuracy: 0.827303\n",
            "Epoch: 4/10..  Training Loss: 0.940943..  Test Loss: 0.493012..  Test Accuracy: 0.860746\n",
            "Epoch: 5/10..  Training Loss: 0.744956..  Test Loss: 0.395128..  Test Accuracy: 0.885691\n",
            "Epoch: 6/10..  Training Loss: 0.650084..  Test Loss: 0.332801..  Test Accuracy: 0.894189\n",
            "Epoch: 7/10..  Training Loss: 0.584911..  Test Loss: 0.295193..  Test Accuracy: 0.912007\n",
            "Epoch: 8/10..  Training Loss: 0.529476..  Test Loss: 0.262136..  Test Accuracy: 0.907621\n",
            "Epoch: 9/10..  Training Loss: 0.505346..  Test Loss: 0.244295..  Test Accuracy: 0.924890\n",
            "Epoch: 10/10..  Training Loss: 0.471746..  Test Loss: 0.228448..  Test Accuracy: 0.923246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJKFMP__ta_r",
        "colab_type": "code",
        "outputId": "70d1cb1f-f9e6-433a-ed35-eaed33a8e7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#check predicitions:\n",
        "\n",
        "model.to('cpu')\n",
        "\n",
        "images, labels = next(iter(testloader))\n",
        "# Get the class probabilities\n",
        "ps = torch.exp(model(images))\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)\n",
        "\n",
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "# Look at the most likely classes for the first 10 examples\n",
        "print(top_class[10:15,:])\n",
        "print(top_class.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n",
            "tensor([[3],\n",
            "        [4],\n",
            "        [8],\n",
            "        [5],\n",
            "        [5]])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7o96o7TJ6-h",
        "colab_type": "text"
      },
      "source": [
        "# Let's divide train data(60000) into \"*number _of_teachers*\" private datasets :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3uppZ1bbNUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_train_teacher(ds):\n",
        "\n",
        "    num_teachers = 80\n",
        "    batch_size = 64\n",
        "    \n",
        "    max_ind = len (ds)\n",
        "    print(len(ds))\n",
        "    \n",
        "    teacher_len = int(max_ind/num_teachers)\n",
        "\n",
        "    teacher_loader = []\n",
        "    \n",
        "    start = 0\n",
        "    stop = teacher_len           \n",
        "    \n",
        "       \n",
        "    #print(f\"start = {start}\",f\"train_size = {train_size}\",f\"test_size = {test_size}\")\n",
        "\n",
        "    indicies = torch.randperm(max_ind)\n",
        "    \n",
        "\n",
        "    for i in range(num_teachers):\n",
        "      \n",
        "        idx =[j for j in range(start,stop)]\n",
        "        idx = indicies[start:stop]\n",
        "        \n",
        "        #print(\"Teacher indicies = \",start,\" | \", stop, \" | \",{stop-start})\n",
        "    \n",
        "        teacher_loader.append(torch.utils.data.DataLoader( ds, batch_size=batch_size, sampler = SubsetRandomSampler(idx)))\n",
        "\n",
        "        #print(f\"teacher_loader = {len(teacher_loader[i])}\")\n",
        "    \n",
        "        start = stop\n",
        "        stop = stop+teacher_len\n",
        "        \n",
        "    return  teacher_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1370Ma4ZR8P",
        "colab_type": "code",
        "outputId": "2e114c69-f4e1-482f-e7fe-9b5ec625d66d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#loaders for teacher training process\n",
        "batch_size = 64\n",
        "\n",
        "teacher_train_loader = split_train_teacher(train_dataset)    #60000/80 = 750 samples per teacher\n",
        "\n",
        "#500 samples  reused for every teacher validation\n",
        "teacher_valid_loader = torch.utils.data.DataLoader( test_valid_ds, batch_size=batch_size, shuffle = True)   \n",
        "\n",
        "print(f\"teacher_train_loader = {len(teacher_train_loader)}\",f\"teacher_valid_loader = {len(teacher_valid_loader)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000\n",
            "teacher_train_loader = 80 teacher_valid_loader = 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf91DG9xGcc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loader for fed_labels production (9500) from test_dataset (10000)\n",
        "\n",
        "batch_size = 64\n",
        "split = 9500\n",
        "idx =[j for j in range(len(test_dataset))]\n",
        "train_id = [j for j in range(split)]\n",
        "test_id = [j for j in range(split,len(test_dataset))]\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, sampler = torch.utils.data.SequentialSampler(train_id))\n",
        "test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, sampler = torch.utils.data.SequentialSampler(test_id))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v3Roi8o-POv",
        "colab_type": "code",
        "outputId": "0d2b8375-0633-4b3c-8f31-b88214477d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONhjX9YV89EQ",
        "colab_type": "code",
        "outputId": "46b03e8c-6f81-4819-ff8b-698ebe8b63fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZGRTK2qr7QC",
        "colab_type": "code",
        "outputId": "abac9bbe-41f7-4dd4-9a2c-58935e78efb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(valid_loader),len(test_loader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "149 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cek7VYhT_4ep",
        "colab_type": "text"
      },
      "source": [
        "# Train model on 10 teacher datadets \n",
        "#Predict 10 sets of true_labels[ 10 ] on valid dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsjvc2-VC628",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset_model_conv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kun97w7Wv6xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_t(device,testloader_t,criterion):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels in testloader_t:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "\n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels)\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            \n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc \n",
        "  \n",
        "\n",
        "def print_test_accuracy(e,epochs,accuracy):\n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                 )\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwiligkBwDYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_t(trainloader_t,testloader_t):\n",
        "    \n",
        "    #device = 'cuda'\n",
        "    epochs = 12\n",
        "    #count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    \n",
        "    #testloader = testloader\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    true_labels = []\n",
        "    dc_valid = []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader_t:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model.forward(images)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_t(device,testloader_t,criterion)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader_t))\n",
        "            test_losses.append(test_loss/len(testloader_t))\n",
        "            accuracy.append(acc/len(testloader_t))\n",
        "        \n",
        "            #print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "             #     \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "             #    \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "             #    \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "             #    )\n",
        "            print_test_accuracy(e,epochs,accuracy) \n",
        "            \n",
        "    return -1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYw_HLlvwsEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train teacher_model and make predictions\n",
        "\n",
        "device = 'cuda'\n",
        "preds = []\n",
        "true_labels = []\n",
        "dc_valid = torch.Tensor(0).cpu()\n",
        "\n",
        "for t in range(num_teachers):\n",
        "    reset_model_conv(model)\n",
        "    print(f\"Teacher {t+1}\")\n",
        "    #instansiate next teacher model\n",
        "    model = Conv()\n",
        "    \n",
        "    \n",
        "    #train next teacher model\n",
        "    train_model_t(teacher_train_loader[t],teacher_valid_loader[t])\n",
        "    \n",
        "    batch_preds = []\n",
        "    batch_count = 0\n",
        "    dc_t = torch.Tensor(0).cpu()\n",
        "    \n",
        "    for images, label in valid_loader:\n",
        "        \n",
        "        #images, labels = images.to(device), labels.to(device)\n",
        "        images = images.to(device)\n",
        "        true_labels.append(label)\n",
        "        \n",
        "        ps = torch.exp(model.forward(images))\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        \n",
        "        #batch_preds.append(top_class.flatten())\n",
        "        dc_batch = top_class.flatten().float().cpu()        \n",
        "        dc_t = torch.cat((dc_t,dc_batch),0).view(-1)\n",
        "        \n",
        "\n",
        "    #preds.append(np.array(batch_preds))\n",
        "    dc_valid = torch.cat((dc_valid,dc_t),0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4lsb8CnkXd",
        "colab_type": "text"
      },
      "source": [
        "#train teachers and produce federated labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY0q9QfqcVND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_t_labels(device,testloader_t,criterion):\n",
        "    \n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    #v_count = 0\n",
        "    \n",
        "    #dc_t = torch.Tensor(0).cpu()\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "        for inputs, labels in testloader_t:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "\n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels)\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "          \n",
        "    return test_loss, acc\n",
        "  \n",
        "\n",
        "def print_test_accuracy(e,epochs,accuracy):\n",
        "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                 )\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuOXL8OTmVft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_t_labels(trainloader_t,testloader_t,valid_loader):\n",
        "    \n",
        "    #device = 'cuda'\n",
        "    epochs = 12\n",
        "    #count = 0\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for images, labels in trainloader_t:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model.forward(images)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_t_labels(device,testloader_t,criterion)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader_t))\n",
        "            test_losses.append(test_loss/len(testloader_t))\n",
        "            accuracy.append(acc/len(testloader_t))\n",
        "            \n",
        "            \n",
        "        \n",
        "            #print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "             #     \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "             #    \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "             #    \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "             #    )\n",
        "            print_test_accuracy(e,epochs,accuracy)\n",
        "            \n",
        "    #produce true_labels\n",
        "    \n",
        "    acc_teacher = 0\n",
        "    v_count = 0\n",
        "    labels_teacher = torch.Tensor(0).cpu()\n",
        "    ds_labels = torch.FloatTensor(0).cpu()\n",
        "    \n",
        "    #preserve sequence of images,labels in valid_loader\n",
        "    imgs = []\n",
        "    labs = []\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "        for inputs, labels in valid_loader:\n",
        "            #print(\"batch # \",v_count)\n",
        "            \n",
        "            #preserve the sequence of images and labels\n",
        "            imgs.extend(inputs)\n",
        "            labs.extend(labels.flatten().numpy())\n",
        "            \n",
        "            \n",
        "            \n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "                                                 #print(labels.shape,log_ps.shape)\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            acc_teacher += torch.mean(equals.type(torch.FloatTensor))\n",
        "            \n",
        "            #collect original MNIST labels\n",
        "            ds_labels = torch.cat((ds_labels,labels.float().cpu()),0).view(-1)\n",
        "            #print(\"dataset_labels \",len(ds_labels))\n",
        "            \n",
        "            \n",
        "            #Federated learning labels batch tensor\n",
        "            labels_batch = top_class.flatten().float().cpu()\n",
        "            #print(\"dc_batch.size \", dc_batch.size())\n",
        "            labels_teacher = torch.cat((labels_teacher,labels_batch),0).view(-1)\n",
        "            #print(\"labels_teacher.size \",labels_teacher.size())\n",
        "          \n",
        "            \n",
        "            v_count +=1\n",
        "          \n",
        "    #return labels_teacher, acc_teacher, mnist_labels\n",
        "    \n",
        "    \n",
        "            \n",
        "    return labels_teacher, ds_labels, acc_teacher, imgs, labs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2FqlvjWu0jF",
        "colab_type": "code",
        "outputId": "6ececee0-c058-487a-a98c-36de05e5c2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_teachers = 80\n",
        "device = 'cuda'\n",
        "#true_labels = torch.Tensor(0).cpu()\n",
        "true_labels = []\n",
        "mnist_labels = []\n",
        "accuracy_teacher = []\n",
        "\n",
        "for t in range(num_teachers):         \n",
        "    reset_model_conv(model)\n",
        "    \n",
        "    print(f\"Teacher {t}\")\n",
        "    #instansiate next teacher model\n",
        "    model = Conv()\n",
        "    \n",
        "    #train next teacher model\n",
        "    #labels_teacher, ds_labels, acc_teacher = train_model_t_labels(teacher_train_loader[t],teacher_valid_loader[t])\n",
        "    labels_teacher, ds_labels, acc_teacher, imgs, labs = train_model_t_labels(teacher_train_loader[t], teacher_valid_loader,valid_loader)\n",
        "    \n",
        "    #collect true_labels, mnist_labels for the teacher\n",
        "    true_labels.append(labels_teacher.cpu())\n",
        "    mnist_labels.append(ds_labels.cpu())\n",
        "    accuracy_teacher.append(acc_teacher.cpu())\n",
        "    \n",
        "#save sequence of images for local_model training\n",
        "\n",
        "#imgs_iter = itertools.chain(imgs)\n",
        "#labs_iter = itertools.chain(labs)\n",
        "    #print(f\"Teacher {t} has been trained\\n\\n\")\n",
        "    \n",
        "print(f\"All {t+1} Teachers  has been trained\\n\\n\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Teacher 0\n",
            "Epoch: 1/12..  Test Accuracy: 0.095102\n",
            "Epoch: 2/12..  Test Accuracy: 0.144231\n",
            "Epoch: 3/12..  Test Accuracy: 0.620042\n",
            "Epoch: 4/12..  Test Accuracy: 0.643329\n",
            "Epoch: 5/12..  Test Accuracy: 0.780649\n",
            "Epoch: 6/12..  Test Accuracy: 0.801532\n",
            "Epoch: 7/12..  Test Accuracy: 0.827524\n",
            "Epoch: 8/12..  Test Accuracy: 0.844501\n",
            "Epoch: 9/12..  Test Accuracy: 0.862981\n",
            "Epoch: 10/12..  Test Accuracy: 0.842097\n",
            "Epoch: 11/12..  Test Accuracy: 0.863582\n",
            "Epoch: 12/12..  Test Accuracy: 0.876202\n",
            "Teacher 1\n",
            "Epoch: 1/12..  Test Accuracy: 0.223708\n",
            "Epoch: 2/12..  Test Accuracy: 0.224309\n",
            "Epoch: 3/12..  Test Accuracy: 0.470252\n",
            "Epoch: 4/12..  Test Accuracy: 0.602464\n",
            "Epoch: 5/12..  Test Accuracy: 0.697266\n",
            "Epoch: 6/12..  Test Accuracy: 0.773287\n",
            "Epoch: 7/12..  Test Accuracy: 0.787710\n",
            "Epoch: 8/12..  Test Accuracy: 0.812350\n",
            "Epoch: 9/12..  Test Accuracy: 0.819712\n",
            "Epoch: 10/12..  Test Accuracy: 0.840595\n",
            "Epoch: 11/12..  Test Accuracy: 0.855619\n",
            "Epoch: 12/12..  Test Accuracy: 0.849008\n",
            "Teacher 2\n",
            "Epoch: 1/12..  Test Accuracy: 0.266977\n",
            "Epoch: 2/12..  Test Accuracy: 0.399940\n",
            "Epoch: 3/12..  Test Accuracy: 0.515625\n",
            "Epoch: 4/12..  Test Accuracy: 0.632963\n",
            "Epoch: 5/12..  Test Accuracy: 0.754357\n",
            "Epoch: 6/12..  Test Accuracy: 0.793269\n",
            "Epoch: 7/12..  Test Accuracy: 0.840595\n",
            "Epoch: 8/12..  Test Accuracy: 0.849459\n",
            "Epoch: 9/12..  Test Accuracy: 0.866887\n",
            "Epoch: 10/12..  Test Accuracy: 0.890925\n",
            "Epoch: 11/12..  Test Accuracy: 0.887470\n",
            "Epoch: 12/12..  Test Accuracy: 0.879808\n",
            "Teacher 3\n",
            "Epoch: 1/12..  Test Accuracy: 0.366887\n",
            "Epoch: 2/12..  Test Accuracy: 0.485427\n",
            "Epoch: 3/12..  Test Accuracy: 0.627103\n",
            "Epoch: 4/12..  Test Accuracy: 0.736328\n",
            "Epoch: 5/12..  Test Accuracy: 0.766376\n",
            "Epoch: 6/12..  Test Accuracy: 0.795222\n",
            "Epoch: 7/12..  Test Accuracy: 0.838942\n",
            "Epoch: 8/12..  Test Accuracy: 0.829177\n",
            "Epoch: 9/12..  Test Accuracy: 0.859525\n",
            "Epoch: 10/12..  Test Accuracy: 0.849760\n",
            "Epoch: 11/12..  Test Accuracy: 0.855769\n",
            "Epoch: 12/12..  Test Accuracy: 0.877404\n",
            "Teacher 4\n",
            "Epoch: 1/12..  Test Accuracy: 0.089093\n",
            "Epoch: 2/12..  Test Accuracy: 0.301232\n",
            "Epoch: 3/12..  Test Accuracy: 0.544171\n",
            "Epoch: 4/12..  Test Accuracy: 0.630559\n",
            "Epoch: 5/12..  Test Accuracy: 0.752554\n",
            "Epoch: 6/12..  Test Accuracy: 0.804087\n",
            "Epoch: 7/12..  Test Accuracy: 0.835787\n",
            "Epoch: 8/12..  Test Accuracy: 0.841647\n",
            "Epoch: 9/12..  Test Accuracy: 0.840895\n",
            "Epoch: 10/12..  Test Accuracy: 0.856070\n",
            "Epoch: 11/12..  Test Accuracy: 0.864934\n",
            "Epoch: 12/12..  Test Accuracy: 0.873648\n",
            "Teacher 5\n",
            "Epoch: 1/12..  Test Accuracy: 0.093149\n",
            "Epoch: 2/12..  Test Accuracy: 0.181340\n",
            "Epoch: 3/12..  Test Accuracy: 0.410457\n",
            "Epoch: 4/12..  Test Accuracy: 0.581581\n",
            "Epoch: 5/12..  Test Accuracy: 0.754958\n",
            "Epoch: 6/12..  Test Accuracy: 0.803185\n",
            "Epoch: 7/12..  Test Accuracy: 0.814303\n",
            "Epoch: 8/12..  Test Accuracy: 0.838642\n",
            "Epoch: 9/12..  Test Accuracy: 0.842548\n",
            "Epoch: 10/12..  Test Accuracy: 0.856821\n",
            "Epoch: 11/12..  Test Accuracy: 0.869892\n",
            "Epoch: 12/12..  Test Accuracy: 0.875300\n",
            "Teacher 6\n",
            "Epoch: 1/12..  Test Accuracy: 0.087290\n",
            "Epoch: 2/12..  Test Accuracy: 0.250000\n",
            "Epoch: 3/12..  Test Accuracy: 0.640775\n",
            "Epoch: 4/12..  Test Accuracy: 0.774940\n",
            "Epoch: 5/12..  Test Accuracy: 0.806490\n",
            "Epoch: 6/12..  Test Accuracy: 0.814303\n",
            "Epoch: 7/12..  Test Accuracy: 0.837740\n",
            "Epoch: 8/12..  Test Accuracy: 0.828576\n",
            "Epoch: 9/12..  Test Accuracy: 0.862230\n",
            "Epoch: 10/12..  Test Accuracy: 0.869892\n",
            "Epoch: 11/12..  Test Accuracy: 0.867939\n",
            "Epoch: 12/12..  Test Accuracy: 0.878606\n",
            "Teacher 7\n",
            "Epoch: 1/12..  Test Accuracy: 0.090144\n",
            "Epoch: 2/12..  Test Accuracy: 0.226112\n",
            "Epoch: 3/12..  Test Accuracy: 0.606821\n",
            "Epoch: 4/12..  Test Accuracy: 0.708383\n",
            "Epoch: 5/12..  Test Accuracy: 0.746094\n",
            "Epoch: 6/12..  Test Accuracy: 0.795673\n",
            "Epoch: 7/12..  Test Accuracy: 0.825871\n",
            "Epoch: 8/12..  Test Accuracy: 0.822115\n",
            "Epoch: 9/12..  Test Accuracy: 0.851412\n",
            "Epoch: 10/12..  Test Accuracy: 0.857272\n",
            "Epoch: 11/12..  Test Accuracy: 0.867338\n",
            "Epoch: 12/12..  Test Accuracy: 0.866436\n",
            "Teacher 8\n",
            "Epoch: 1/12..  Test Accuracy: 0.193810\n",
            "Epoch: 2/12..  Test Accuracy: 0.090745\n",
            "Epoch: 3/12..  Test Accuracy: 0.346454\n",
            "Epoch: 4/12..  Test Accuracy: 0.561899\n",
            "Epoch: 5/12..  Test Accuracy: 0.573618\n",
            "Epoch: 6/12..  Test Accuracy: 0.661058\n",
            "Epoch: 7/12..  Test Accuracy: 0.711388\n",
            "Epoch: 8/12..  Test Accuracy: 0.750451\n",
            "Epoch: 9/12..  Test Accuracy: 0.761268\n",
            "Epoch: 10/12..  Test Accuracy: 0.811448\n",
            "Epoch: 11/12..  Test Accuracy: 0.811298\n",
            "Epoch: 12/12..  Test Accuracy: 0.803636\n",
            "Teacher 9\n",
            "Epoch: 1/12..  Test Accuracy: 0.110727\n",
            "Epoch: 2/12..  Test Accuracy: 0.306040\n",
            "Epoch: 3/12..  Test Accuracy: 0.562500\n",
            "Epoch: 4/12..  Test Accuracy: 0.695463\n",
            "Epoch: 5/12..  Test Accuracy: 0.727915\n",
            "Epoch: 6/12..  Test Accuracy: 0.806641\n",
            "Epoch: 7/12..  Test Accuracy: 0.816707\n",
            "Epoch: 8/12..  Test Accuracy: 0.852013\n",
            "Epoch: 9/12..  Test Accuracy: 0.851262\n",
            "Epoch: 10/12..  Test Accuracy: 0.867488\n",
            "Epoch: 11/12..  Test Accuracy: 0.874850\n",
            "Epoch: 12/12..  Test Accuracy: 0.881611\n",
            "Teacher 10\n",
            "Epoch: 1/12..  Test Accuracy: 0.296424\n",
            "Epoch: 2/12..  Test Accuracy: 0.443359\n",
            "Epoch: 3/12..  Test Accuracy: 0.623197\n",
            "Epoch: 4/12..  Test Accuracy: 0.718299\n",
            "Epoch: 5/12..  Test Accuracy: 0.750300\n",
            "Epoch: 6/12..  Test Accuracy: 0.803936\n",
            "Epoch: 7/12..  Test Accuracy: 0.823918\n",
            "Epoch: 8/12..  Test Accuracy: 0.820763\n",
            "Epoch: 9/12..  Test Accuracy: 0.846004\n",
            "Epoch: 10/12..  Test Accuracy: 0.850361\n",
            "Epoch: 11/12..  Test Accuracy: 0.859225\n",
            "Epoch: 12/12..  Test Accuracy: 0.877704\n",
            "Teacher 11\n",
            "Epoch: 1/12..  Test Accuracy: 0.253305\n",
            "Epoch: 2/12..  Test Accuracy: 0.348558\n",
            "Epoch: 3/12..  Test Accuracy: 0.516076\n",
            "Epoch: 4/12..  Test Accuracy: 0.601562\n",
            "Epoch: 5/12..  Test Accuracy: 0.728516\n",
            "Epoch: 6/12..  Test Accuracy: 0.760667\n",
            "Epoch: 7/12..  Test Accuracy: 0.817308\n",
            "Epoch: 8/12..  Test Accuracy: 0.833383\n",
            "Epoch: 9/12..  Test Accuracy: 0.845102\n",
            "Epoch: 10/12..  Test Accuracy: 0.870192\n",
            "Epoch: 11/12..  Test Accuracy: 0.862079\n",
            "Epoch: 12/12..  Test Accuracy: 0.901142\n",
            "Teacher 12\n",
            "Epoch: 1/12..  Test Accuracy: 0.249850\n",
            "Epoch: 2/12..  Test Accuracy: 0.297776\n",
            "Epoch: 3/12..  Test Accuracy: 0.461088\n",
            "Epoch: 4/12..  Test Accuracy: 0.527193\n",
            "Epoch: 5/12..  Test Accuracy: 0.664964\n",
            "Epoch: 6/12..  Test Accuracy: 0.728966\n",
            "Epoch: 7/12..  Test Accuracy: 0.798377\n",
            "Epoch: 8/12..  Test Accuracy: 0.822716\n",
            "Epoch: 9/12..  Test Accuracy: 0.829026\n",
            "Epoch: 10/12..  Test Accuracy: 0.844501\n",
            "Epoch: 11/12..  Test Accuracy: 0.859225\n",
            "Epoch: 12/12..  Test Accuracy: 0.853816\n",
            "Teacher 13\n",
            "Epoch: 1/12..  Test Accuracy: 0.262169\n",
            "Epoch: 2/12..  Test Accuracy: 0.355168\n",
            "Epoch: 3/12..  Test Accuracy: 0.513672\n",
            "Epoch: 4/12..  Test Accuracy: 0.587139\n",
            "Epoch: 5/12..  Test Accuracy: 0.721755\n",
            "Epoch: 6/12..  Test Accuracy: 0.784555\n",
            "Epoch: 7/12..  Test Accuracy: 0.825571\n",
            "Epoch: 8/12..  Test Accuracy: 0.841196\n",
            "Epoch: 9/12..  Test Accuracy: 0.857873\n",
            "Epoch: 10/12..  Test Accuracy: 0.865535\n",
            "Epoch: 11/12..  Test Accuracy: 0.870343\n",
            "Epoch: 12/12..  Test Accuracy: 0.871845\n",
            "Teacher 14\n",
            "Epoch: 1/12..  Test Accuracy: 0.307091\n",
            "Epoch: 2/12..  Test Accuracy: 0.319862\n",
            "Epoch: 3/12..  Test Accuracy: 0.505859\n",
            "Epoch: 4/12..  Test Accuracy: 0.676382\n",
            "Epoch: 5/12..  Test Accuracy: 0.743540\n",
            "Epoch: 6/12..  Test Accuracy: 0.790865\n",
            "Epoch: 7/12..  Test Accuracy: 0.828876\n",
            "Epoch: 8/12..  Test Accuracy: 0.843600\n",
            "Epoch: 9/12..  Test Accuracy: 0.857272\n",
            "Epoch: 10/12..  Test Accuracy: 0.853816\n",
            "Epoch: 11/12..  Test Accuracy: 0.863582\n",
            "Epoch: 12/12..  Test Accuracy: 0.860877\n",
            "Teacher 15\n",
            "Epoch: 1/12..  Test Accuracy: 0.314904\n",
            "Epoch: 2/12..  Test Accuracy: 0.504207\n",
            "Epoch: 3/12..  Test Accuracy: 0.647686\n",
            "Epoch: 4/12..  Test Accuracy: 0.728966\n",
            "Epoch: 5/12..  Test Accuracy: 0.769982\n",
            "Epoch: 6/12..  Test Accuracy: 0.833383\n",
            "Epoch: 7/12..  Test Accuracy: 0.843600\n",
            "Epoch: 8/12..  Test Accuracy: 0.861629\n",
            "Epoch: 9/12..  Test Accuracy: 0.875751\n",
            "Epoch: 10/12..  Test Accuracy: 0.864483\n",
            "Epoch: 11/12..  Test Accuracy: 0.885968\n",
            "Epoch: 12/12..  Test Accuracy: 0.883864\n",
            "Teacher 16\n",
            "Epoch: 1/12..  Test Accuracy: 0.251803\n",
            "Epoch: 2/12..  Test Accuracy: 0.316406\n",
            "Epoch: 3/12..  Test Accuracy: 0.499399\n",
            "Epoch: 4/12..  Test Accuracy: 0.676983\n",
            "Epoch: 5/12..  Test Accuracy: 0.715895\n",
            "Epoch: 6/12..  Test Accuracy: 0.751653\n",
            "Epoch: 7/12..  Test Accuracy: 0.835787\n",
            "Epoch: 8/12..  Test Accuracy: 0.837440\n",
            "Epoch: 9/12..  Test Accuracy: 0.862680\n",
            "Epoch: 10/12..  Test Accuracy: 0.873498\n",
            "Epoch: 11/12..  Test Accuracy: 0.885968\n",
            "Epoch: 12/12..  Test Accuracy: 0.904898\n",
            "Teacher 17\n",
            "Epoch: 1/12..  Test Accuracy: 0.172025\n",
            "Epoch: 2/12..  Test Accuracy: 0.229417\n",
            "Epoch: 3/12..  Test Accuracy: 0.606971\n",
            "Epoch: 4/12..  Test Accuracy: 0.745042\n",
            "Epoch: 5/12..  Test Accuracy: 0.807091\n",
            "Epoch: 6/12..  Test Accuracy: 0.830829\n",
            "Epoch: 7/12..  Test Accuracy: 0.844952\n",
            "Epoch: 8/12..  Test Accuracy: 0.855168\n",
            "Epoch: 9/12..  Test Accuracy: 0.861929\n",
            "Epoch: 10/12..  Test Accuracy: 0.872296\n",
            "Epoch: 11/12..  Test Accuracy: 0.865986\n",
            "Epoch: 12/12..  Test Accuracy: 0.867939\n",
            "Teacher 18\n",
            "Epoch: 1/12..  Test Accuracy: 0.117488\n",
            "Epoch: 2/12..  Test Accuracy: 0.344651\n",
            "Epoch: 3/12..  Test Accuracy: 0.368389\n",
            "Epoch: 4/12..  Test Accuracy: 0.636719\n",
            "Epoch: 5/12..  Test Accuracy: 0.742638\n",
            "Epoch: 6/12..  Test Accuracy: 0.773438\n",
            "Epoch: 7/12..  Test Accuracy: 0.811148\n",
            "Epoch: 8/12..  Test Accuracy: 0.827073\n",
            "Epoch: 9/12..  Test Accuracy: 0.842999\n",
            "Epoch: 10/12..  Test Accuracy: 0.841647\n",
            "Epoch: 11/12..  Test Accuracy: 0.866436\n",
            "Epoch: 12/12..  Test Accuracy: 0.865385\n",
            "Teacher 19\n",
            "Epoch: 1/12..  Test Accuracy: 0.107722\n",
            "Epoch: 2/12..  Test Accuracy: 0.319712\n",
            "Epoch: 3/12..  Test Accuracy: 0.617939\n",
            "Epoch: 4/12..  Test Accuracy: 0.657752\n",
            "Epoch: 5/12..  Test Accuracy: 0.730919\n",
            "Epoch: 6/12..  Test Accuracy: 0.786208\n",
            "Epoch: 7/12..  Test Accuracy: 0.825421\n",
            "Epoch: 8/12..  Test Accuracy: 0.836238\n",
            "Epoch: 9/12..  Test Accuracy: 0.829477\n",
            "Epoch: 10/12..  Test Accuracy: 0.861178\n",
            "Epoch: 11/12..  Test Accuracy: 0.862230\n",
            "Epoch: 12/12..  Test Accuracy: 0.857272\n",
            "Teacher 20\n",
            "Epoch: 1/12..  Test Accuracy: 0.142728\n",
            "Epoch: 2/12..  Test Accuracy: 0.258864\n",
            "Epoch: 3/12..  Test Accuracy: 0.565054\n",
            "Epoch: 4/12..  Test Accuracy: 0.623347\n",
            "Epoch: 5/12..  Test Accuracy: 0.682843\n",
            "Epoch: 6/12..  Test Accuracy: 0.748498\n",
            "Epoch: 7/12..  Test Accuracy: 0.795373\n",
            "Epoch: 8/12..  Test Accuracy: 0.806040\n",
            "Epoch: 9/12..  Test Accuracy: 0.827674\n",
            "Epoch: 10/12..  Test Accuracy: 0.828576\n",
            "Epoch: 11/12..  Test Accuracy: 0.856220\n",
            "Epoch: 12/12..  Test Accuracy: 0.867037\n",
            "Teacher 21\n",
            "Epoch: 1/12..  Test Accuracy: 0.137770\n",
            "Epoch: 2/12..  Test Accuracy: 0.398438\n",
            "Epoch: 3/12..  Test Accuracy: 0.477614\n",
            "Epoch: 4/12..  Test Accuracy: 0.697266\n",
            "Epoch: 5/12..  Test Accuracy: 0.731520\n",
            "Epoch: 6/12..  Test Accuracy: 0.815355\n",
            "Epoch: 7/12..  Test Accuracy: 0.838191\n",
            "Epoch: 8/12..  Test Accuracy: 0.853215\n",
            "Epoch: 9/12..  Test Accuracy: 0.873648\n",
            "Epoch: 10/12..  Test Accuracy: 0.873798\n",
            "Epoch: 11/12..  Test Accuracy: 0.879057\n",
            "Epoch: 12/12..  Test Accuracy: 0.888371\n",
            "Teacher 22\n",
            "Epoch: 1/12..  Test Accuracy: 0.189603\n",
            "Epoch: 2/12..  Test Accuracy: 0.401142\n",
            "Epoch: 3/12..  Test Accuracy: 0.480018\n",
            "Epoch: 4/12..  Test Accuracy: 0.694862\n",
            "Epoch: 5/12..  Test Accuracy: 0.770132\n",
            "Epoch: 6/12..  Test Accuracy: 0.797776\n",
            "Epoch: 7/12..  Test Accuracy: 0.825871\n",
            "Epoch: 8/12..  Test Accuracy: 0.837740\n",
            "Epoch: 9/12..  Test Accuracy: 0.844050\n",
            "Epoch: 10/12..  Test Accuracy: 0.857722\n",
            "Epoch: 11/12..  Test Accuracy: 0.864483\n",
            "Epoch: 12/12..  Test Accuracy: 0.871394\n",
            "Teacher 23\n",
            "Epoch: 1/12..  Test Accuracy: 0.174129\n",
            "Epoch: 2/12..  Test Accuracy: 0.439603\n",
            "Epoch: 3/12..  Test Accuracy: 0.563251\n",
            "Epoch: 4/12..  Test Accuracy: 0.719802\n",
            "Epoch: 5/12..  Test Accuracy: 0.805138\n",
            "Epoch: 6/12..  Test Accuracy: 0.810547\n",
            "Epoch: 7/12..  Test Accuracy: 0.850962\n",
            "Epoch: 8/12..  Test Accuracy: 0.867939\n",
            "Epoch: 9/12..  Test Accuracy: 0.873648\n",
            "Epoch: 10/12..  Test Accuracy: 0.884916\n",
            "Epoch: 11/12..  Test Accuracy: 0.880108\n",
            "Epoch: 12/12..  Test Accuracy: 0.899639\n",
            "Teacher 24\n",
            "Epoch: 1/12..  Test Accuracy: 0.120643\n",
            "Epoch: 2/12..  Test Accuracy: 0.189603\n",
            "Epoch: 3/12..  Test Accuracy: 0.496995\n",
            "Epoch: 4/12..  Test Accuracy: 0.586839\n",
            "Epoch: 5/12..  Test Accuracy: 0.740685\n",
            "Epoch: 6/12..  Test Accuracy: 0.770883\n",
            "Epoch: 7/12..  Test Accuracy: 0.809345\n",
            "Epoch: 8/12..  Test Accuracy: 0.805138\n",
            "Epoch: 9/12..  Test Accuracy: 0.839844\n",
            "Epoch: 10/12..  Test Accuracy: 0.848558\n",
            "Epoch: 11/12..  Test Accuracy: 0.836839\n",
            "Epoch: 12/12..  Test Accuracy: 0.852915\n",
            "Teacher 25\n",
            "Epoch: 1/12..  Test Accuracy: 0.243990\n",
            "Epoch: 2/12..  Test Accuracy: 0.415865\n",
            "Epoch: 3/12..  Test Accuracy: 0.535156\n",
            "Epoch: 4/12..  Test Accuracy: 0.626803\n",
            "Epoch: 5/12..  Test Accuracy: 0.718299\n",
            "Epoch: 6/12..  Test Accuracy: 0.806190\n",
            "Epoch: 7/12..  Test Accuracy: 0.821364\n",
            "Epoch: 8/12..  Test Accuracy: 0.836839\n",
            "Epoch: 9/12..  Test Accuracy: 0.837740\n",
            "Epoch: 10/12..  Test Accuracy: 0.860276\n",
            "Epoch: 11/12..  Test Accuracy: 0.863582\n",
            "Epoch: 12/12..  Test Accuracy: 0.880108\n",
            "Teacher 26\n",
            "Epoch: 1/12..  Test Accuracy: 0.211989\n",
            "Epoch: 2/12..  Test Accuracy: 0.301532\n",
            "Epoch: 3/12..  Test Accuracy: 0.458233\n",
            "Epoch: 4/12..  Test Accuracy: 0.601863\n",
            "Epoch: 5/12..  Test Accuracy: 0.708534\n",
            "Epoch: 6/12..  Test Accuracy: 0.751953\n",
            "Epoch: 7/12..  Test Accuracy: 0.801683\n",
            "Epoch: 8/12..  Test Accuracy: 0.834736\n",
            "Epoch: 9/12..  Test Accuracy: 0.843900\n",
            "Epoch: 10/12..  Test Accuracy: 0.845102\n",
            "Epoch: 11/12..  Test Accuracy: 0.855318\n",
            "Epoch: 12/12..  Test Accuracy: 0.877704\n",
            "Teacher 27\n",
            "Epoch: 1/12..  Test Accuracy: 0.109225\n",
            "Epoch: 2/12..  Test Accuracy: 0.125150\n",
            "Epoch: 3/12..  Test Accuracy: 0.605919\n",
            "Epoch: 4/12..  Test Accuracy: 0.694261\n",
            "Epoch: 5/12..  Test Accuracy: 0.741286\n",
            "Epoch: 6/12..  Test Accuracy: 0.822115\n",
            "Epoch: 7/12..  Test Accuracy: 0.851412\n",
            "Epoch: 8/12..  Test Accuracy: 0.843450\n",
            "Epoch: 9/12..  Test Accuracy: 0.878756\n",
            "Epoch: 10/12..  Test Accuracy: 0.881611\n",
            "Epoch: 11/12..  Test Accuracy: 0.883864\n",
            "Epoch: 12/12..  Test Accuracy: 0.903546\n",
            "Teacher 28\n",
            "Epoch: 1/12..  Test Accuracy: 0.159405\n",
            "Epoch: 2/12..  Test Accuracy: 0.282903\n",
            "Epoch: 3/12..  Test Accuracy: 0.554838\n",
            "Epoch: 4/12..  Test Accuracy: 0.592698\n",
            "Epoch: 5/12..  Test Accuracy: 0.705379\n",
            "Epoch: 6/12..  Test Accuracy: 0.766827\n",
            "Epoch: 7/12..  Test Accuracy: 0.782602\n",
            "Epoch: 8/12..  Test Accuracy: 0.801232\n",
            "Epoch: 9/12..  Test Accuracy: 0.822115\n",
            "Epoch: 10/12..  Test Accuracy: 0.841647\n",
            "Epoch: 11/12..  Test Accuracy: 0.849008\n",
            "Epoch: 12/12..  Test Accuracy: 0.842097\n",
            "Teacher 29\n",
            "Epoch: 1/12..  Test Accuracy: 0.089093\n",
            "Epoch: 2/12..  Test Accuracy: 0.266076\n",
            "Epoch: 3/12..  Test Accuracy: 0.407903\n",
            "Epoch: 4/12..  Test Accuracy: 0.549129\n",
            "Epoch: 5/12..  Test Accuracy: 0.692458\n",
            "Epoch: 6/12..  Test Accuracy: 0.755709\n",
            "Epoch: 7/12..  Test Accuracy: 0.772987\n",
            "Epoch: 8/12..  Test Accuracy: 0.813852\n",
            "Epoch: 9/12..  Test Accuracy: 0.839693\n",
            "Epoch: 10/12..  Test Accuracy: 0.846154\n",
            "Epoch: 11/12..  Test Accuracy: 0.846304\n",
            "Epoch: 12/12..  Test Accuracy: 0.879507\n",
            "Teacher 30\n",
            "Epoch: 1/12..  Test Accuracy: 0.176382\n",
            "Epoch: 2/12..  Test Accuracy: 0.541767\n",
            "Epoch: 3/12..  Test Accuracy: 0.611478\n",
            "Epoch: 4/12..  Test Accuracy: 0.758864\n",
            "Epoch: 5/12..  Test Accuracy: 0.797175\n",
            "Epoch: 6/12..  Test Accuracy: 0.834435\n",
            "Epoch: 7/12..  Test Accuracy: 0.835337\n",
            "Epoch: 8/12..  Test Accuracy: 0.847055\n",
            "Epoch: 9/12..  Test Accuracy: 0.857121\n",
            "Epoch: 10/12..  Test Accuracy: 0.874700\n",
            "Epoch: 11/12..  Test Accuracy: 0.882061\n",
            "Epoch: 12/12..  Test Accuracy: 0.880709\n",
            "Teacher 31\n",
            "Epoch: 1/12..  Test Accuracy: 0.196514\n",
            "Epoch: 2/12..  Test Accuracy: 0.444862\n",
            "Epoch: 3/12..  Test Accuracy: 0.549129\n",
            "Epoch: 4/12..  Test Accuracy: 0.649489\n",
            "Epoch: 5/12..  Test Accuracy: 0.726562\n",
            "Epoch: 6/12..  Test Accuracy: 0.799279\n",
            "Epoch: 7/12..  Test Accuracy: 0.832031\n",
            "Epoch: 8/12..  Test Accuracy: 0.828425\n",
            "Epoch: 9/12..  Test Accuracy: 0.869441\n",
            "Epoch: 10/12..  Test Accuracy: 0.852464\n",
            "Epoch: 11/12..  Test Accuracy: 0.871845\n",
            "Epoch: 12/12..  Test Accuracy: 0.884014\n",
            "Teacher 32\n",
            "Epoch: 1/12..  Test Accuracy: 0.282602\n",
            "Epoch: 2/12..  Test Accuracy: 0.461538\n",
            "Epoch: 3/12..  Test Accuracy: 0.455679\n",
            "Epoch: 4/12..  Test Accuracy: 0.591196\n",
            "Epoch: 5/12..  Test Accuracy: 0.702224\n",
            "Epoch: 6/12..  Test Accuracy: 0.733774\n",
            "Epoch: 7/12..  Test Accuracy: 0.772386\n",
            "Epoch: 8/12..  Test Accuracy: 0.802584\n",
            "Epoch: 9/12..  Test Accuracy: 0.820613\n",
            "Epoch: 10/12..  Test Accuracy: 0.821665\n",
            "Epoch: 11/12..  Test Accuracy: 0.843750\n",
            "Epoch: 12/12..  Test Accuracy: 0.853816\n",
            "Teacher 33\n",
            "Epoch: 1/12..  Test Accuracy: 0.233023\n",
            "Epoch: 2/12..  Test Accuracy: 0.433744\n",
            "Epoch: 3/12..  Test Accuracy: 0.543870\n",
            "Epoch: 4/12..  Test Accuracy: 0.613131\n",
            "Epoch: 5/12..  Test Accuracy: 0.689603\n",
            "Epoch: 6/12..  Test Accuracy: 0.748197\n",
            "Epoch: 7/12..  Test Accuracy: 0.763221\n",
            "Epoch: 8/12..  Test Accuracy: 0.801983\n",
            "Epoch: 9/12..  Test Accuracy: 0.811148\n",
            "Epoch: 10/12..  Test Accuracy: 0.827073\n",
            "Epoch: 11/12..  Test Accuracy: 0.836839\n",
            "Epoch: 12/12..  Test Accuracy: 0.837290\n",
            "Teacher 34\n",
            "Epoch: 1/12..  Test Accuracy: 0.147085\n",
            "Epoch: 2/12..  Test Accuracy: 0.351262\n",
            "Epoch: 3/12..  Test Accuracy: 0.556190\n",
            "Epoch: 4/12..  Test Accuracy: 0.677584\n",
            "Epoch: 5/12..  Test Accuracy: 0.731070\n",
            "Epoch: 6/12..  Test Accuracy: 0.773888\n",
            "Epoch: 7/12..  Test Accuracy: 0.807542\n",
            "Epoch: 8/12..  Test Accuracy: 0.819261\n",
            "Epoch: 9/12..  Test Accuracy: 0.842097\n",
            "Epoch: 10/12..  Test Accuracy: 0.847506\n",
            "Epoch: 11/12..  Test Accuracy: 0.861629\n",
            "Epoch: 12/12..  Test Accuracy: 0.858624\n",
            "Teacher 35\n",
            "Epoch: 1/12..  Test Accuracy: 0.161358\n",
            "Epoch: 2/12..  Test Accuracy: 0.520583\n",
            "Epoch: 3/12..  Test Accuracy: 0.541466\n",
            "Epoch: 4/12..  Test Accuracy: 0.674279\n",
            "Epoch: 5/12..  Test Accuracy: 0.749099\n",
            "Epoch: 6/12..  Test Accuracy: 0.815204\n",
            "Epoch: 7/12..  Test Accuracy: 0.834435\n",
            "Epoch: 8/12..  Test Accuracy: 0.839994\n",
            "Epoch: 9/12..  Test Accuracy: 0.848107\n",
            "Epoch: 10/12..  Test Accuracy: 0.861178\n",
            "Epoch: 11/12..  Test Accuracy: 0.872746\n",
            "Epoch: 12/12..  Test Accuracy: 0.861779\n",
            "Teacher 36\n",
            "Epoch: 1/12..  Test Accuracy: 0.143780\n",
            "Epoch: 2/12..  Test Accuracy: 0.342698\n",
            "Epoch: 3/12..  Test Accuracy: 0.531550\n",
            "Epoch: 4/12..  Test Accuracy: 0.697416\n",
            "Epoch: 5/12..  Test Accuracy: 0.682091\n",
            "Epoch: 6/12..  Test Accuracy: 0.798377\n",
            "Epoch: 7/12..  Test Accuracy: 0.805739\n",
            "Epoch: 8/12..  Test Accuracy: 0.838642\n",
            "Epoch: 9/12..  Test Accuracy: 0.853816\n",
            "Epoch: 10/12..  Test Accuracy: 0.856220\n",
            "Epoch: 11/12..  Test Accuracy: 0.870943\n",
            "Epoch: 12/12..  Test Accuracy: 0.865535\n",
            "Teacher 37\n",
            "Epoch: 1/12..  Test Accuracy: 0.090144\n",
            "Epoch: 2/12..  Test Accuracy: 0.165565\n",
            "Epoch: 3/12..  Test Accuracy: 0.345102\n",
            "Epoch: 4/12..  Test Accuracy: 0.530198\n",
            "Epoch: 5/12..  Test Accuracy: 0.666617\n",
            "Epoch: 6/12..  Test Accuracy: 0.705679\n",
            "Epoch: 7/12..  Test Accuracy: 0.772536\n",
            "Epoch: 8/12..  Test Accuracy: 0.769982\n",
            "Epoch: 9/12..  Test Accuracy: 0.820162\n",
            "Epoch: 10/12..  Test Accuracy: 0.823167\n",
            "Epoch: 11/12..  Test Accuracy: 0.828425\n",
            "Epoch: 12/12..  Test Accuracy: 0.836689\n",
            "Teacher 38\n",
            "Epoch: 1/12..  Test Accuracy: 0.231671\n",
            "Epoch: 2/12..  Test Accuracy: 0.318660\n",
            "Epoch: 3/12..  Test Accuracy: 0.408353\n",
            "Epoch: 4/12..  Test Accuracy: 0.525841\n",
            "Epoch: 5/12..  Test Accuracy: 0.683894\n",
            "Epoch: 6/12..  Test Accuracy: 0.728516\n",
            "Epoch: 7/12..  Test Accuracy: 0.779748\n",
            "Epoch: 8/12..  Test Accuracy: 0.833383\n",
            "Epoch: 9/12..  Test Accuracy: 0.816707\n",
            "Epoch: 10/12..  Test Accuracy: 0.823468\n",
            "Epoch: 11/12..  Test Accuracy: 0.822115\n",
            "Epoch: 12/12..  Test Accuracy: 0.830228\n",
            "Teacher 39\n",
            "Epoch: 1/12..  Test Accuracy: 0.092548\n",
            "Epoch: 2/12..  Test Accuracy: 0.209886\n",
            "Epoch: 3/12..  Test Accuracy: 0.511268\n",
            "Epoch: 4/12..  Test Accuracy: 0.746995\n",
            "Epoch: 5/12..  Test Accuracy: 0.756160\n",
            "Epoch: 6/12..  Test Accuracy: 0.800180\n",
            "Epoch: 7/12..  Test Accuracy: 0.823618\n",
            "Epoch: 8/12..  Test Accuracy: 0.840144\n",
            "Epoch: 9/12..  Test Accuracy: 0.829477\n",
            "Epoch: 10/12..  Test Accuracy: 0.846154\n",
            "Epoch: 11/12..  Test Accuracy: 0.861178\n",
            "Epoch: 12/12..  Test Accuracy: 0.866436\n",
            "Teacher 40\n",
            "Epoch: 1/12..  Test Accuracy: 0.184495\n",
            "Epoch: 2/12..  Test Accuracy: 0.335186\n",
            "Epoch: 3/12..  Test Accuracy: 0.486779\n",
            "Epoch: 4/12..  Test Accuracy: 0.675932\n",
            "Epoch: 5/12..  Test Accuracy: 0.753906\n",
            "Epoch: 6/12..  Test Accuracy: 0.821665\n",
            "Epoch: 7/12..  Test Accuracy: 0.847506\n",
            "Epoch: 8/12..  Test Accuracy: 0.835337\n",
            "Epoch: 9/12..  Test Accuracy: 0.865385\n",
            "Epoch: 10/12..  Test Accuracy: 0.874700\n",
            "Epoch: 11/12..  Test Accuracy: 0.867338\n",
            "Epoch: 12/12..  Test Accuracy: 0.882512\n",
            "Teacher 41\n",
            "Epoch: 1/12..  Test Accuracy: 0.091196\n",
            "Epoch: 2/12..  Test Accuracy: 0.381310\n",
            "Epoch: 3/12..  Test Accuracy: 0.529748\n",
            "Epoch: 4/12..  Test Accuracy: 0.686298\n",
            "Epoch: 5/12..  Test Accuracy: 0.736478\n",
            "Epoch: 6/12..  Test Accuracy: 0.828425\n",
            "Epoch: 7/12..  Test Accuracy: 0.818209\n",
            "Epoch: 8/12..  Test Accuracy: 0.857873\n",
            "Epoch: 9/12..  Test Accuracy: 0.865385\n",
            "Epoch: 10/12..  Test Accuracy: 0.869441\n",
            "Epoch: 11/12..  Test Accuracy: 0.894832\n",
            "Epoch: 12/12..  Test Accuracy: 0.885066\n",
            "Teacher 42\n",
            "Epoch: 1/12..  Test Accuracy: 0.268029\n",
            "Epoch: 2/12..  Test Accuracy: 0.296875\n",
            "Epoch: 3/12..  Test Accuracy: 0.431490\n",
            "Epoch: 4/12..  Test Accuracy: 0.574068\n",
            "Epoch: 5/12..  Test Accuracy: 0.667818\n",
            "Epoch: 6/12..  Test Accuracy: 0.779297\n",
            "Epoch: 7/12..  Test Accuracy: 0.784105\n",
            "Epoch: 8/12..  Test Accuracy: 0.835787\n",
            "Epoch: 9/12..  Test Accuracy: 0.884465\n",
            "Epoch: 10/12..  Test Accuracy: 0.868389\n",
            "Epoch: 11/12..  Test Accuracy: 0.890775\n",
            "Epoch: 12/12..  Test Accuracy: 0.889874\n",
            "Teacher 43\n",
            "Epoch: 1/12..  Test Accuracy: 0.247596\n",
            "Epoch: 2/12..  Test Accuracy: 0.436899\n",
            "Epoch: 3/12..  Test Accuracy: 0.617788\n",
            "Epoch: 4/12..  Test Accuracy: 0.720703\n",
            "Epoch: 5/12..  Test Accuracy: 0.744742\n",
            "Epoch: 6/12..  Test Accuracy: 0.801082\n",
            "Epoch: 7/12..  Test Accuracy: 0.833684\n",
            "Epoch: 8/12..  Test Accuracy: 0.835337\n",
            "Epoch: 9/12..  Test Accuracy: 0.853666\n",
            "Epoch: 10/12..  Test Accuracy: 0.845102\n",
            "Epoch: 11/12..  Test Accuracy: 0.875300\n",
            "Epoch: 12/12..  Test Accuracy: 0.872296\n",
            "Teacher 44\n",
            "Epoch: 1/12..  Test Accuracy: 0.109075\n",
            "Epoch: 2/12..  Test Accuracy: 0.345252\n",
            "Epoch: 3/12..  Test Accuracy: 0.482572\n",
            "Epoch: 4/12..  Test Accuracy: 0.624850\n",
            "Epoch: 5/12..  Test Accuracy: 0.686599\n",
            "Epoch: 6/12..  Test Accuracy: 0.735577\n",
            "Epoch: 7/12..  Test Accuracy: 0.769081\n",
            "Epoch: 8/12..  Test Accuracy: 0.839543\n",
            "Epoch: 9/12..  Test Accuracy: 0.802734\n",
            "Epoch: 10/12..  Test Accuracy: 0.849459\n",
            "Epoch: 11/12..  Test Accuracy: 0.849459\n",
            "Epoch: 12/12..  Test Accuracy: 0.869892\n",
            "Teacher 45\n",
            "Epoch: 1/12..  Test Accuracy: 0.175331\n",
            "Epoch: 2/12..  Test Accuracy: 0.493089\n",
            "Epoch: 3/12..  Test Accuracy: 0.532302\n",
            "Epoch: 4/12..  Test Accuracy: 0.633714\n",
            "Epoch: 5/12..  Test Accuracy: 0.683744\n",
            "Epoch: 6/12..  Test Accuracy: 0.735577\n",
            "Epoch: 7/12..  Test Accuracy: 0.772386\n",
            "Epoch: 8/12..  Test Accuracy: 0.803636\n",
            "Epoch: 9/12..  Test Accuracy: 0.822115\n",
            "Epoch: 10/12..  Test Accuracy: 0.843450\n",
            "Epoch: 11/12..  Test Accuracy: 0.855769\n",
            "Epoch: 12/12..  Test Accuracy: 0.858624\n",
            "Teacher 46\n",
            "Epoch: 1/12..  Test Accuracy: 0.135066\n",
            "Epoch: 2/12..  Test Accuracy: 0.344501\n",
            "Epoch: 3/12..  Test Accuracy: 0.475060\n",
            "Epoch: 4/12..  Test Accuracy: 0.697115\n",
            "Epoch: 5/12..  Test Accuracy: 0.788462\n",
            "Epoch: 6/12..  Test Accuracy: 0.811298\n",
            "Epoch: 7/12..  Test Accuracy: 0.825571\n",
            "Epoch: 8/12..  Test Accuracy: 0.843450\n",
            "Epoch: 9/12..  Test Accuracy: 0.851562\n",
            "Epoch: 10/12..  Test Accuracy: 0.850962\n",
            "Epoch: 11/12..  Test Accuracy: 0.874700\n",
            "Epoch: 12/12..  Test Accuracy: 0.871845\n",
            "Teacher 47\n",
            "Epoch: 1/12..  Test Accuracy: 0.132512\n",
            "Epoch: 2/12..  Test Accuracy: 0.273438\n",
            "Epoch: 3/12..  Test Accuracy: 0.485427\n",
            "Epoch: 4/12..  Test Accuracy: 0.507061\n",
            "Epoch: 5/12..  Test Accuracy: 0.638522\n",
            "Epoch: 6/12..  Test Accuracy: 0.738882\n",
            "Epoch: 7/12..  Test Accuracy: 0.790415\n",
            "Epoch: 8/12..  Test Accuracy: 0.813852\n",
            "Epoch: 9/12..  Test Accuracy: 0.833233\n",
            "Epoch: 10/12..  Test Accuracy: 0.842548\n",
            "Epoch: 11/12..  Test Accuracy: 0.859225\n",
            "Epoch: 12/12..  Test Accuracy: 0.880559\n",
            "Teacher 48\n",
            "Epoch: 1/12..  Test Accuracy: 0.175030\n",
            "Epoch: 2/12..  Test Accuracy: 0.388371\n",
            "Epoch: 3/12..  Test Accuracy: 0.592097\n",
            "Epoch: 4/12..  Test Accuracy: 0.698317\n",
            "Epoch: 5/12..  Test Accuracy: 0.780799\n",
            "Epoch: 6/12..  Test Accuracy: 0.816256\n",
            "Epoch: 7/12..  Test Accuracy: 0.830829\n",
            "Epoch: 8/12..  Test Accuracy: 0.855619\n",
            "Epoch: 9/12..  Test Accuracy: 0.844501\n",
            "Epoch: 10/12..  Test Accuracy: 0.849910\n",
            "Epoch: 11/12..  Test Accuracy: 0.850962\n",
            "Epoch: 12/12..  Test Accuracy: 0.864483\n",
            "Teacher 49\n",
            "Epoch: 1/12..  Test Accuracy: 0.309345\n",
            "Epoch: 2/12..  Test Accuracy: 0.259315\n",
            "Epoch: 3/12..  Test Accuracy: 0.700871\n",
            "Epoch: 4/12..  Test Accuracy: 0.751052\n",
            "Epoch: 5/12..  Test Accuracy: 0.761719\n",
            "Epoch: 6/12..  Test Accuracy: 0.784856\n",
            "Epoch: 7/12..  Test Accuracy: 0.795222\n",
            "Epoch: 8/12..  Test Accuracy: 0.829477\n",
            "Epoch: 9/12..  Test Accuracy: 0.833684\n",
            "Epoch: 10/12..  Test Accuracy: 0.839994\n",
            "Epoch: 11/12..  Test Accuracy: 0.851863\n",
            "Epoch: 12/12..  Test Accuracy: 0.858774\n",
            "Teacher 50\n",
            "Epoch: 1/12..  Test Accuracy: 0.179688\n",
            "Epoch: 2/12..  Test Accuracy: 0.491587\n",
            "Epoch: 3/12..  Test Accuracy: 0.523588\n",
            "Epoch: 4/12..  Test Accuracy: 0.714393\n",
            "Epoch: 5/12..  Test Accuracy: 0.776743\n",
            "Epoch: 6/12..  Test Accuracy: 0.797626\n",
            "Epoch: 7/12..  Test Accuracy: 0.816406\n",
            "Epoch: 8/12..  Test Accuracy: 0.837290\n",
            "Epoch: 9/12..  Test Accuracy: 0.847957\n",
            "Epoch: 10/12..  Test Accuracy: 0.864032\n",
            "Epoch: 11/12..  Test Accuracy: 0.868990\n",
            "Epoch: 12/12..  Test Accuracy: 0.870493\n",
            "Teacher 51\n",
            "Epoch: 1/12..  Test Accuracy: 0.422626\n",
            "Epoch: 2/12..  Test Accuracy: 0.392879\n",
            "Epoch: 3/12..  Test Accuracy: 0.566106\n",
            "Epoch: 4/12..  Test Accuracy: 0.704627\n",
            "Epoch: 5/12..  Test Accuracy: 0.774339\n",
            "Epoch: 6/12..  Test Accuracy: 0.795222\n",
            "Epoch: 7/12..  Test Accuracy: 0.821064\n",
            "Epoch: 8/12..  Test Accuracy: 0.825120\n",
            "Epoch: 9/12..  Test Accuracy: 0.844501\n",
            "Epoch: 10/12..  Test Accuracy: 0.853365\n",
            "Epoch: 11/12..  Test Accuracy: 0.861478\n",
            "Epoch: 12/12..  Test Accuracy: 0.863432\n",
            "Teacher 52\n",
            "Epoch: 1/12..  Test Accuracy: 0.139423\n",
            "Epoch: 2/12..  Test Accuracy: 0.310697\n",
            "Epoch: 3/12..  Test Accuracy: 0.453425\n",
            "Epoch: 4/12..  Test Accuracy: 0.676382\n",
            "Epoch: 5/12..  Test Accuracy: 0.769982\n",
            "Epoch: 6/12..  Test Accuracy: 0.812650\n",
            "Epoch: 7/12..  Test Accuracy: 0.847957\n",
            "Epoch: 8/12..  Test Accuracy: 0.835787\n",
            "Epoch: 9/12..  Test Accuracy: 0.858474\n",
            "Epoch: 10/12..  Test Accuracy: 0.867338\n",
            "Epoch: 11/12..  Test Accuracy: 0.874700\n",
            "Epoch: 12/12..  Test Accuracy: 0.877704\n",
            "Teacher 53\n",
            "Epoch: 1/12..  Test Accuracy: 0.162710\n",
            "Epoch: 2/12..  Test Accuracy: 0.266677\n",
            "Epoch: 3/12..  Test Accuracy: 0.407452\n",
            "Epoch: 4/12..  Test Accuracy: 0.598107\n",
            "Epoch: 5/12..  Test Accuracy: 0.734976\n",
            "Epoch: 6/12..  Test Accuracy: 0.822716\n",
            "Epoch: 7/12..  Test Accuracy: 0.830980\n",
            "Epoch: 8/12..  Test Accuracy: 0.834285\n",
            "Epoch: 9/12..  Test Accuracy: 0.848407\n",
            "Epoch: 10/12..  Test Accuracy: 0.868990\n",
            "Epoch: 11/12..  Test Accuracy: 0.879207\n",
            "Epoch: 12/12..  Test Accuracy: 0.873197\n",
            "Teacher 54\n",
            "Epoch: 1/12..  Test Accuracy: 0.377704\n",
            "Epoch: 2/12..  Test Accuracy: 0.417819\n",
            "Epoch: 3/12..  Test Accuracy: 0.557242\n",
            "Epoch: 4/12..  Test Accuracy: 0.589844\n",
            "Epoch: 5/12..  Test Accuracy: 0.725511\n",
            "Epoch: 6/12..  Test Accuracy: 0.787560\n",
            "Epoch: 7/12..  Test Accuracy: 0.797927\n",
            "Epoch: 8/12..  Test Accuracy: 0.850811\n",
            "Epoch: 9/12..  Test Accuracy: 0.828876\n",
            "Epoch: 10/12..  Test Accuracy: 0.867338\n",
            "Epoch: 11/12..  Test Accuracy: 0.874249\n",
            "Epoch: 12/12..  Test Accuracy: 0.873798\n",
            "Teacher 55\n",
            "Epoch: 1/12..  Test Accuracy: 0.253906\n",
            "Epoch: 2/12..  Test Accuracy: 0.352614\n",
            "Epoch: 3/12..  Test Accuracy: 0.530198\n",
            "Epoch: 4/12..  Test Accuracy: 0.694712\n",
            "Epoch: 5/12..  Test Accuracy: 0.816707\n",
            "Epoch: 6/12..  Test Accuracy: 0.827825\n",
            "Epoch: 7/12..  Test Accuracy: 0.847506\n",
            "Epoch: 8/12..  Test Accuracy: 0.849459\n",
            "Epoch: 9/12..  Test Accuracy: 0.864032\n",
            "Epoch: 10/12..  Test Accuracy: 0.870793\n",
            "Epoch: 11/12..  Test Accuracy: 0.877254\n",
            "Epoch: 12/12..  Test Accuracy: 0.872446\n",
            "Teacher 56\n",
            "Epoch: 1/12..  Test Accuracy: 0.260216\n",
            "Epoch: 2/12..  Test Accuracy: 0.462891\n",
            "Epoch: 3/12..  Test Accuracy: 0.492638\n",
            "Epoch: 4/12..  Test Accuracy: 0.674579\n",
            "Epoch: 5/12..  Test Accuracy: 0.743239\n",
            "Epoch: 6/12..  Test Accuracy: 0.771484\n",
            "Epoch: 7/12..  Test Accuracy: 0.835938\n",
            "Epoch: 8/12..  Test Accuracy: 0.820763\n",
            "Epoch: 9/12..  Test Accuracy: 0.839844\n",
            "Epoch: 10/12..  Test Accuracy: 0.866136\n",
            "Epoch: 11/12..  Test Accuracy: 0.863582\n",
            "Epoch: 12/12..  Test Accuracy: 0.868540\n",
            "Teacher 57\n",
            "Epoch: 1/12..  Test Accuracy: 0.185697\n",
            "Epoch: 2/12..  Test Accuracy: 0.378155\n",
            "Epoch: 3/12..  Test Accuracy: 0.508714\n",
            "Epoch: 4/12..  Test Accuracy: 0.584585\n",
            "Epoch: 5/12..  Test Accuracy: 0.652193\n",
            "Epoch: 6/12..  Test Accuracy: 0.698918\n",
            "Epoch: 7/12..  Test Accuracy: 0.771034\n",
            "Epoch: 8/12..  Test Accuracy: 0.804988\n",
            "Epoch: 9/12..  Test Accuracy: 0.827975\n",
            "Epoch: 10/12..  Test Accuracy: 0.836689\n",
            "Epoch: 11/12..  Test Accuracy: 0.862981\n",
            "Epoch: 12/12..  Test Accuracy: 0.858624\n",
            "Teacher 58\n",
            "Epoch: 1/12..  Test Accuracy: 0.155499\n",
            "Epoch: 2/12..  Test Accuracy: 0.400541\n",
            "Epoch: 3/12..  Test Accuracy: 0.507512\n",
            "Epoch: 4/12..  Test Accuracy: 0.673978\n",
            "Epoch: 5/12..  Test Accuracy: 0.765625\n",
            "Epoch: 6/12..  Test Accuracy: 0.807392\n",
            "Epoch: 7/12..  Test Accuracy: 0.832933\n",
            "Epoch: 8/12..  Test Accuracy: 0.846004\n",
            "Epoch: 9/12..  Test Accuracy: 0.855318\n",
            "Epoch: 10/12..  Test Accuracy: 0.858624\n",
            "Epoch: 11/12..  Test Accuracy: 0.857272\n",
            "Epoch: 12/12..  Test Accuracy: 0.875150\n",
            "Teacher 59\n",
            "Epoch: 1/12..  Test Accuracy: 0.096905\n",
            "Epoch: 2/12..  Test Accuracy: 0.360877\n",
            "Epoch: 3/12..  Test Accuracy: 0.433894\n",
            "Epoch: 4/12..  Test Accuracy: 0.638822\n",
            "Epoch: 5/12..  Test Accuracy: 0.728065\n",
            "Epoch: 6/12..  Test Accuracy: 0.803035\n",
            "Epoch: 7/12..  Test Accuracy: 0.823017\n",
            "Epoch: 8/12..  Test Accuracy: 0.838792\n",
            "Epoch: 9/12..  Test Accuracy: 0.841647\n",
            "Epoch: 10/12..  Test Accuracy: 0.850361\n",
            "Epoch: 11/12..  Test Accuracy: 0.856520\n",
            "Epoch: 12/12..  Test Accuracy: 0.872296\n",
            "Teacher 60\n",
            "Epoch: 1/12..  Test Accuracy: 0.187049\n",
            "Epoch: 2/12..  Test Accuracy: 0.307542\n",
            "Epoch: 3/12..  Test Accuracy: 0.405198\n",
            "Epoch: 4/12..  Test Accuracy: 0.526743\n",
            "Epoch: 5/12..  Test Accuracy: 0.670072\n",
            "Epoch: 6/12..  Test Accuracy: 0.718450\n",
            "Epoch: 7/12..  Test Accuracy: 0.814754\n",
            "Epoch: 8/12..  Test Accuracy: 0.830379\n",
            "Epoch: 9/12..  Test Accuracy: 0.829026\n",
            "Epoch: 10/12..  Test Accuracy: 0.850210\n",
            "Epoch: 11/12..  Test Accuracy: 0.871695\n",
            "Epoch: 12/12..  Test Accuracy: 0.874700\n",
            "Teacher 61\n",
            "Epoch: 1/12..  Test Accuracy: 0.195913\n",
            "Epoch: 2/12..  Test Accuracy: 0.291466\n",
            "Epoch: 3/12..  Test Accuracy: 0.559946\n",
            "Epoch: 4/12..  Test Accuracy: 0.597957\n",
            "Epoch: 5/12..  Test Accuracy: 0.737831\n",
            "Epoch: 6/12..  Test Accuracy: 0.783954\n",
            "Epoch: 7/12..  Test Accuracy: 0.822416\n",
            "Epoch: 8/12..  Test Accuracy: 0.827524\n",
            "Epoch: 9/12..  Test Accuracy: 0.854267\n",
            "Epoch: 10/12..  Test Accuracy: 0.876202\n",
            "Epoch: 11/12..  Test Accuracy: 0.887019\n",
            "Epoch: 12/12..  Test Accuracy: 0.882963\n",
            "Teacher 62\n",
            "Epoch: 1/12..  Test Accuracy: 0.196965\n",
            "Epoch: 2/12..  Test Accuracy: 0.381160\n",
            "Epoch: 3/12..  Test Accuracy: 0.625150\n",
            "Epoch: 4/12..  Test Accuracy: 0.700871\n",
            "Epoch: 5/12..  Test Accuracy: 0.755709\n",
            "Epoch: 6/12..  Test Accuracy: 0.771334\n",
            "Epoch: 7/12..  Test Accuracy: 0.826472\n",
            "Epoch: 8/12..  Test Accuracy: 0.857572\n",
            "Epoch: 9/12..  Test Accuracy: 0.863582\n",
            "Epoch: 10/12..  Test Accuracy: 0.873648\n",
            "Epoch: 11/12..  Test Accuracy: 0.889874\n",
            "Epoch: 12/12..  Test Accuracy: 0.889874\n",
            "Teacher 63\n",
            "Epoch: 1/12..  Test Accuracy: 0.209886\n",
            "Epoch: 2/12..  Test Accuracy: 0.365385\n",
            "Epoch: 3/12..  Test Accuracy: 0.549730\n",
            "Epoch: 4/12..  Test Accuracy: 0.685847\n",
            "Epoch: 5/12..  Test Accuracy: 0.734225\n",
            "Epoch: 6/12..  Test Accuracy: 0.803185\n",
            "Epoch: 7/12..  Test Accuracy: 0.841647\n",
            "Epoch: 8/12..  Test Accuracy: 0.847807\n",
            "Epoch: 9/12..  Test Accuracy: 0.858774\n",
            "Epoch: 10/12..  Test Accuracy: 0.869441\n",
            "Epoch: 11/12..  Test Accuracy: 0.877704\n",
            "Epoch: 12/12..  Test Accuracy: 0.884014\n",
            "Teacher 64\n",
            "Epoch: 1/12..  Test Accuracy: 0.234976\n",
            "Epoch: 2/12..  Test Accuracy: 0.344952\n",
            "Epoch: 3/12..  Test Accuracy: 0.527945\n",
            "Epoch: 4/12..  Test Accuracy: 0.646184\n",
            "Epoch: 5/12..  Test Accuracy: 0.716346\n",
            "Epoch: 6/12..  Test Accuracy: 0.775841\n",
            "Epoch: 7/12..  Test Accuracy: 0.798227\n",
            "Epoch: 8/12..  Test Accuracy: 0.833233\n",
            "Epoch: 9/12..  Test Accuracy: 0.846905\n",
            "Epoch: 10/12..  Test Accuracy: 0.850511\n",
            "Epoch: 11/12..  Test Accuracy: 0.854718\n",
            "Epoch: 12/12..  Test Accuracy: 0.863582\n",
            "Teacher 65\n",
            "Epoch: 1/12..  Test Accuracy: 0.170523\n",
            "Epoch: 2/12..  Test Accuracy: 0.252404\n",
            "Epoch: 3/12..  Test Accuracy: 0.373648\n",
            "Epoch: 4/12..  Test Accuracy: 0.548377\n",
            "Epoch: 5/12..  Test Accuracy: 0.682392\n",
            "Epoch: 6/12..  Test Accuracy: 0.781550\n",
            "Epoch: 7/12..  Test Accuracy: 0.782602\n",
            "Epoch: 8/12..  Test Accuracy: 0.824068\n",
            "Epoch: 9/12..  Test Accuracy: 0.852314\n",
            "Epoch: 10/12..  Test Accuracy: 0.855619\n",
            "Epoch: 11/12..  Test Accuracy: 0.851863\n",
            "Epoch: 12/12..  Test Accuracy: 0.872446\n",
            "Teacher 66\n",
            "Epoch: 1/12..  Test Accuracy: 0.103816\n",
            "Epoch: 2/12..  Test Accuracy: 0.287560\n",
            "Epoch: 3/12..  Test Accuracy: 0.474459\n",
            "Epoch: 4/12..  Test Accuracy: 0.582182\n",
            "Epoch: 5/12..  Test Accuracy: 0.740234\n",
            "Epoch: 6/12..  Test Accuracy: 0.827825\n",
            "Epoch: 7/12..  Test Accuracy: 0.837740\n",
            "Epoch: 8/12..  Test Accuracy: 0.844651\n",
            "Epoch: 9/12..  Test Accuracy: 0.864032\n",
            "Epoch: 10/12..  Test Accuracy: 0.875751\n",
            "Epoch: 11/12..  Test Accuracy: 0.893179\n",
            "Epoch: 12/12..  Test Accuracy: 0.887019\n",
            "Teacher 67\n",
            "Epoch: 1/12..  Test Accuracy: 0.311448\n",
            "Epoch: 2/12..  Test Accuracy: 0.454627\n",
            "Epoch: 3/12..  Test Accuracy: 0.587590\n",
            "Epoch: 4/12..  Test Accuracy: 0.719802\n",
            "Epoch: 5/12..  Test Accuracy: 0.783053\n",
            "Epoch: 6/12..  Test Accuracy: 0.821514\n",
            "Epoch: 7/12..  Test Accuracy: 0.803035\n",
            "Epoch: 8/12..  Test Accuracy: 0.846154\n",
            "Epoch: 9/12..  Test Accuracy: 0.835337\n",
            "Epoch: 10/12..  Test Accuracy: 0.842097\n",
            "Epoch: 11/12..  Test Accuracy: 0.846454\n",
            "Epoch: 12/12..  Test Accuracy: 0.858173\n",
            "Teacher 68\n",
            "Epoch: 1/12..  Test Accuracy: 0.294020\n",
            "Epoch: 2/12..  Test Accuracy: 0.282151\n",
            "Epoch: 3/12..  Test Accuracy: 0.415865\n",
            "Epoch: 4/12..  Test Accuracy: 0.700721\n",
            "Epoch: 5/12..  Test Accuracy: 0.732572\n",
            "Epoch: 6/12..  Test Accuracy: 0.809044\n",
            "Epoch: 7/12..  Test Accuracy: 0.856671\n",
            "Epoch: 8/12..  Test Accuracy: 0.864483\n",
            "Epoch: 9/12..  Test Accuracy: 0.863582\n",
            "Epoch: 10/12..  Test Accuracy: 0.871394\n",
            "Epoch: 11/12..  Test Accuracy: 0.874850\n",
            "Epoch: 12/12..  Test Accuracy: 0.890325\n",
            "Teacher 69\n",
            "Epoch: 1/12..  Test Accuracy: 0.170222\n",
            "Epoch: 2/12..  Test Accuracy: 0.309495\n",
            "Epoch: 3/12..  Test Accuracy: 0.563101\n",
            "Epoch: 4/12..  Test Accuracy: 0.696514\n",
            "Epoch: 5/12..  Test Accuracy: 0.782302\n",
            "Epoch: 6/12..  Test Accuracy: 0.800781\n",
            "Epoch: 7/12..  Test Accuracy: 0.824519\n",
            "Epoch: 8/12..  Test Accuracy: 0.820162\n",
            "Epoch: 9/12..  Test Accuracy: 0.844201\n",
            "Epoch: 10/12..  Test Accuracy: 0.855318\n",
            "Epoch: 11/12..  Test Accuracy: 0.863131\n",
            "Epoch: 12/12..  Test Accuracy: 0.871845\n",
            "Teacher 70\n",
            "Epoch: 1/12..  Test Accuracy: 0.210487\n",
            "Epoch: 2/12..  Test Accuracy: 0.252103\n",
            "Epoch: 3/12..  Test Accuracy: 0.371845\n",
            "Epoch: 4/12..  Test Accuracy: 0.517879\n",
            "Epoch: 5/12..  Test Accuracy: 0.637320\n",
            "Epoch: 6/12..  Test Accuracy: 0.710938\n",
            "Epoch: 7/12..  Test Accuracy: 0.701472\n",
            "Epoch: 8/12..  Test Accuracy: 0.798377\n",
            "Epoch: 9/12..  Test Accuracy: 0.827524\n",
            "Epoch: 10/12..  Test Accuracy: 0.852764\n",
            "Epoch: 11/12..  Test Accuracy: 0.841947\n",
            "Epoch: 12/12..  Test Accuracy: 0.857722\n",
            "Teacher 71\n",
            "Epoch: 1/12..  Test Accuracy: 0.100361\n",
            "Epoch: 2/12..  Test Accuracy: 0.146635\n",
            "Epoch: 3/12..  Test Accuracy: 0.562350\n",
            "Epoch: 4/12..  Test Accuracy: 0.646785\n",
            "Epoch: 5/12..  Test Accuracy: 0.745493\n",
            "Epoch: 6/12..  Test Accuracy: 0.763672\n",
            "Epoch: 7/12..  Test Accuracy: 0.804838\n",
            "Epoch: 8/12..  Test Accuracy: 0.846454\n",
            "Epoch: 9/12..  Test Accuracy: 0.852464\n",
            "Epoch: 10/12..  Test Accuracy: 0.863882\n",
            "Epoch: 11/12..  Test Accuracy: 0.870493\n",
            "Epoch: 12/12..  Test Accuracy: 0.872296\n",
            "Teacher 72\n",
            "Epoch: 1/12..  Test Accuracy: 0.204327\n",
            "Epoch: 2/12..  Test Accuracy: 0.305739\n",
            "Epoch: 3/12..  Test Accuracy: 0.481671\n",
            "Epoch: 4/12..  Test Accuracy: 0.627103\n",
            "Epoch: 5/12..  Test Accuracy: 0.711088\n",
            "Epoch: 6/12..  Test Accuracy: 0.776743\n",
            "Epoch: 7/12..  Test Accuracy: 0.813552\n",
            "Epoch: 8/12..  Test Accuracy: 0.839243\n",
            "Epoch: 9/12..  Test Accuracy: 0.848407\n",
            "Epoch: 10/12..  Test Accuracy: 0.867939\n",
            "Epoch: 11/12..  Test Accuracy: 0.861328\n",
            "Epoch: 12/12..  Test Accuracy: 0.888972\n",
            "Teacher 73\n",
            "Epoch: 1/12..  Test Accuracy: 0.092698\n",
            "Epoch: 2/12..  Test Accuracy: 0.542518\n",
            "Epoch: 3/12..  Test Accuracy: 0.669020\n",
            "Epoch: 4/12..  Test Accuracy: 0.729567\n",
            "Epoch: 5/12..  Test Accuracy: 0.800932\n",
            "Epoch: 6/12..  Test Accuracy: 0.794772\n",
            "Epoch: 7/12..  Test Accuracy: 0.831280\n",
            "Epoch: 8/12..  Test Accuracy: 0.851863\n",
            "Epoch: 9/12..  Test Accuracy: 0.834435\n",
            "Epoch: 10/12..  Test Accuracy: 0.839693\n",
            "Epoch: 11/12..  Test Accuracy: 0.870343\n",
            "Epoch: 12/12..  Test Accuracy: 0.867338\n",
            "Teacher 74\n",
            "Epoch: 1/12..  Test Accuracy: 0.315054\n",
            "Epoch: 2/12..  Test Accuracy: 0.409255\n",
            "Epoch: 3/12..  Test Accuracy: 0.516076\n",
            "Epoch: 4/12..  Test Accuracy: 0.630559\n",
            "Epoch: 5/12..  Test Accuracy: 0.738131\n",
            "Epoch: 6/12..  Test Accuracy: 0.796274\n",
            "Epoch: 7/12..  Test Accuracy: 0.784105\n",
            "Epoch: 8/12..  Test Accuracy: 0.826022\n",
            "Epoch: 9/12..  Test Accuracy: 0.835337\n",
            "Epoch: 10/12..  Test Accuracy: 0.861028\n",
            "Epoch: 11/12..  Test Accuracy: 0.867338\n",
            "Epoch: 12/12..  Test Accuracy: 0.871695\n",
            "Teacher 75\n",
            "Epoch: 1/12..  Test Accuracy: 0.090745\n",
            "Epoch: 2/12..  Test Accuracy: 0.112079\n",
            "Epoch: 3/12..  Test Accuracy: 0.439603\n",
            "Epoch: 4/12..  Test Accuracy: 0.558894\n",
            "Epoch: 5/12..  Test Accuracy: 0.697416\n",
            "Epoch: 6/12..  Test Accuracy: 0.803185\n",
            "Epoch: 7/12..  Test Accuracy: 0.830980\n",
            "Epoch: 8/12..  Test Accuracy: 0.850811\n",
            "Epoch: 9/12..  Test Accuracy: 0.861478\n",
            "Epoch: 10/12..  Test Accuracy: 0.886869\n",
            "Epoch: 11/12..  Test Accuracy: 0.875150\n",
            "Epoch: 12/12..  Test Accuracy: 0.877855\n",
            "Teacher 76\n",
            "Epoch: 1/12..  Test Accuracy: 0.090144\n",
            "Epoch: 2/12..  Test Accuracy: 0.240234\n",
            "Epoch: 3/12..  Test Accuracy: 0.552734\n",
            "Epoch: 4/12..  Test Accuracy: 0.726713\n",
            "Epoch: 5/12..  Test Accuracy: 0.795072\n",
            "Epoch: 6/12..  Test Accuracy: 0.805589\n",
            "Epoch: 7/12..  Test Accuracy: 0.841046\n",
            "Epoch: 8/12..  Test Accuracy: 0.833684\n",
            "Epoch: 9/12..  Test Accuracy: 0.852314\n",
            "Epoch: 10/12..  Test Accuracy: 0.863582\n",
            "Epoch: 11/12..  Test Accuracy: 0.878606\n",
            "Epoch: 12/12..  Test Accuracy: 0.884014\n",
            "Teacher 77\n",
            "Epoch: 1/12..  Test Accuracy: 0.118840\n",
            "Epoch: 2/12..  Test Accuracy: 0.120343\n",
            "Epoch: 3/12..  Test Accuracy: 0.555138\n",
            "Epoch: 4/12..  Test Accuracy: 0.676382\n",
            "Epoch: 5/12..  Test Accuracy: 0.760216\n",
            "Epoch: 6/12..  Test Accuracy: 0.762620\n",
            "Epoch: 7/12..  Test Accuracy: 0.806641\n",
            "Epoch: 8/12..  Test Accuracy: 0.803636\n",
            "Epoch: 9/12..  Test Accuracy: 0.824068\n",
            "Epoch: 10/12..  Test Accuracy: 0.847506\n",
            "Epoch: 11/12..  Test Accuracy: 0.843149\n",
            "Epoch: 12/12..  Test Accuracy: 0.858474\n",
            "Teacher 78\n",
            "Epoch: 1/12..  Test Accuracy: 0.117939\n",
            "Epoch: 2/12..  Test Accuracy: 0.406851\n",
            "Epoch: 3/12..  Test Accuracy: 0.587440\n",
            "Epoch: 4/12..  Test Accuracy: 0.725210\n",
            "Epoch: 5/12..  Test Accuracy: 0.793119\n",
            "Epoch: 6/12..  Test Accuracy: 0.801983\n",
            "Epoch: 7/12..  Test Accuracy: 0.837290\n",
            "Epoch: 8/12..  Test Accuracy: 0.828576\n",
            "Epoch: 9/12..  Test Accuracy: 0.836238\n",
            "Epoch: 10/12..  Test Accuracy: 0.852764\n",
            "Epoch: 11/12..  Test Accuracy: 0.863582\n",
            "Epoch: 12/12..  Test Accuracy: 0.856220\n",
            "Teacher 79\n",
            "Epoch: 1/12..  Test Accuracy: 0.128305\n",
            "Epoch: 2/12..  Test Accuracy: 0.209585\n",
            "Epoch: 3/12..  Test Accuracy: 0.577825\n",
            "Epoch: 4/12..  Test Accuracy: 0.760066\n",
            "Epoch: 5/12..  Test Accuracy: 0.826172\n",
            "Epoch: 6/12..  Test Accuracy: 0.829928\n",
            "Epoch: 7/12..  Test Accuracy: 0.838642\n",
            "Epoch: 8/12..  Test Accuracy: 0.862079\n",
            "Epoch: 9/12..  Test Accuracy: 0.853816\n",
            "Epoch: 10/12..  Test Accuracy: 0.868990\n",
            "Epoch: 11/12..  Test Accuracy: 0.864934\n",
            "Epoch: 12/12..  Test Accuracy: 0.874249\n",
            "All 80 Teachers  has been trained\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgihRYmu2kkD",
        "colab_type": "code",
        "outputId": "23476f62-2475-4273-f2d0-106d63397e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(labs),len(imgs))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9500 9500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YGDRpv05LEP",
        "colab_type": "code",
        "outputId": "53ea9265-a86a-4d8c-d92c-45354f34d8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "np.transpose(labs[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMR3k4GU0O3T",
        "colab_type": "code",
        "outputId": "4af02a70-bd08-4665-935d-65db014ba254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"len(mnist_labels) {len(mnist_labels)}\",f\"len(true_labels) {len(true_labels)}\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(mnist_labels) 80 len(true_labels) 80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMX4BByqIAOR",
        "colab_type": "code",
        "outputId": "1274c51d-a407-4517-a730-9d64ba82eefd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "mnist_labels[0][:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7., 2., 1., 0., 4., 1., 4., 9., 5., 9., 0., 6., 9., 0., 1., 5., 9., 7.,\n",
              "        3., 4., 9., 6., 6., 5., 4., 0., 7., 4., 0., 1., 3., 1., 3., 4., 7., 2.,\n",
              "        7., 1., 2., 1., 1., 7., 4., 2., 3., 5., 1., 2., 4., 4., 6., 3., 5., 5.,\n",
              "        6., 0., 4., 1., 9., 5., 7., 8., 9., 3., 7., 4., 6., 4., 3., 0., 7., 0.,\n",
              "        2., 9., 1., 7., 3., 2., 9., 7., 7., 6., 2., 7., 8., 4., 7., 3., 6., 1.,\n",
              "        3., 6., 9., 3., 1., 4., 1., 7., 6., 9.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMa--HY0I3dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_label = np.array( [ item.int() for item in mnist_labels[0].flatten() ] )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqQgpXigJkWP",
        "colab_type": "code",
        "outputId": "f6e54346-6061-44f9-f7b0-1a9799ac8b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(mnist_label)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzQtuIqdt2EL",
        "colab_type": "code",
        "outputId": "dcc8a4c5-7866-43ad-a64c-74441d272efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "mnist_label.shape\n",
        "mnist_label[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhwbaicNJqwv",
        "colab_type": "code",
        "outputId": "d34ca3dd-d9b7-4225-9233-7710b96c35fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(true_labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gln7xCkSKOu3",
        "colab_type": "code",
        "outputId": "020fd408-2513-4aa9-b79e-774690ef38d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "true_labels[12][:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7., 2., 1., 0., 4., 1., 4., 4., 5., 9., 0., 6., 9., 0., 1., 5., 9., 7.,\n",
              "        3., 4., 9., 6., 6., 5., 4., 0., 7., 4., 0., 1., 3., 1., 3., 0., 7., 2.,\n",
              "        7., 1., 2., 1., 1., 7., 4., 2., 3., 5., 3., 2., 4., 4., 6., 3., 5., 5.,\n",
              "        6., 0., 4., 1., 9., 5., 7., 8., 4., 3., 7., 4., 0., 4., 3., 0., 7., 0.,\n",
              "        2., 8., 1., 7., 3., 7., 9., 7., 7., 6., 2., 7., 8., 4., 7., 3., 6., 1.,\n",
              "        3., 6., 4., 3., 1., 4., 3., 1., 6., 9.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D3IO5fhtsGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_label = [item.int() for sublist in true_labels for item in sublist.flatten()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtrnxQvzt6kv",
        "colab_type": "code",
        "outputId": "609f8fb3-bace-44ef-ec77-7ab3b1fc7797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(true_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "760000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDKNvbchuOzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictions from num_teachers on test_train (9500)\n",
        "preds = np.array(true_label).reshape(num_teachers,-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uus_01xMAGhL",
        "colab_type": "code",
        "outputId": "44018b75-600c-4322-e597-ecb86032d906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(mnist_label.shape,preds.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9500,) (80, 9500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ZitwWXJHKX",
        "colab_type": "code",
        "outputId": "c00ad053-2cd7-4be7-b0ca-3268672c9445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc_teacher/num_teachers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.7457)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjKjyZ_aXPRo",
        "colab_type": "text"
      },
      "source": [
        "#Differential Privacy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QM5YdC3jVBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install syft\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgzXenrNnG9p",
        "colab_type": "code",
        "outputId": "8b4f4d68-cb4d-4757-8b44-7695938e4a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "import syft as sy\n",
        "from syft.frameworks.torch.differential_privacy import pate"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0831 03:06:55.901216 140253058959232 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0831 03:06:55.914040 140253058959232 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVZPCSvZ7aQU",
        "colab_type": "code",
        "outputId": "ccba640d-c7d1-4310-d1fa-f57b8da374d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mnist_label[:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 9, 0, 1], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkCuJVRVwtEe",
        "colab_type": "code",
        "outputId": "53c86e66-5c0e-4193-c7c2-19f6ef5be60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "preds[:,0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo-23B7zw5oY",
        "colab_type": "code",
        "outputId": "43fb5566-a28f-4715-f1a9-3279e25e16ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_labels = 10\n",
        "label_counts = np.bincount(preds[:,0], minlength=num_labels)\n",
        "label_counts"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0, 80,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKe7YGSKxGhI",
        "colab_type": "code",
        "outputId": "d81b8513-b223-4ad9-831d-feb4645cb70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_label = np.argmax(label_counts)\n",
        "new_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQSW3qKx2NW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#iterate through all images\n",
        "num_labels = 10\n",
        "\n",
        "new_labels = list()\n",
        "for an_image in np.transpose(preds):\n",
        "\n",
        "    label_counts = np.bincount(an_image, minlength=num_labels)\n",
        "\n",
        "    new_label = np.argmax(label_counts)\n",
        "    \n",
        "    new_labels.append(new_label)\n",
        "    \n",
        "new_labels = np.array(new_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPVi1LVoOjaf",
        "colab_type": "code",
        "outputId": "6d7451d1-1e4f-4088-a335-b8b72efc9587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_labels.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZpCL_eGyBsR",
        "colab_type": "code",
        "outputId": "e6ec5ed9-35ec-45d8-e683-1e3a832eff75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(new_labels[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 6 7 2 7\n",
            " 1 2 1 1 7 4 2 3 5 3 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 4 3 7 4 6 4 3 0 7 0 2 7\n",
            " 1 7 3 7 9 7 7 6 2 7 8 4 7 3 6 1 3 6 4 3 1 4 1 7 6 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IO6QR7FGPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create dp_labels\n",
        "#iterate through 9500 images\n",
        "\n",
        "num_labels = 10\n",
        "\n",
        "dp_labels = list()\n",
        "for item in np.transpose(preds):\n",
        "\n",
        "    label_counts = np.bincount(item, minlength=num_labels)\n",
        "\n",
        "    epsilon = 0.1     #131.5/3000 = 0.044 68.77/3000 = 0.023\n",
        "    beta = 1 / epsilon\n",
        "\n",
        "    for i in range(len(label_counts)):\n",
        "        label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "    dp_label = np.argmax(label_counts)\n",
        "    \n",
        "    dp_labels.append(new_label)\n",
        "    \n",
        "dp_labels = np.array(new_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw0kHlEnVNIo",
        "colab_type": "code",
        "outputId": "9f176a95-e6ec-43f1-a98a-c006083f8049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dp_labels.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9500,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HfGxX6UyWYA",
        "colab_type": "code",
        "outputId": "8fba0a58-b616-4722-86ba-a5e33b59719c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "mnist_label[0:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R_RDhV_sBc8",
        "colab_type": "code",
        "outputId": "c57fac89-0e11-4e01-aeb5-212ac158e935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "new_labels[0:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 7, 1, 7, 3, 7, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 4, 3, 1, 4, 1, 7, 6, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wirnQKvnF_nW",
        "colab_type": "code",
        "outputId": "c28d02d6-8d6d-4f2e-8f93-386d5660fa5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "dp_labels[0:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4,\n",
              "       6, 4, 3, 0, 7, 0, 2, 7, 1, 7, 3, 7, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3,\n",
              "       6, 1, 3, 6, 4, 3, 1, 4, 1, 7, 6, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D6U28_DSROM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fed_accuracy(alt_labels, true_labels):\n",
        "    x =np.array( [(alt_labels[i] == true_labels[i]) for i in new_labels]).astype(int)\n",
        "    fed_accuracy = sum(x)/len(new_labels)\n",
        "    print(fed_accuracy)\n",
        "    return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hksa2XnqyBlT",
        "colab_type": "code",
        "outputId": "2b26689b-3cc3-44ab-c4db-c8626f52f67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# test_dataset(10000)\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=dp_labels, noise_eps=0.1, delta=1e-5) #,moments =20)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 411.5129254649703\n",
            "Data Dependent Epsilon: 93.89830871784949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCOqtsDWrq_U",
        "colab_type": "code",
        "outputId": "33788061-d418-4b88-efc7-30734fbd2c19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "fed_accuracy(dp_labels, mnist_label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9151578947368421\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc_EYd406QvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY4sY-M9SuYx",
        "colab_type": "code",
        "outputId": "23ffff38-84ba-4ba9-9dc3-dd41a64652e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=dp_labels, noise_eps=0.1, delta=1e-5) #,moments =20)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 131.51292546497027\n",
            "Data Dependent Epsilon: 68.7707820260897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6fhFy5NyPH3",
        "colab_type": "code",
        "outputId": "3d31916d-6463-49a0-9cfe-9af15ff7f76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#data_ind_eps = 0.044\n",
        "fed_accuracy(dp_labels, mnist_label)\n",
        "#0.7833"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7833333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzRpa7z7Su9Q",
        "colab_type": "code",
        "outputId": "d374736a-56aa-4644-fd4a-1b092c86ca1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#data_ind_eps = 0.023\n",
        "fed_accuracy(dp_labels, mnist_label)\n",
        "#0.7833"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7833333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OakWL1IVd-VP",
        "colab_type": "text"
      },
      "source": [
        "#create new dp_dataset = images+dp_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJkkeIbmLF_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_N6cYrQOgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_ds = zip(dp_labels, imgs)\n",
        "\n",
        "result = list(new_ds)\n",
        "print(result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzOwKjxJYS7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_ds = zip(imgs, dp_labels)\n",
        "\n",
        "local_result = list(local_ds)\n",
        "print(local_result[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK78bRk9gTu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_UF3I3yNkum",
        "colab_type": "code",
        "outputId": "9db3af0a-c5e8-4fa8-aba2-7d0cdc2ac622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tensor_x = torch.stack([torch.Tensor(r) for r in result[0][1]])\n",
        "tensor_x.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFisievOEMse",
        "colab_type": "code",
        "outputId": "e677fd4b-6dfb-4157-9e7e-82ea09eaf923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(result[0][1].squeeze().shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ--sKEorMi8",
        "colab_type": "code",
        "outputId": "fe92008b-d2e3-40b5-cbf9-19db5e881c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x=np.array([sublist[0] for sublist in result])\n",
        "x[:64].transpose()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDISGUV1U3bs",
        "colab_type": "code",
        "outputId": "a1edb00c-b795-465d-f633-53f19e5ec0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "imgs_arr = [item.numpy() for item in imgs]\n",
        "\n",
        "len(imgs_arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rVQzh45oYi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(dataset = local_result,  batch_size=64,sampler = torch.utils.data.SequentialSampler(local_result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_KjpnpowST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlb = []\n",
        "for label,image in trainloader:\n",
        "  nlb.append(label)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW9RleKTTtYf",
        "colab_type": "code",
        "outputId": "c384c1c8-affd-4c0b-a4e9-c07a90c1a7d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nlb[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
              "        4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 3, 2,\n",
              "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sCKNfmspbYf",
        "colab_type": "code",
        "outputId": "d784e9b2-8318-458d-859c-18227d03bdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dp_labels[:64]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6,\n",
              "       6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2,\n",
              "       3, 5, 3, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWhPap0pO0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.array(nlb[:64]).transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zplLW5qAIZt",
        "colab_type": "text"
      },
      "source": [
        "#Create remote workers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F_pzaAgAYTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(torch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX57iACDAuhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bob = sy.VirtualWorker(hook, id=\"bob\")#.add_worker(sy.local_worker)\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")#.add_worker(sy.local_worker)\n",
        "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")#.add_worker(sy.local_worker)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnoZsZYN1M4s",
        "colab_type": "text"
      },
      "source": [
        "#train local model on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrP89c73me32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trainloader = valid_loader\n",
        "testloader = test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5I9rNW5CH_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor_encrypt(tens):\n",
        "    return tens.fix_precision().share(alice,bob, crypto_provider = secure_worker, requires_grad = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiOXOa0VBUJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encrypted dataloader\n",
        "def encrypt_trainloader():\n",
        "    encrypted_target = []\n",
        "    encrypted_images = []\n",
        "    for i, (image, target) in enumerate(trainloader):\n",
        "        x = tensor_encrypt(target)     #torch.from_numpy(target)\n",
        "        y = tensor_encrypt(torch.stack([torch.FloatTensor(r) for r in image]))\n",
        "\n",
        "        encrypted_target.append(x)\n",
        "        encrypted_images.append(y)\n",
        "        \n",
        "    encrypted_trainloader = list(zip(encrypted_images,encrypted_target))    \n",
        "    return encrypted_trainloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K4yPMenXbPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encrypted_trainloader = encrypt_trainloader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdGq88KzRS4F",
        "colab_type": "code",
        "outputId": "ea4e4617-ba22-45a3-abab-faa21e99bd31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "encrypted_trainloader[0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
              " \t-> [PointerTensor | me:20392186314 -> alice:77919678737]\n",
              " \t-> [PointerTensor | me:39774904561 -> bob:6846002777]\n",
              " \t*crypto provider: secure_worker*,\n",
              " (Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
              " \t-> [PointerTensor | me:73363614545 -> alice:81744867945]\n",
              " \t-> [PointerTensor | me:12988787576 -> bob:68724884360]\n",
              " \t*crypto provider: secure_worker*)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBBIvqgmX0um",
        "colab_type": "code",
        "outputId": "141c502b-0a0e-4037-dafc-081d770b8258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "img, target = next(iter(testloader))\n",
        "print(img.type(),target.type(),target)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.FloatTensor torch.LongTensor tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
            "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
            "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEvL5WV1aCvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encrypt_testloader():\n",
        "    encrypted_target = []\n",
        "    encrypted_images = []\n",
        "    for i, (image, target) in enumerate(testloader):\n",
        "\n",
        "        x = tensor_encrypt(target)     #torch.from_numpy(target)\n",
        "        \n",
        "        y = tensor_encrypt(image)\n",
        "\n",
        "        encrypted_target.append(x)\n",
        "        encrypted_images.append(y)\n",
        "        \n",
        "    encrypted_testloader = list(zip(encrypted_images, encrypted_target))    \n",
        "    return encrypted_testloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFq3Mp4hQ6DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encrypted_testloader = encrypt_testloader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtH2m92VUFDp",
        "colab_type": "code",
        "outputId": "eb7cdf81-f48d-443a-cc69-38b9e8b2d6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "encrypted_testloader[0]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
              " \t-> [PointerTensor | me:52203504153 -> alice:51235638974]\n",
              " \t-> [PointerTensor | me:17220461085 -> bob:31109358671]\n",
              " \t*crypto provider: secure_worker*,\n",
              " (Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
              " \t-> [PointerTensor | me:97521740341 -> alice:45049752039]\n",
              " \t-> [PointerTensor | me:22886434918 -> bob:13814933053]\n",
              " \t*crypto provider: secure_worker*)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BejOj4x0n2NX",
        "colab_type": "code",
        "outputId": "44d3e5f1-867c-4b4b-cf85-31047d27acbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "images,labels =next(iter(trainloader))\n",
        "print(images.shape, labels.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHGVDL04oO4",
        "colab_type": "code",
        "outputId": "40e5a5ec-b1de-4027-b4c0-e4e4abcdf6bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "local_model = Conv()\n",
        "local_model"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdR5BFJTffPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "aa217f11-09a3-4d17-a712-1d0ead0fbe33"
      },
      "source": [
        "local_model.fix_precision().share(alice,bob, crypto_provider = secure_worker, requires_grad = True)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (conv2_drop): Dropout2d(p=0.5)\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmzzrcLV8PaX",
        "colab_type": "code",
        "outputId": "b20bd174-49b2-4985-aedc-c97a64bde79c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "reset_model_conv(local_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMhJfvTJ4U8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model_1(device,criterion,optimizer):\n",
        "    \n",
        "    #device = device\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "                \n",
        "         for inputs, labels  in encrypted_testloader:\n",
        "\n",
        "            #inputs = inputs.view(inputs.shape[0], -1)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            log_ps = model.forward(inputs)\n",
        "            \n",
        "            #print(labels.shape,log_ps.shape)\n",
        "            test_loss += criterion(log_ps, labels).item()\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            \n",
        "            #print(f\"top_p: {top_p}\")\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            #print(f\"equals: {equals}\")\n",
        "            acc += torch.mean(equals.type(torch.FloatTensor))\n",
        "            #print(f\"acc: {acc}\")\n",
        "            #test_count +=1\n",
        "            \n",
        "    return test_loss, acc      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r051uvRO4cny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model_1():\n",
        "    \n",
        "    device = 'cuda'\n",
        "    epochs = 1\n",
        "    count = 0\n",
        "\n",
        "    #model.to(device)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(),lr = 0.01)\n",
        "    optimizer = optim.SGD(model.parameters(),lr= 0.01, momentum = 0.5)\n",
        "    optimizer.fix_precision()\n",
        "    \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    accuracy =  []\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        train_count = 0\n",
        "\n",
        "        for labels, images in encrypted_trainloader:\n",
        "\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # TODO: Training pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #output = model(images)\n",
        "            output = model.forward(images)\n",
        "\n",
        "            #print(output.shape, labels.shape)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            #print(train_count)\n",
        "            train_count+=1\n",
        "            \n",
        "        else:\n",
        "            #model evaluation\n",
        "            test_loss,acc = evaluate_model_1(device,criterion,optimizer)\n",
        "\n",
        "            #collect accuracy, train_losses and test_losses for each epoch\n",
        "            train_losses.append(running_loss/len(trainloader))\n",
        "            test_losses.append(test_loss/len(testloader))\n",
        "            accuracy.append(acc/len(testloader))\n",
        "        \n",
        "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                  \"Training Loss: {:.6f}.. \".format(train_losses[e]),\n",
        "                  \"Test Loss: {:.6f}.. \".format(test_losses[e]),\n",
        "                  \"Test Accuracy: {:.6f}\".format(accuracy[e])\n",
        "                  )\n",
        "          \n",
        "          \n",
        "    return -1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd9Skf1wHNZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_count = 0\n",
        "for images, labels in trainloader:\n",
        "    \n",
        "    if train_count == int(len(test_train_ds)/batch_size):\n",
        "        print(f\"Num_samples = {train_count*batch_size}\")\n",
        "        break\n",
        "    train_count+=1  \n",
        "    print(train_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6d1mr8L4lb1",
        "colab_type": "code",
        "outputId": "6c059ca8-c350-46dd-c5cf-daa50a6b1154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        " #train local_model on test_ds\n",
        "  train_model_1()\n",
        "  \n",
        " "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    301\u001b[0m             new_args, new_kwargs, new_type, args_type = syft.frameworks.torch.hook_args.unwrap_args_from_function(\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_args_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36munwrap_args_from_function\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Run it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mnew_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs_hook_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36mseven_fold\u001b[0;34m(lambdas, args, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# Last if not, rule is probably == 1 so use type to return the right transformation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mforward_func\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# And do this for all the args / rules provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPureFrameworkTensorFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mLoggingTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/hook/hook_args.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPureFrameworkTensorFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mLoggingTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-5da404935fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-fa713fbfe2b3>\u001b[0m in \u001b[0;36mtrain_model_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#output = model(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#print(output.shape, labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c85c618446f4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mhandle_func_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/syft-0.1.25a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# in the execute_command function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 10, but got 1-dimensional input of size [0] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtx6uVHIRdmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Epoch: 15/15..  Training Loss: 0.282330..  Test Loss: 0.357445..  Test Accuracy: 0.900691\n",
        "#Epoch: 15/15..  Training Loss: 0.272720..  Test Loss: 0.347765..  Test Accuracy: 0.904147"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9wGfxarr4aT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}